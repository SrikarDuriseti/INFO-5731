{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "I have an interesting and practical question which I haven't tried ever. So I would like to predict the pollution of a location. For this I would need bundles of data from the past . So I will be able to feed it to my model and give a final prediction.\n",
        "\n",
        "So with the help of API request I will hit the server of the page and get the response in json format which looks like a dictionary datatype.\n",
        "This data will be used to feed my model and give a future prediction."
      ],
      "metadata": {
        "id": "guc1r4j9Nd2e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "city_name=input('What is the name of the city?')\n",
        "\n",
        "import requests\n",
        "import json\n",
        "\n",
        "geo_city=\"http://api.openweathermap.org/data/2.5/weather?q=\"+city_name+\"&appid=cac0f99dc37c6e8658996f0431e835b8\"\n",
        "#my API id- cac0f99dc37c6e8658996f0431e835b8\n",
        "\n",
        "response_city=requests.get(geo_city)\n",
        "\n",
        "res_city=response_city.json()\n",
        "\n",
        "print(res_city)\n",
        "\n",
        "longitude=str(res_city['coord']['lon'])   #Lat and Long from JSON using REST API\n",
        "\n",
        "latitude=str(res_city['coord']['lat'])\n",
        "\n",
        "#now call API for air-pollution\n",
        "\n",
        "api_pollution='http://api.openweathermap.org/data/2.5/air_pollution/forecast?lat='+latitude+'&lon='+longitude+'&appid=cac0f99dc37c6e8658996f0431e835b8'\n",
        "\n",
        "pollution_response=requests.get(api_pollution)\n",
        "\n",
        "pol_city=pollution_response.json()\n",
        "\n",
        "print(pol_city)"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16f004c3-b03e-4114-ceea-a80fa353e7bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is the name of the city?Dallas\n",
            "{'coord': {'lon': -96.7836, 'lat': 32.7668}, 'weather': [{'id': 804, 'main': 'Clouds', 'description': 'overcast clouds', 'icon': '04n'}], 'base': 'stations', 'main': {'temp': 292.21, 'feels_like': 291.97, 'temp_min': 291.12, 'temp_max': 293.29, 'pressure': 1017, 'humidity': 69}, 'visibility': 10000, 'wind': {'speed': 3.6, 'deg': 180}, 'clouds': {'all': 100}, 'dt': 1708042919, 'sys': {'type': 2, 'id': 2075302, 'country': 'US', 'sunrise': 1708002653, 'sunset': 1708042316}, 'timezone': -21600, 'id': 4684904, 'name': 'Dallas', 'cod': 200}\n",
            "{'coord': {'lon': -96.7836, 'lat': 32.7668}, 'list': [{'main': {'aqi': 3}, 'components': {'co': 734.33, 'no': 17.43, 'no2': 75.4, 'o3': 7.15, 'so2': 2.95, 'pm2_5': 23.62, 'pm10': 35.15, 'nh3': 4.81}, 'dt': 1708041600}, {'main': {'aqi': 3}, 'components': {'co': 767.71, 'no': 24.36, 'no2': 72.66, 'o3': 0.05, 'so2': 2.15, 'pm2_5': 22.32, 'pm10': 35.11, 'nh3': 4.56}, 'dt': 1708045200}, {'main': {'aqi': 2}, 'components': {'co': 700.95, 'no': 19.89, 'no2': 63.06, 'o3': 0.1, 'so2': 1.48, 'pm2_5': 20.28, 'pm10': 33.47, 'nh3': 3.29}, 'dt': 1708048800}, {'main': {'aqi': 2}, 'components': {'co': 587.46, 'no': 9.16, 'no2': 53.47, 'o3': 1.17, 'so2': 1.12, 'pm2_5': 17.54, 'pm10': 29.71, 'nh3': 2.19}, 'dt': 1708052400}, {'main': {'aqi': 2}, 'components': {'co': 487.33, 'no': 2.29, 'no2': 41.13, 'o3': 5.9, 'so2': 0.91, 'pm2_5': 15.43, 'pm10': 25.98, 'nh3': 1.54}, 'dt': 1708056000}, {'main': {'aqi': 2}, 'components': {'co': 427.25, 'no': 0.66, 'no2': 31.87, 'o3': 12.7, 'so2': 0.85, 'pm2_5': 14.33, 'pm10': 23.12, 'nh3': 1.17}, 'dt': 1708059600}, {'main': {'aqi': 2}, 'components': {'co': 360.49, 'no': 0.05, 'no2': 20.56, 'o3': 28.97, 'so2': 1.01, 'pm2_5': 12.35, 'pm10': 19.32, 'nh3': 0.89}, 'dt': 1708063200}, {'main': {'aqi': 2}, 'components': {'co': 307.08, 'no': 0, 'no2': 11.65, 'o3': 48.64, 'so2': 1.54, 'pm2_5': 10.09, 'pm10': 16.22, 'nh3': 0.9}, 'dt': 1708066800}, {'main': {'aqi': 1}, 'components': {'co': 297.07, 'no': 0, 'no2': 10.11, 'o3': 52.93, 'so2': 1.61, 'pm2_5': 8.58, 'pm10': 14.9, 'nh3': 0.8}, 'dt': 1708070400}, {'main': {'aqi': 1}, 'components': {'co': 287.06, 'no': 0, 'no2': 9.25, 'o3': 55.08, 'so2': 1.59, 'pm2_5': 7.32, 'pm10': 14.42, 'nh3': 0.78}, 'dt': 1708074000}, {'main': {'aqi': 1}, 'components': {'co': 277.04, 'no': 0, 'no2': 8.65, 'o3': 53.64, 'so2': 1.64, 'pm2_5': 6.49, 'pm10': 13.95, 'nh3': 0.82}, 'dt': 1708077600}, {'main': {'aqi': 1}, 'components': {'co': 273.71, 'no': 0, 'no2': 8.48, 'o3': 50.78, 'so2': 1.67, 'pm2_5': 5.97, 'pm10': 12.64, 'nh3': 0.79}, 'dt': 1708081200}, {'main': {'aqi': 1}, 'components': {'co': 273.71, 'no': 0, 'no2': 9, 'o3': 47.21, 'so2': 1.7, 'pm2_5': 5.71, 'pm10': 11.41, 'nh3': 0.78}, 'dt': 1708084800}, {'main': {'aqi': 1}, 'components': {'co': 287.06, 'no': 0.01, 'no2': 12.68, 'o3': 42.56, 'so2': 1.83, 'pm2_5': 5.92, 'pm10': 11.53, 'nh3': 0.9}, 'dt': 1708088400}, {'main': {'aqi': 1}, 'components': {'co': 307.08, 'no': 0.04, 'no2': 16.79, 'o3': 37.19, 'so2': 2, 'pm2_5': 6.29, 'pm10': 12.37, 'nh3': 1}, 'dt': 1708092000}, {'main': {'aqi': 1}, 'components': {'co': 307.08, 'no': 0.39, 'no2': 16.62, 'o3': 36.84, 'so2': 2.09, 'pm2_5': 6.4, 'pm10': 12.44, 'nh3': 1}, 'dt': 1708095600}, {'main': {'aqi': 1}, 'components': {'co': 297.07, 'no': 0.28, 'no2': 13.88, 'o3': 40.05, 'so2': 1.91, 'pm2_5': 5.8, 'pm10': 10.08, 'nh3': 0.85}, 'dt': 1708099200}, {'main': {'aqi': 1}, 'components': {'co': 287.06, 'no': 0.25, 'no2': 12, 'o3': 40.77, 'so2': 1.71, 'pm2_5': 4.76, 'pm10': 7.35, 'nh3': 0.79}, 'dt': 1708102800}, {'main': {'aqi': 1}, 'components': {'co': 287.06, 'no': 0.32, 'no2': 10.63, 'o3': 41.13, 'so2': 1.57, 'pm2_5': 4.15, 'pm10': 5.79, 'nh3': 0.84}, 'dt': 1708106400}, {'main': {'aqi': 1}, 'components': {'co': 280.38, 'no': 0.34, 'no2': 8.31, 'o3': 48.64, 'so2': 1.21, 'pm2_5': 3.92, 'pm10': 5.31, 'nh3': 0.95}, 'dt': 1708110000}, {'main': {'aqi': 1}, 'components': {'co': 277.04, 'no': 0.31, 'no2': 8.4, 'o3': 55.08, 'so2': 1.37, 'pm2_5': 4.09, 'pm10': 5.87, 'nh3': 1.11}, 'dt': 1708113600}, {'main': {'aqi': 2}, 'components': {'co': 280.38, 'no': 0.24, 'no2': 9.77, 'o3': 60.08, 'so2': 1.76, 'pm2_5': 5.03, 'pm10': 7.67, 'nh3': 1.19}, 'dt': 1708117200}, {'main': {'aqi': 2}, 'components': {'co': 283.72, 'no': 0.24, 'no2': 11.48, 'o3': 65.09, 'so2': 1.91, 'pm2_5': 7.41, 'pm10': 12.36, 'nh3': 1.41}, 'dt': 1708120800}, {'main': {'aqi': 2}, 'components': {'co': 297.07, 'no': 0.27, 'no2': 14.22, 'o3': 67.95, 'so2': 1.97, 'pm2_5': 8.73, 'pm10': 15.72, 'nh3': 2.25}, 'dt': 1708124400}, {'main': {'aqi': 2}, 'components': {'co': 313.76, 'no': 0.11, 'no2': 17.31, 'o3': 70.1, 'so2': 1.88, 'pm2_5': 8.28, 'pm10': 14.74, 'nh3': 3.07}, 'dt': 1708128000}, {'main': {'aqi': 2}, 'components': {'co': 330.45, 'no': 0, 'no2': 17.82, 'o3': 60.08, 'so2': 1.76, 'pm2_5': 9.89, 'pm10': 15.15, 'nh3': 3.14}, 'dt': 1708131600}, {'main': {'aqi': 2}, 'components': {'co': 350.48, 'no': 0, 'no2': 16.62, 'o3': 50.78, 'so2': 1.91, 'pm2_5': 12.29, 'pm10': 15.71, 'nh3': 2.82}, 'dt': 1708135200}, {'main': {'aqi': 1}, 'components': {'co': 333.79, 'no': 0, 'no2': 11.82, 'o3': 58.65, 'so2': 1.73, 'pm2_5': 9.48, 'pm10': 11.34, 'nh3': 2.6}, 'dt': 1708138800}, {'main': {'aqi': 2}, 'components': {'co': 310.42, 'no': 0, 'no2': 9.43, 'o3': 64.37, 'so2': 1.42, 'pm2_5': 6.36, 'pm10': 7.26, 'nh3': 2.22}, 'dt': 1708142400}, {'main': {'aqi': 2}, 'components': {'co': 303.75, 'no': 0, 'no2': 9, 'o3': 64.37, 'so2': 1.51, 'pm2_5': 5.82, 'pm10': 6.44, 'nh3': 1.87}, 'dt': 1708146000}, {'main': {'aqi': 2}, 'components': {'co': 293.73, 'no': 0, 'no2': 7.97, 'o3': 64.37, 'so2': 1.62, 'pm2_5': 5.51, 'pm10': 5.95, 'nh3': 1.54}, 'dt': 1708149600}, {'main': {'aqi': 2}, 'components': {'co': 287.06, 'no': 0, 'no2': 6.6, 'o3': 65.09, 'so2': 1.59, 'pm2_5': 5.12, 'pm10': 5.48, 'nh3': 1.33}, 'dt': 1708153200}, {'main': {'aqi': 2}, 'components': {'co': 280.38, 'no': 0, 'no2': 5.91, 'o3': 67.23, 'so2': 1.54, 'pm2_5': 4.61, 'pm10': 4.95, 'nh3': 1.2}, 'dt': 1708156800}, {'main': {'aqi': 2}, 'components': {'co': 273.71, 'no': 0, 'no2': 5.36, 'o3': 70.1, 'so2': 1.42, 'pm2_5': 3.81, 'pm10': 4.16, 'nh3': 1.14}, 'dt': 1708160400}, {'main': {'aqi': 2}, 'components': {'co': 270.37, 'no': 0, 'no2': 4.97, 'o3': 73.67, 'so2': 1.43, 'pm2_5': 2.93, 'pm10': 3.3, 'nh3': 1.14}, 'dt': 1708164000}, {'main': {'aqi': 2}, 'components': {'co': 267.03, 'no': 0, 'no2': 4.88, 'o3': 75.82, 'so2': 1.59, 'pm2_5': 2.47, 'pm10': 2.87, 'nh3': 1.14}, 'dt': 1708167600}, {'main': {'aqi': 2}, 'components': {'co': 270.37, 'no': 0, 'no2': 5.44, 'o3': 75.82, 'so2': 1.83, 'pm2_5': 2.6, 'pm10': 3.06, 'nh3': 1.2}, 'dt': 1708171200}, {'main': {'aqi': 2}, 'components': {'co': 283.72, 'no': 0, 'no2': 7.63, 'o3': 73.67, 'so2': 2.09, 'pm2_5': 3.38, 'pm10': 4.01, 'nh3': 1.38}, 'dt': 1708174800}, {'main': {'aqi': 2}, 'components': {'co': 297.07, 'no': 0.12, 'no2': 10.28, 'o3': 70.1, 'so2': 2.38, 'pm2_5': 4.37, 'pm10': 5.17, 'nh3': 1.58}, 'dt': 1708178400}, {'main': {'aqi': 2}, 'components': {'co': 300.41, 'no': 1.29, 'no2': 9.94, 'o3': 70.1, 'so2': 2.62, 'pm2_5': 5.15, 'pm10': 6.02, 'nh3': 1.69}, 'dt': 1708182000}, {'main': {'aqi': 2}, 'components': {'co': 297.07, 'no': 2.15, 'no2': 7.97, 'o3': 72.24, 'so2': 2.74, 'pm2_5': 5.71, 'pm10': 6.62, 'nh3': 1.68}, 'dt': 1708185600}, {'main': {'aqi': 2}, 'components': {'co': 293.73, 'no': 2.4, 'no2': 6.68, 'o3': 75.1, 'so2': 2.77, 'pm2_5': 6.42, 'pm10': 7.33, 'nh3': 1.65}, 'dt': 1708189200}, {'main': {'aqi': 2}, 'components': {'co': 290.39, 'no': 2.35, 'no2': 5.91, 'o3': 78.68, 'so2': 2.68, 'pm2_5': 7.23, 'pm10': 8.08, 'nh3': 1.54}, 'dt': 1708192800}, {'main': {'aqi': 2}, 'components': {'co': 283.72, 'no': 2.1, 'no2': 5.44, 'o3': 84.4, 'so2': 2.47, 'pm2_5': 7.3, 'pm10': 8.05, 'nh3': 1.38}, 'dt': 1708196400}, {'main': {'aqi': 2}, 'components': {'co': 280.38, 'no': 1.84, 'no2': 5.23, 'o3': 89.41, 'so2': 2.35, 'pm2_5': 6.86, 'pm10': 7.53, 'nh3': 1.27}, 'dt': 1708200000}, {'main': {'aqi': 2}, 'components': {'co': 273.71, 'no': 1.58, 'no2': 5.14, 'o3': 92.98, 'so2': 2.33, 'pm2_5': 6.51, 'pm10': 7.13, 'nh3': 1.25}, 'dt': 1708203600}, {'main': {'aqi': 2}, 'components': {'co': 277.04, 'no': 1.63, 'no2': 6.6, 'o3': 91.55, 'so2': 2.38, 'pm2_5': 6.26, 'pm10': 6.9, 'nh3': 1.33}, 'dt': 1708207200}, {'main': {'aqi': 2}, 'components': {'co': 290.39, 'no': 1.79, 'no2': 11.31, 'o3': 86.55, 'so2': 2.5, 'pm2_5': 5.8, 'pm10': 6.57, 'nh3': 1.6}, 'dt': 1708210800}, {'main': {'aqi': 2}, 'components': {'co': 310.42, 'no': 0.75, 'no2': 17.99, 'o3': 77.96, 'so2': 2.5, 'pm2_5': 5.17, 'pm10': 6.13, 'nh3': 1.95}, 'dt': 1708214400}, {'main': {'aqi': 1}, 'components': {'co': 387.19, 'no': 0.02, 'no2': 38.39, 'o3': 53.64, 'so2': 2.35, 'pm2_5': 6.91, 'pm10': 8.89, 'nh3': 2.82}, 'dt': 1708218000}, {'main': {'aqi': 2}, 'components': {'co': 500.68, 'no': 0.34, 'no2': 63.75, 'o3': 24.68, 'so2': 2, 'pm2_5': 10.79, 'pm10': 14.31, 'nh3': 3.86}, 'dt': 1708221600}, {'main': {'aqi': 3}, 'components': {'co': 547.41, 'no': 1.43, 'no2': 71.29, 'o3': 11, 'so2': 1.64, 'pm2_5': 13.41, 'pm10': 17.99, 'nh3': 4.24}, 'dt': 1708225200}, {'main': {'aqi': 3}, 'components': {'co': 567.44, 'no': 3.05, 'no2': 70.6, 'o3': 5.36, 'so2': 1.49, 'pm2_5': 14.81, 'pm10': 20, 'nh3': 4.37}, 'dt': 1708228800}, {'main': {'aqi': 2}, 'components': {'co': 547.41, 'no': 2.35, 'no2': 65.12, 'o3': 6.26, 'so2': 1.42, 'pm2_5': 14.1, 'pm10': 19.01, 'nh3': 3.77}, 'dt': 1708232400}, {'main': {'aqi': 2}, 'components': {'co': 473.98, 'no': 0.44, 'no2': 51.41, 'o3': 15.56, 'so2': 1.28, 'pm2_5': 10.91, 'pm10': 14.53, 'nh3': 2.66}, 'dt': 1708236000}, {'main': {'aqi': 1}, 'components': {'co': 417.23, 'no': 0.11, 'no2': 38.39, 'o3': 23.25, 'so2': 1.06, 'pm2_5': 8.32, 'pm10': 11.08, 'nh3': 1.85}, 'dt': 1708239600}, {'main': {'aqi': 1}, 'components': {'co': 400.54, 'no': 0.14, 'no2': 34.62, 'o3': 20.92, 'so2': 0.97, 'pm2_5': 7.62, 'pm10': 10.3, 'nh3': 1.35}, 'dt': 1708243200}, {'main': {'aqi': 1}, 'components': {'co': 397.21, 'no': 0.28, 'no2': 34.27, 'o3': 16.09, 'so2': 1.04, 'pm2_5': 7.67, 'pm10': 10.52, 'nh3': 1.17}, 'dt': 1708246800}, {'main': {'aqi': 1}, 'components': {'co': 393.87, 'no': 0.48, 'no2': 33.59, 'o3': 11.98, 'so2': 1.25, 'pm2_5': 7.95, 'pm10': 11.05, 'nh3': 1.36}, 'dt': 1708250400}, {'main': {'aqi': 1}, 'components': {'co': 397.21, 'no': 0.89, 'no2': 33.24, 'o3': 8.4, 'so2': 1.67, 'pm2_5': 8.52, 'pm10': 11.97, 'nh3': 1.92}, 'dt': 1708254000}, {'main': {'aqi': 1}, 'components': {'co': 420.57, 'no': 2.93, 'no2': 35.3, 'o3': 3.31, 'so2': 2.33, 'pm2_5': 9.72, 'pm10': 13.85, 'nh3': 2.88}, 'dt': 1708257600}, {'main': {'aqi': 2}, 'components': {'co': 514.03, 'no': 16.54, 'no2': 37.7, 'o3': 0.14, 'so2': 3.04, 'pm2_5': 13.41, 'pm10': 19.29, 'nh3': 4.56}, 'dt': 1708261200}, {'main': {'aqi': 2}, 'components': {'co': 660.9, 'no': 38.89, 'no2': 40.1, 'o3': 0.6, 'so2': 3.04, 'pm2_5': 17.82, 'pm10': 25.62, 'nh3': 5}, 'dt': 1708264800}, {'main': {'aqi': 2}, 'components': {'co': 754.36, 'no': 54.99, 'no2': 38.39, 'o3': 5.99, 'so2': 2.8, 'pm2_5': 20.44, 'pm10': 29.44, 'nh3': 4.37}, 'dt': 1708268400}, {'main': {'aqi': 2}, 'components': {'co': 540.73, 'no': 24.59, 'no2': 36.67, 'o3': 29.68, 'so2': 6.08, 'pm2_5': 15.14, 'pm10': 21.34, 'nh3': 3.2}, 'dt': 1708272000}, {'main': {'aqi': 2}, 'components': {'co': 480.65, 'no': 18.11, 'no2': 29.47, 'o3': 42.2, 'so2': 6.86, 'pm2_5': 15.26, 'pm10': 21.29, 'nh3': 2.85}, 'dt': 1708275600}, {'main': {'aqi': 2}, 'components': {'co': 433.92, 'no': 13.41, 'no2': 23.31, 'o3': 50.78, 'so2': 6.91, 'pm2_5': 15.72, 'pm10': 20.96, 'nh3': 2.5}, 'dt': 1708279200}, {'main': {'aqi': 2}, 'components': {'co': 307.08, 'no': 4.25, 'no2': 11.82, 'o3': 83.69, 'so2': 4.05, 'pm2_5': 8.61, 'pm10': 10.59, 'nh3': 1.43}, 'dt': 1708282800}, {'main': {'aqi': 2}, 'components': {'co': 283.72, 'no': 2.43, 'no2': 8.05, 'o3': 94.41, 'so2': 2.71, 'pm2_5': 7.54, 'pm10': 8.79, 'nh3': 1.3}, 'dt': 1708286400}, {'main': {'aqi': 3}, 'components': {'co': 280.38, 'no': 1.61, 'no2': 6.17, 'o3': 101.57, 'so2': 2.3, 'pm2_5': 9.46, 'pm10': 10.61, 'nh3': 1.39}, 'dt': 1708290000}, {'main': {'aqi': 3}, 'components': {'co': 290.39, 'no': 1.02, 'no2': 7.37, 'o3': 103, 'so2': 2.24, 'pm2_5': 11.45, 'pm10': 12.71, 'nh3': 1.62}, 'dt': 1708293600}, {'main': {'aqi': 2}, 'components': {'co': 303.75, 'no': 0.72, 'no2': 10.54, 'o3': 95.84, 'so2': 2.06, 'pm2_5': 11.54, 'pm10': 12.95, 'nh3': 2}, 'dt': 1708297200}, {'main': {'aqi': 2}, 'components': {'co': 307.08, 'no': 0.15, 'no2': 12.85, 'o3': 87.26, 'so2': 1.97, 'pm2_5': 8.76, 'pm10': 10.15, 'nh3': 2.38}, 'dt': 1708300800}, {'main': {'aqi': 2}, 'components': {'co': 297.07, 'no': 0, 'no2': 13.2, 'o3': 82.97, 'so2': 2.74, 'pm2_5': 6.53, 'pm10': 8, 'nh3': 2.57}, 'dt': 1708304400}, {'main': {'aqi': 2}, 'components': {'co': 287.06, 'no': 0, 'no2': 12.17, 'o3': 82.97, 'so2': 3.07, 'pm2_5': 5.44, 'pm10': 7.01, 'nh3': 2.72}, 'dt': 1708308000}, {'main': {'aqi': 2}, 'components': {'co': 280.38, 'no': 0, 'no2': 10.2, 'o3': 82.97, 'so2': 2.5, 'pm2_5': 5.19, 'pm10': 6.74, 'nh3': 2.95}, 'dt': 1708311600}, {'main': {'aqi': 2}, 'components': {'co': 273.71, 'no': 0, 'no2': 8.65, 'o3': 82.97, 'so2': 2.12, 'pm2_5': 5.58, 'pm10': 7.08, 'nh3': 3.14}, 'dt': 1708315200}, {'main': {'aqi': 2}, 'components': {'co': 267.03, 'no': 0, 'no2': 7.54, 'o3': 82.97, 'so2': 2, 'pm2_5': 6.18, 'pm10': 7.58, 'nh3': 3.26}, 'dt': 1708318800}, {'main': {'aqi': 2}, 'components': {'co': 260.35, 'no': 0, 'no2': 6, 'o3': 85.83, 'so2': 2.33, 'pm2_5': 7.06, 'pm10': 8.22, 'nh3': 3.07}, 'dt': 1708322400}, {'main': {'aqi': 2}, 'components': {'co': 257.02, 'no': 0, 'no2': 5.83, 'o3': 85.12, 'so2': 3.1, 'pm2_5': 7.56, 'pm10': 8.65, 'nh3': 2.85}, 'dt': 1708326000}, {'main': {'aqi': 2}, 'components': {'co': 253.68, 'no': 0, 'no2': 5.83, 'o3': 81.54, 'so2': 3.37, 'pm2_5': 7.77, 'pm10': 8.9, 'nh3': 2.63}, 'dt': 1708329600}, {'main': {'aqi': 2}, 'components': {'co': 253.68, 'no': 0, 'no2': 5.74, 'o3': 78.68, 'so2': 3.07, 'pm2_5': 8.27, 'pm10': 9.4, 'nh3': 2.47}, 'dt': 1708333200}, {'main': {'aqi': 2}, 'components': {'co': 257.02, 'no': 0, 'no2': 5.66, 'o3': 77.96, 'so2': 2.65, 'pm2_5': 8.92, 'pm10': 10.06, 'nh3': 2.34}, 'dt': 1708336800}, {'main': {'aqi': 2}, 'components': {'co': 263.69, 'no': 0, 'no2': 5.66, 'o3': 77.96, 'so2': 2.24, 'pm2_5': 9.47, 'pm10': 10.59, 'nh3': 2.28}, 'dt': 1708340400}, {'main': {'aqi': 2}, 'components': {'co': 267.03, 'no': 0, 'no2': 6.17, 'o3': 79.39, 'so2': 1.97, 'pm2_5': 9.98, 'pm10': 11.08, 'nh3': 2.09}, 'dt': 1708344000}, {'main': {'aqi': 2}, 'components': {'co': 277.04, 'no': 0, 'no2': 8.65, 'o3': 77.96, 'so2': 1.91, 'pm2_5': 10.7, 'pm10': 12, 'nh3': 1.82}, 'dt': 1708347600}, {'main': {'aqi': 2}, 'components': {'co': 290.39, 'no': 0.09, 'no2': 11.31, 'o3': 74.39, 'so2': 2.03, 'pm2_5': 11.16, 'pm10': 12.74, 'nh3': 1.55}, 'dt': 1708351200}, {'main': {'aqi': 2}, 'components': {'co': 293.73, 'no': 1.06, 'no2': 10.54, 'o3': 74.39, 'so2': 2.21, 'pm2_5': 11.06, 'pm10': 12.93, 'nh3': 1.33}, 'dt': 1708354800}, {'main': {'aqi': 2}, 'components': {'co': 290.39, 'no': 1.75, 'no2': 7.88, 'o3': 78.68, 'so2': 2.18, 'pm2_5': 9.96, 'pm10': 12.12, 'nh3': 1.31}, 'dt': 1708358400}, {'main': {'aqi': 2}, 'components': {'co': 283.72, 'no': 1.68, 'no2': 6, 'o3': 83.69, 'so2': 2.03, 'pm2_5': 8.72, 'pm10': 11.1, 'nh3': 1.49}, 'dt': 1708362000}, {'main': {'aqi': 2}, 'components': {'co': 280.38, 'no': 1.34, 'no2': 4.76, 'o3': 90.84, 'so2': 1.82, 'pm2_5': 8.82, 'pm10': 11.34, 'nh3': 1.52}, 'dt': 1708365600}, {'main': {'aqi': 2}, 'components': {'co': 273.71, 'no': 1.08, 'no2': 4.03, 'o3': 95.84, 'so2': 1.54, 'pm2_5': 8.46, 'pm10': 11.21, 'nh3': 1.66}, 'dt': 1708369200}, {'main': {'aqi': 2}, 'components': {'co': 273.71, 'no': 1.09, 'no2': 4.37, 'o3': 98.71, 'so2': 1.59, 'pm2_5': 7.66, 'pm10': 10.83, 'nh3': 1.9}, 'dt': 1708372800}, {'main': {'aqi': 3}, 'components': {'co': 270.37, 'no': 1.1, 'no2': 5.01, 'o3': 101.57, 'so2': 1.76, 'pm2_5': 6.22, 'pm10': 10.07, 'nh3': 2.09}, 'dt': 1708376400}, {'main': {'aqi': 2}, 'components': {'co': 290.39, 'no': 1.96, 'no2': 10.11, 'o3': 97.28, 'so2': 2.27, 'pm2_5': 5.24, 'pm10': 9.38, 'nh3': 2.34}, 'dt': 1708380000}, {'main': {'aqi': 2}, 'components': {'co': 330.45, 'no': 2.82, 'no2': 19.71, 'o3': 82.97, 'so2': 2.62, 'pm2_5': 4.54, 'pm10': 8.41, 'nh3': 2.66}, 'dt': 1708383600}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So here in this above code, I used API so that it requests and gets the response from the website back rather than downloading the csv,xlsx location/pollution dataset."
      ],
      "metadata": {
        "id": "Qc-Rk6f9UeUN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import time\n",
        "\n",
        "def fetch_arxiv_articles(keyword, max_results=1000):\n",
        "    \"\"\"Fetches articles from arXiv matching the given keyword.\n",
        "\n",
        "    Args:\n",
        "        keyword: The keyword to search for.\n",
        "        max_results: The maximum number of articles to fetch (default: 1000).\n",
        "\n",
        "    Returns:\n",
        "        A list of dictionaries, where each dictionary represents an article.\n",
        "    \"\"\"\n",
        "\n",
        "    base_url = \"http://export.arxiv.org/api/query?\"\n",
        "    search_url = \"{http://www.w3.org/2005/Atom}\"\n",
        "    articles = []\n",
        "    start = 0\n",
        "    max_per_query = 100\n",
        "\n",
        "    while len(articles) < max_results:\n",
        "        query_params = f\"search_query=all:{keyword}&start={start}&max_results={max_per_query}\"\n",
        "        response = requests.get(base_url + query_params)\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            root = ET.fromstring(response.content)\n",
        "            for entry in root.findall(search_url + 'entry'):\n",
        "                article = {\n",
        "                    \"title\": entry.find(search_url + 'title').text.strip(),\n",
        "                    \"venue/journal/conference\": \"arxiv\",\n",
        "                    \"year\": entry.find(search_url + 'published').text[:4],  # Extract year\n",
        "                    \"authors\": \", \".join([author.find(search_url + 'name').text for author in entry.findall(search_url + 'author')]),\n",
        "                    \"abstract\": entry.find(search_url + 'summary').text.strip()\n",
        "                }\n",
        "                articles.append(article)\n",
        "\n",
        "                if len(articles) >= max_results:\n",
        "                    break\n",
        "            start += max_per_query\n",
        "        else:\n",
        "            print(f\"Failed to get the data: {response.status_code}\")\n",
        "            break\n",
        "\n",
        "        time.sleep(3)\n",
        "\n",
        "    return articles\n",
        "\n",
        "keyword = \"information science\"\n",
        "articles = fetch_arxiv_articles(keyword, 200)\n",
        "for article in articles:\n",
        "    print(article)\n",
        "print(f\"Got {len(articles)} articles\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3T3BDRPbPRJ",
        "outputId": "7068680f-e164-43a3-b254-189ce91aeeaa"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'title': 'Quantum information science and complex quantum systems', 'venue/journal/conference': 'arxiv', 'year': '2002', 'authors': 'Michael A. Nielsen', 'abstract': 'What makes quantum information science a science? This paper explores the\\nidea that quantum information science may offer a powerful approach to the\\nstudy of complex quantum systems.'}\n",
            "{'title': 'Three fundamental problems in risk modeling on big data: an information\\n  theory view', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Jiamin Yu', 'abstract': \"Since Claude Shannon founded Information Theory, information theory has\\nwidely fostered other scientific fields, such as statistics, artificial\\nintelligence, biology, behavioral science, neuroscience, economics, and\\nfinance. Unfortunately, actuarial science has hardly benefited from information\\ntheory. So far, only one actuarial paper on information theory can be searched\\nby academic search engines. Undoubtedly, information and risk, both as\\nUncertainty, are constrained by entropy law. Today's insurance big data era\\nmeans more data and more information. It is unacceptable for risk management\\nand actuarial science to ignore information theory. Therefore, this paper aims\\nto exploit information theory to discover the performance limits of insurance\\nbig data systems and seek guidance for risk modeling and the development of\\nactuarial pricing systems.\"}\n",
            "{'title': 'Synergies Between Operations Research and Quantum Information Science', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Ojas Parekh', 'abstract': 'This article highlights synergies between quantum information science (QIS)\\nand operations research for QIS-curious operations researchers (and\\nvice-versa).'}\n",
            "{'title': 'Citizen Science: An Information Quality Research Frontier', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Roman Lukyanenko, Andrea Wiggins, Holly K. Rosser', 'abstract': 'The rapid proliferation of online content producing and sharing technologies\\nresulted in an explosion of user-generated content (UGC), which now extends to\\nscientific data. Citizen science, in which ordinary people contribute\\ninformation for scientific research, epitomizes UGC. Citizen science projects\\nare typically open to everyone, engage diverse audiences, and challenge\\nordinary people to produce data of highest quality to be usable in science.\\nThis also makes citizen science a very exciting area to study both traditional\\nand innovative approaches to information quality management. With this paper we\\nposition citizen science as a leading information quality research frontier. We\\nalso show how citizen science opens a unique opportunity for the information\\nsystems community to contribute to a broad range of disciplines in natural and\\nsocial sciences and humanities.'}\n",
            "{'title': 'Science Factionalism: How Group Identity Language Affects Public\\n  Engagement with Misinformation and Debunking Narratives on a Popular Q&A\\n  Platform in China', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Kaiping Chen, Yepeng Jin, Anqi Shao', 'abstract': 'Misinformation and intergroup bias are two pathologies challenging informed\\ncitizenship. This paper examines how identity language is used in\\nmisinformation and debunking messages about controversial science on Chinese\\ndigital public sphere, and their impact on how the public engage with science.\\nWe collected an eight-year time series dataset of public discussion (N=6039) on\\none of the most controversial science issues in China (GMO) from a popular Q&A\\nplatform, Zhihu. We found that both misinformation and debunking messages use a\\nsubstantial amount of group identity languages when discussing the\\ncontroversial science issue, which we define as science factionalism --\\ndiscussion about science is divided by factions that are formed upon science\\nattitudes. We found that posts that use science factionalism receive more\\ndigital votes and comments, even among the science-savvy community in China.\\nScience factionalism also increases the use of negativity in public discourse.\\nWe discussed the implications of how science factionalism interacts with the\\ndigital attention economy to affect public engagement with science\\nmisinformation.'}\n",
            "{'title': \"The Solution to Science's Replication Crisis\", 'venue/journal/conference': 'arxiv', 'year': '2016', 'authors': 'Bruce Knuteson', 'abstract': \"The solution to science's replication crisis is a new ecosystem in which\\nscientists sell what they learn from their research. In each pairwise\\ntransaction, the information seller makes (loses) money if he turns out to be\\ncorrect (incorrect). Responsibility for the determination of correctness is\\ndelegated, with appropriate incentives, to the information purchaser. Each\\ntransaction is brokered by a central exchange, which holds money from the\\nanonymous information buyer and anonymous information seller in escrow, and\\nwhich enforces a set of incentives facilitating the transfer of useful, bluntly\\nhonest information from the seller to the buyer. This new ecosystem, capitalist\\nscience, directly addresses socialist science's replication crisis by\\nexplicitly rewarding accuracy and penalizing inaccuracy.\"}\n",
            "{'title': 'An elementary introduction to information geometry', 'venue/journal/conference': 'arxiv', 'year': '2018', 'authors': 'Frank Nielsen', 'abstract': 'In this survey, we describe the fundamental differential-geometric structures\\nof information manifolds, state the fundamental theorem of information\\ngeometry, and illustrate some use cases of these information manifolds in\\ninformation sciences. The exposition is self-contained by concisely introducing\\nthe necessary concepts of differential geometry, but proofs are omitted for\\nbrevity.'}\n",
            "{'title': 'Semidefinite Programming in Quantum Information Science', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Paul Skrzypczyk, Daniel Cavalcanti', 'abstract': \"Semidefinite programs (SDPs) are a class of optimisation problems that find\\napplication in numerous areas of physics, engineering and mathematics.\\nSemidefinite programming is particularly suited to problems in quantum physics\\nand quantum information science. Following a review of the theory of\\nsemidefinite programming, the book proceeds to describe how it can be used to\\naddress a wide range of important problems from across quantum information\\nscience. Specific applications include quantum state, measurement, and channel\\nestimation and discrimination, entanglement detection and quantification,\\nquantum distance measures, and measurement incompatibility. Though SDPs have\\nbecome an increasingly important tool in quantum information science it's not\\nyet the kind of mathematics students learn routinely. Assuming only a basic\\nknowledge of linear algebra and quantum physics and quantum information, this\\ngraduate-level book provides a unified and accessible presentation of one of\\nthe key numerical methods used in quantum information science.\"}\n",
            "{'title': 'Entity Alignment Method of Science and Technology Patent based on Graph\\n  Convolution Network and Information Fusion', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Runze Fang, Yawen Li, Yingxia Shao, Zeli Guan, Zhe Xue', 'abstract': 'The entity alignment of science and technology patents aims to link the\\nequivalent entities in the knowledge graph of different science and technology\\npatent data sources. Most entity alignment methods only use graph neural\\nnetwork to obtain the embedding of graph structure or use attribute text\\ndescription to obtain semantic representation, ignoring the process of\\nmulti-information fusion in science and technology patents. In order to make\\nuse of the graphic structure and auxiliary information such as the name,\\ndescription and attribute of the patent entity, this paper proposes an entity\\nalignment method based on the graph convolution network for science and\\ntechnology patent information fusion. Through the graph convolution network and\\nBERT model, the structure information and entity attribute information of the\\nscience and technology patent knowledge graph are embedded and represented to\\nachieve multi-information fusion, thus improving the performance of entity\\nalignment. Experiments on three benchmark data sets show that the proposed\\nmethod Hit@K The evaluation indicators are better than the existing methods.'}\n",
            "{'title': 'Data Science in Perspective', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Rogerio Rossi', 'abstract': 'Data and Science has stood out in the generation of results, whether in the\\nprojects of the scientific domain or business domain. CERN Project, Scientific\\nInstitutes, companies like Walmart, Google, Apple, among others, need data to\\npresent their results and make predictions in the competitive data world. Data\\nand Science are words that together culminated in a globally recognized term\\ncalled Data Science. Data Science is in its initial phase, possibly being part\\nof formal sciences and also being presented as part of applied sciences,\\ncapable of generating value and supporting decision making. Data Science\\nconsiders science and, consequently, the scientific method to promote decision\\nmaking through data intelligence. In many cases, the application of the method\\n(or part of it) is considered in Data Science projects in scientific domain\\n(social sciences, bioinformatics, geospatial projects) or business domain\\n(finance, logistic, retail), among others. In this sense, this article\\naddresses the perspectives of Data Science as a multidisciplinary area,\\nconsidering science and the scientific method, and its formal structure which\\nintegrate Statistics, Computer Science, and Business Science, also taking into\\naccount Artificial Intelligence, emphasizing Machine Learning, among others.\\nThe article also deals with the perspective of applied Data Science, since Data\\nScience is used for generating value through scientific and business projects.\\nData Science persona is also discussed in the article, concerning the education\\nof Data Science professionals and its corresponding profiles, since its\\nprojection changes the field of data in the world.'}\n",
            "{'title': 'Research on Cross-media Science and Technology Information Data\\n  Retrieval', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Yang Jiang, Zhe Xue, Ang Li', 'abstract': \"Since the era of big data, the Internet has been flooded with all kinds of\\ninformation. Browsing information through the Internet has become an integral\\npart of people's daily life. Unlike the news data and social data in the\\nInternet, the cross-media technology information data has different\\ncharacteristics. This data has become an important basis for researchers and\\nscholars to track the current hot spots and explore the future direction of\\ntechnology development. As the volume of science and technology information\\ndata becomes richer, the traditional science and technology information\\nretrieval system, which only supports unimodal data retrieval and uses outdated\\ndata keyword matching model, can no longer meet the daily retrieval needs of\\nscience and technology scholars. Therefore, in view of the above research\\nbackground, it is of profound practical significance to study the cross-media\\nscience and technology information data retrieval system based on deep semantic\\nfeatures, which is in line with the development trend of domestic and\\ninternational technologies.\"}\n",
            "{'title': 'Information Distance: New Developments', 'venue/journal/conference': 'arxiv', 'year': '2012', 'authors': 'P. M. B. Vitanyi', 'abstract': 'In pattern recognition, learning, and data mining one obtains information\\nfrom information-carrying objects. This involves an objective definition of the\\ninformation in a single object, the information to go from one object to\\nanother object in a pair of objects, the information to go from one object to\\nany other object in a multiple of objects, and the shared information between\\nobjects. This is called \"information distance.\" We survey a selection of new\\ndevelopments in information distance.'}\n",
            "{'title': 'Comment: Quantifying Information Loss in Survival Studies', 'venue/journal/conference': 'arxiv', 'year': '2011', 'authors': 'Hani Doss', 'abstract': 'Comment on \"Quantifying the Fraction of Missing Information for Hypothesis\\nTesting in Statistical and Genetic Studies\" [arXiv:1102.2774]'}\n",
            "{'title': 'MatSciRE: Leveraging Pointer Networks to Automate Entity and Relation\\n  Extraction for Material Science Knowledge-base Construction', 'venue/journal/conference': 'arxiv', 'year': '2024', 'authors': 'Ankan Mullick, Akash Ghosh, G Sai Chaitanya, Samir Ghui, Tapas Nayak, Seung-Cheol Lee, Satadeep Bhattacharjee, Pawan Goyal', 'abstract': 'Material science literature is a rich source of factual information about\\nvarious categories of entities (like materials and compositions) and various\\nrelations between these entities, such as conductivity, voltage, etc.\\nAutomatically extracting this information to generate a material science\\nknowledge base is a challenging task. In this paper, we propose MatSciRE\\n(Material Science Relation Extractor), a Pointer Network-based encoder-decoder\\nframework, to jointly extract entities and relations from material science\\narticles as a triplet ($entity1, relation, entity2$). Specifically, we target\\nthe battery materials and identify five relations to work on - conductivity,\\ncoulombic efficiency, capacity, voltage, and energy. Our proposed approach\\nachieved a much better F1-score (0.771) than a previous attempt using\\nChemDataExtractor (0.716). The overall graphical framework of MatSciRE is shown\\nin Fig 1. The material information is extracted from material science\\nliterature in the form of entity-relation triplets using MatSciRE.'}\n",
            "{'title': 'Information Geometry and Evolutionary Game Theory', 'venue/journal/conference': 'arxiv', 'year': '2009', 'authors': 'Marc Harper', 'abstract': 'The Shahshahani geometry of evolutionary game theory is realized as the\\ninformation geometry of the simplex, deriving from the Fisher information\\nmetric of the manifold of categorical probability distributions. Some essential\\nconcepts in evolutionary game theory are realized information-theoretically.\\nResults are extended to the Lotka-Volterra equation and to multiple population\\nsystems.'}\n",
            "{'title': 'Comments on \"Characteristic matrix of covering and its application to\\n  Boolean matrix decomposition[Information Sciences 263(1), 186-197, 2014]\"', 'venue/journal/conference': 'arxiv', 'year': '2016', 'authors': 'Guangming Lang', 'abstract': 'In this note, we show some improvements for Theorem 7 and Example 8 in\\nShiping Wang[Information Sciences 263(1), 186-197, 2014]. Concretely, we study\\nfurther the sixth lower and upper approximations of sets for covering\\napproximation spaces. Furthermore, we present the sixth dual lower and upper\\napproximations of sets for covering approximation spaces. We also construct the\\nsixth dual lower and upper approximations of sets from the view of matrix.\\nThroughout, we use the same notations as Shiping Wang[Information Sciences\\n263(1), 186-197, 2014].'}\n",
            "{'title': 'Optimal Control Strategies in Delayed Sharing Information Structures', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'Ashutosh Nayyar, Aditya Mahajan, Demosthenis Teneketzis', 'abstract': 'The $n$-step delayed sharing information structure is investigated. This\\ninformation structure comprises of $K$ controllers that share their information\\nwith a delay of $n$ time steps. This information structure is a link between\\nthe classical information structure, where information is shared perfectly\\nbetween the controllers, and a non-classical information structure, where there\\nis no \"lateral\" sharing of information among the controllers. Structural\\nresults for optimal control strategies for systems with such information\\nstructures are presented. A sequential methodology for finding the optimal\\nstrategies is also derived. The solution approach provides an insight for\\nidentifying structural results and sequential decomposition for general\\ndecentralized stochastic control problems.'}\n",
            "{'title': 'Reinforcement Learning-driven Information Seeking: A Quantum\\n  Probabilistic Approach', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Amit Kumar Jaiswal, Haiming Liu, Ingo Frommholz', 'abstract': \"Understanding an information forager's actions during interaction is very\\nimportant for the study of interactive information retrieval. Although\\ninformation spread in uncertain information space is substantially complex due\\nto the high entanglement of users interacting with information objects~(text,\\nimage, etc.). However, an information forager, in general, accompanies a piece\\nof information (information diet) while searching (or foraging) alternative\\ncontents, typically subject to decisive uncertainty. Such types of uncertainty\\nare analogous to measurements in quantum mechanics which follow the uncertainty\\nprinciple. In this paper, we discuss information seeking as a reinforcement\\nlearning task. We then present a reinforcement learning-based framework to\\nmodel forager exploration that treats the information forager as an agent to\\nguide their behaviour. Also, our framework incorporates the inherent\\nuncertainty of the foragers' action using the mathematical formalism of quantum\\nmechanics.\"}\n",
            "{'title': 'Computer Science', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Mahyuddin K. M. Nasution, Rahmat Hidayat, Rahmad Syah', 'abstract': 'Possible for science itself, conceptually, to have and will understand\\ndifferently, let alone science also seen as technology, such as computer\\nscience. After all, science and technology are viewpoints diverse by either\\nindividual, community, or social. Generally, it depends on socioeconomic\\ncapabilities. So it is with computer science has become a phenomenon and\\nfashionable, where based on the stream of documents, various issues arise in\\neither its theory or implementation, adapting different communities, or\\ndesigning curriculum holds in the education system.'}\n",
            "{'title': 'Informaticology: combining Computer Science, Data Science, and Fiction\\n  Science', 'venue/journal/conference': 'arxiv', 'year': '2012', 'authors': 'Jan A. Bergstra', 'abstract': 'Motivated by an intention to remedy current complications with Dutch\\nterminology concerning informatics, the term informaticology is positioned to\\ndenote an academic counterpart of informatics where informatics is conceived of\\nas a container for a coherent family of practical disciplines ranging from\\ncomputer engineering and software engineering to network technology, data\\ncenter management, information technology, and information management in a\\nbroad sense.\\n  Informaticology escapes from the limitations of instrumental objectives and\\nthe perspective of usage that both restrict the scope of informatics. That is\\nachieved by including fiction science in informaticology and by ranking fiction\\nscience on equal terms with computer science and data science, and framing (the\\nstudy of) game design, evelopment, assessment and distribution, ranging from\\nserious gaming to entertainment gaming, as a chapter of fiction science. A\\nsuggestion for the scope of fiction science is specified in some detail.\\n  In order to illustrate the coherence of informaticology thus conceived, a\\npotential application of fiction to the ontology of instruction sequences and\\nto software quality assessment is sketched, thereby highlighting a possible\\nrole of fiction (science) within informaticology but outside gaming.'}\n",
            "{'title': 'Quantum Information Science from the Perspective of a Device and\\n  Materials Engineer', 'venue/journal/conference': 'arxiv', 'year': '2004', 'authors': 'S. Bandyopadhyay', 'abstract': 'Quantum information science is presented in a fashion that may be useful to a\\ndevice or materials engineer. Spintronic implementations are emphasized.'}\n",
            "{'title': 'Supporting Information for Enthalpy of formation for Cu-Zn-Sn-S (CZTS)', 'venue/journal/conference': 'arxiv', 'year': '2014', 'authors': 'Sergey V. Baryshev, Elijah Thimsen', 'abstract': 'This is Supporting Information for Enthalpy of formation for Cu-Zn-Sn-S\\n(CZTS)'}\n",
            "{'title': 'Information Security Games: A Survey', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Erick Galinkin', 'abstract': 'We introduce some preliminaries about game theory and information security.\\nThen surveying a subset of the literature, we identify opportunities for future\\nresearch.'}\n",
            "{'title': 'Control Flow Information Analysis in Process Model Matching Techniques', 'venue/journal/conference': 'arxiv', 'year': '2017', 'authors': 'Christopher Klinkmüler, Ingo Weber', 'abstract': 'Online Appendix to: \"Analyzing Control Flow Information to Improve the\\nEffectiveness of Process Model Matching Techniques\" by the same authors.'}\n",
            "{'title': 'Understanding the Information needs of Social Scientists in Germany', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Dagmar Kern, Daniel Hienert', 'abstract': 'The information needs of social science researchers are manifold and almost\\nstudied in every decade since the 1950s. With this paper, we contribute to this\\nseries and present the results of three studies. We asked 367 social science\\nresearchers in Germany for their information needs and identified needs in\\ndifferent categories: literature, research data, measurement instruments,\\nsupport for data analysis, support for data collection, variables in research\\ndata, software support, networking/cooperation, and illustrative material.\\nThereby, the search for literature and research data is still the main\\ninformation need with more than three-quarter of our participants expressing\\nneeds in these categories. With comprehensive lists of altogether 154 concrete\\ninformation needs, even those that are only expressed by one participant, we\\ncontribute to the holistic understanding of the information needs of social\\nscience researchers of today.'}\n",
            "{'title': 'Event Coreference Resolution via a Multi-loss Neural Network without\\n  Using Argument Information', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Xinyu Zuo, Yubo Chen, Kang Liu, Jun Zhao', 'abstract': 'Event coreference resolution(ECR) is an important task in Natural Language\\nProcessing (NLP) and nearly all the existing approaches to this task rely on\\nevent argument information. However, these methods tend to suffer from error\\npropagation from the stage of event argument extraction. Besides, not every\\nevent mention contains all arguments of an event, and argument information may\\nconfuse the model that events have arguments to detect event coreference in\\nreal text. Furthermore, the context information of an event is useful to infer\\nthe coreference between events. Thus, in order to reduce the errors propagated\\nfrom event argument extraction and use context information effectively, we\\npropose a multi-loss neural network model that does not need any argument\\ninformation to do the within-document event coreference resolution task and\\nachieve a significant performance than the state-of-the-art methods.'}\n",
            "{'title': 'Science Models as Value-Added Services for Scholarly Information Systems', 'venue/journal/conference': 'arxiv', 'year': '2011', 'authors': 'Peter Mutschke, Philipp Mayr, Philipp Schaer, York Sure', 'abstract': 'The paper introduces scholarly Information Retrieval (IR) as a further\\ndimension that should be considered in the science modeling debate. The IR use\\ncase is seen as a validation model of the adequacy of science models in\\nrepresenting and predicting structure and dynamics in science. Particular\\nconceptualizations of scholarly activity and structures in science are used as\\nvalue-added search services to improve retrieval quality: a co-word model\\ndepicting the cognitive structure of a field (used for query expansion), the\\nBradford law of information concentration, and a model of co-authorship\\nnetworks (both used for re-ranking search results). An evaluation of the\\nretrieval quality when science model driven services are used turned out that\\nthe models proposed actually provide beneficial effects to retrieval quality.\\nFrom an IR perspective, the models studied are therefore verified as expressive\\nconceptualizations of central phenomena in science. Thus, it could be shown\\nthat the IR perspective can significantly contribute to a better understanding\\nof scholarly structures and activities.'}\n",
            "{'title': 'The Operationalization of \"Fields\" as WoS Subject Categories (WCs) in\\n  Evaluative Bibliometrics: The cases of \"Library and Information Science\" and\\n  \"Science & Technology Studies\"', 'venue/journal/conference': 'arxiv', 'year': '2014', 'authors': 'Loet Leydesdorff, Lutz Bornmann', 'abstract': 'Normalization of citation scores using reference sets based on Web-of-Science\\nSubject Categories (WCs) has become an established (\"best\") practice in\\nevaluative bibliometrics. For example, the Times Higher Education World\\nUniversity Rankings are, among other things, based on this operationalization.\\nHowever, WCs were developed decades ago for the purpose of information\\nretrieval and evolved incrementally with the database; the classification is\\nmachine-based and partially manually corrected. Using the WC \"information\\nscience & library science\" and the WCs attributed to journals in the field of\\n\"science and technology studies,\" we show that WCs do not provide sufficient\\nanalytical clarity to carry bibliometric normalization in evaluation practices\\nbecause of \"indexer effects.\" Can the compliance with \"best practices\" be\\nreplaced with an ambition to develop \"best possible practices\"? New research\\nquestions can then be envisaged.'}\n",
            "{'title': 'The role of immersive informal science programs', 'venue/journal/conference': 'arxiv', 'year': '2004', 'authors': 'Jacob Noel-Storr', 'abstract': 'Immersive informal environments (such as summer camps, residential programs\\nat museums and science centers, etc.) can provide a venue for young people to\\nexplore their scientific thinking in a less formalized context than is\\navailable in most traditional classrooms. While class instruction is beneficial\\nfor children to develop formal science skills and content knowledge, venues\\nthat offer more opportunities for experimentation and exploration can promote\\ndeeper understandings. In this article I explore the background of science\\nlearning and venues where this learning can take place followed by a review of\\nthe benefits and necessary components of well designed immersive informal\\nprograms.'}\n",
            "{'title': 'Quantum information science as an approach to complex quantum systems', 'venue/journal/conference': 'arxiv', 'year': '2002', 'authors': 'Michael A. Nielsen', 'abstract': \"What makes quantum information science a science? These notes explore the\\nidea that quantum information science may offer a powerful approach to the\\nstudy of complex quantum systems. We discuss how to quantify complexity in\\nquantum systems, and argue that there are two qualitatively different types of\\ncomplex quantum system. We also explore ways of understanding complex quantum\\ndynamics by quantifying the strength of a quantum dynamical operation as a\\nphysical resource. This is the text for a talk at the ``Sixth International\\nConference on Quantum Communication, Measurement and Computing'', held at MIT,\\nJuly 2002. Viewgraphs for the talk may be found at http://www.qinfo.org/talks/.\"}\n",
            "{'title': 'Some results on a $χ$-divergence, an~extended~Fisher information\\n  and~generalized~Cramér-Rao inequalities', 'venue/journal/conference': 'arxiv', 'year': '2013', 'authors': 'Jean-François Bercher', 'abstract': \"We propose a modified $\\\\chi^{\\\\beta}$-divergence, give some of its properties,\\nand show that this leads to the definition of a generalized Fisher information.\\nWe give generalized Cram\\\\'er-Rao inequalities, involving this Fisher\\ninformation, an extension of the Fisher information matrix, and arbitrary norms\\nand power of the estimation error. In the case of a location parameter, we\\nobtain new characterizations of the generalized $q$-Gaussians, for instance as\\nthe distribution with a given moment that minimizes the generalized Fisher\\ninformation. Finally we indicate how the generalized Fisher information can\\nlead to new uncertainty relations.\"}\n",
            "{'title': 'Information and Set Algebras: Interpretation and Uniqueness of\\n  Conditional Independence', 'venue/journal/conference': 'arxiv', 'year': '2018', 'authors': 'Juerg Kohlas', 'abstract': 'A new seemingly weak axiomatic formulation of information algebras is given.\\nIt is shown how such information algebras can be embedded into set\\n(information) algebras. In set algebras there is a natural relation of\\nconditional independence between partitions. Via the embedding of information\\nalgebras this relation carries over to information algebras. The new axiomatic\\nformulation is thereby shown to be equivalent to the one given in\\narXiv:1701.02658. In this way the abstract concept of conditional independence\\nin information algebras gets a concrete interpretation in terms of set\\ntheoretical relations.'}\n",
            "{'title': 'Square Kilometre Array Science Data Challenge 1', 'venue/journal/conference': 'arxiv', 'year': '2018', 'authors': 'Anna Bonaldi, Robert Braun', 'abstract': \"The Square Kilometre Array (SKA, https://skatelescope.org) will be the\\nworld's largest radio telescope. SKA Science Data Challenges will be regularly\\nissued to the community as part of the science preparatory activities. The\\npurpose of these challenges is to inform the development of the data reduction\\nworkflows, to allow the science community to get familiar with the standard\\nproducts the SKA will deliver, and optimise their analyses to extract science\\nfrom them. These challenges may consist of real data from currently operating\\nradio facilities or of simulated SKA data. The purpose of this document is to\\nprovide information on how the SKA Science data challenge #1 (SDC1) has been\\nproduced and to set the challenge for the community. For more information on\\nhow to take part in the challenge and to download the data see\\nhttps://astronomers.skatelescope.org/ska-science-data-challenge-1/\"}\n",
            "{'title': 'Science as a Public Good: Public Use and Funding of Science', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Yian Yin, Yuxiao Dong, Kuansan Wang, Dashun Wang, Benjamin F. Jones', 'abstract': \"Knowledge of how science is consumed in public domains is essential for a\\ndeeper understanding of the role of science in human society. While science is\\nheavily supported by public funding, common depictions suggest that scientific\\nresearch remains an isolated or 'ivory tower' activity, with weak connectivity\\nto public use, little relationship between the quality of research and its\\npublic use, and little correspondence between the funding of science and its\\npublic use. This paper introduces a measurement framework to examine public\\ngood features of science, allowing us to study public uses of science, the\\npublic funding of science, and how use and funding relate. Specifically, we\\nintegrate five large-scale datasets that link scientific publications from all\\nscientific fields to their upstream funding support and downstream public uses\\nacross three public domains - government documents, the news media, and\\nmarketplace invention. We find that the public uses of science are extremely\\ndiverse, with different public domains drawing distinctively across scientific\\nfields. Yet amidst these differences, we find key forms of alignment in the\\ninterface between science and society. First, despite concerns that the public\\ndoes not engage high-quality science, we find universal alignment, in each\\nscientific field and public domain, between what the public consumes and what\\nis highly impactful within science. Second, despite myriad factors underpinning\\nthe public funding of science, the resulting allocation across fields presents\\na striking alignment with the field's collective public use. Overall, public\\nuses of science present a rich landscape of specialized consumption, yet\\ncollectively science and society interface with remarkable, quantifiable\\nalignment between scientific use, public use, and funding.\"}\n",
            "{'title': 'A Bibliometric Perspective of Social Science Scientific Communities of\\n  Pakistan and India', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Sami Ul-Haq, Saeed-Ul Hassan', 'abstract': 'In this study, we use research publication data from the field of social\\nscience to identify collaboration networks among social science research\\ncommunities of India and Pakistan. We have used Scopus database to extract\\ninformation of social science journals for both countries India and Pakistan.\\nStudy of this data is significant as both countries have common social issues\\nand many of common social values. Keywords analysis has been done to see common\\nresearch areas in both communities like poverty, education, the issue of gender\\netc. Despite having many of the common social issues, collaboration among\\nsocial science research communities of both countries is not strong.'}\n",
            "{'title': 'On bounds of Tsallis relative entropy and an inequality for generalized\\n  skew information', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'Shigeru Furuichi', 'abstract': 'Quantum entropy and skew information play important roles in quantum\\ninformation science. They are defined by the trace of the positive operators so\\nthat the trace inequalities often have important roles to develop the\\nmathematical theory in quantum information science. In this paper, we study\\nsome properties for information quantities in quantum system through trace\\ninequalities. Especially, we give upper bounds and lower bounds of Tsallis\\nrelative entropy, which is a one-parameter extension of the relative entropy in\\nquantum system. In addition, we compare the known bounds and the new bounds,\\nfor both upper and lower bounds, respectively. We also give an inequality for\\ngeneralized skew information by introducing a generalized correlation measure.'}\n",
            "{'title': 'Linking Social Networking Sites to Scholarly Information Portals by\\n  ScholarLib', 'venue/journal/conference': 'arxiv', 'year': '2012', 'authors': 'Peter Mutschke, Mark Thamm', 'abstract': 'Online Social Networks usually provide no or limited way to access scholarly\\ninformation provided by Digital Libraries (DLs) in order to share and discuss\\nscholarly content with other online community members. The paper addresses the\\npotentials of Social Networking sites (SNSs) for science and proposes initial\\nuse cases as well as a basic bi-directional model called ScholarLib for linking\\nSNSs to scholarly DLs. The major aim of ScholarLib is to make scholarly\\ninformation provided by DLs accessible at SNSs, and vice versa, to enhance\\nretrieval quality at DL side by social information provided by SNSs.'}\n",
            "{'title': 'Neurocognitive Informatics Manifesto', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Włodzisław Duch', 'abstract': 'Informatics studies all aspects of the structure of natural and artificial\\ninformation systems. Theoretical and abstract approaches to information have\\nmade great advances, but human information processing is still unmatched in\\nmany areas, including information management, representation and understanding.\\nNeurocognitive informatics is a new, emerging field that should help to improve\\nthe matching of artificial and natural systems, and inspire better\\ncomputational algorithms to solve problems that are still beyond the reach of\\nmachines. In this position paper examples of neurocognitive inspirations and\\npromising directions in this area are given.'}\n",
            "{'title': 'Rational Aversion to Information', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Sven Neth', 'abstract': \"Is more information always better? Or are there some situations in which more\\ninformation can make us worse off? Good (1967) argues that expected utility\\nmaximizers should always accept more information if the information is\\ncost-free and relevant. But Good's argument presupposes that you are certain\\nyou will update by conditionalization. If we relax this assumption and allow\\nagents to be uncertain about updating, these agents can be rationally required\\nto reject free and relevant information. Since there are good reasons to be\\nuncertain about updating, rationality can require you to prefer ignorance.\"}\n",
            "{'title': 'Awareness of Predatory Journals in Library and Information Science\\n  Faculties in India', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Madhuri Kumari, Subaveerapandiyan A', 'abstract': 'Predatory journals that pretended to resemble refereed journals but are used\\nfor money-making purposes. Predatory publishers produce less quality scientific\\nand research papers; it is a severe academic threat in scientific publications.\\nResearchers are ensuring the quality of the journal and peer-reviewing process\\nbefore submitting the manuscript. This paper aims to know the Indian Library\\nand Information Science faculties awareness and knowledge about Predatory\\njournals.'}\n",
            "{'title': 'Personal Information Databases', 'venue/journal/conference': 'arxiv', 'year': '2009', 'authors': 'Sabah S. Al-Fedaghi, Bernhard Thalheim', 'abstract': 'One of the most important aspects of security organization is to establish a\\nframework to identify security significant points where policies and procedures\\nare declared. The (information) security infrastructure comprises entities,\\nprocesses, and technology. All are participants in handling information, which\\nis the item that needs to be protected. Privacy and security information\\ntechnology is a critical and unmet need in the management of personal\\ninformation. This paper proposes concepts and technologies for management of\\npersonal information. Two different types of information can be distinguished:\\npersonal information and nonpersonal information. Personal information can be\\neither personal identifiable information (PII), or nonidentifiable information\\n(NII). Security, policy, and technical requirements can be based on this\\ndistinction. At the conceptual level, PII is defined and formalized by\\npropositions over infons (discrete pieces of information) that specify\\ntransformations in PII and NII. PII is categorized into simple infons that\\nreflect the proprietor s aspects, relationships with objects, and relationships\\nwith other proprietors. The proprietor is the identified person about whom the\\ninformation is communicated. The paper proposes a database organization that\\nfocuses on the PII spheres of proprietors. At the design level, the paper\\ndescribes databases of personal identifiable information built exclusively for\\nthis type of information, with their own conceptual scheme, system management,\\nand physical structure.'}\n",
            "{'title': 'Emergence as the conversion of information: A unifying theory', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Thomas Varley, Erik Hoel', 'abstract': 'Is reduction always a good scientific strategy? Does it always lead to a gain\\nin information? The very existence of the special sciences above and beyond\\nphysics seems to hint no. Previous research has shown that dimension reduction\\n(macroscales) can increase the dependency between elements of a system (a\\nphenomenon called \"causal emergence\"). However, this has been shown only for\\nspecific measures like effective information or integrated information. Here,\\nwe provide an umbrella mathematical framework for emergence based on\\ninformation conversion. Specifically, we show evidence that a macroscale can\\nhave more of a certain type of information than its underlying microscale. This\\nis because macroscales can convert information from one type to another. In\\nsuch cases, reduction to a microscale means the loss of this type of\\ninformation. We demonstrate this using the well-understood mutual information\\nmeasure applied to Boolean networks. By using the partial information\\ndecomposition, the mutual information can be decomposed into redundant, unique,\\nand synergistic information atoms. Then by introducing a novel measure of the\\nsynergy bias of a given decomposition, we are able to show that the synergy\\ncomponent of a Boolean network\\'s mutual information can increase at\\nmacroscales. This can occur even when there is no difference in the total\\nmutual information between a macroscale and its underlying microscale, proving\\ninformation conversion. We relate this broad framework to previous work,\\ncompare it to other theories, and argue it complexifies any notion of universal\\nreduction in the sciences, since such reduction would likely lead to a loss of\\nsynergistic information in scientific models.'}\n",
            "{'title': 'On the Dynamic Statistical Information Theory', 'venue/journal/conference': 'arxiv', 'year': '2005', 'authors': 'Xing Xiu-San', 'abstract': \"We extend present Shannon's static statistical information theory to dynamic\\nprocesses and establish a dynamic statistical information theory. We derive the\\nnonlinear evolution equations of dynamic information density and dynamic\\ninformation entropy density. We present the expressions of drift information\\nflow and diffusion information flow, the formulas of information entropy\\nproduction rate and information dissipation rate. The information dissipation\\nrate is equal to the information entropy production rate in a same dynamic\\nsystem. Information diffusion and information dissipation occur at the same\\ntime. We obtain the dynamic mutual information and dynamic channel capacity\\nreflecting the dynamic dissipation character in the transmission process. These\\nderivations and results are unified and rigorous from evolution equations of\\ndynamic information and dynamic information entropy without adding any extra\\nassumption. Two actual dynamic topics are discussed.\"}\n",
            "{'title': 'The use of information theory in evolutionary biology', 'venue/journal/conference': 'arxiv', 'year': '2011', 'authors': 'Christoph Adami', 'abstract': 'Information is a key concept in evolutionary biology. Information is stored\\nin biological organism\\'s genomes, and used to generate the organism as well as\\nto maintain and control it. Information is also \"that which evolves\". When a\\npopulation adapts to a local environment, information about this environment is\\nfixed in a representative genome. However, when an environment changes,\\ninformation can be lost. At the same time, information is processed by animal\\nbrains to survive in complex environments, and the capacity for information\\nprocessing also evolves. Here I review applications of information theory to\\nthe evolution of proteins as well as to the evolution of information processing\\nin simulated agents that adapt to perform a complex task.'}\n",
            "{'title': 'A visual introduction to information theory', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Henry Pinkard, Laura Waller', 'abstract': 'Though originally developed for communications engineering, information\\ntheory contains mathematical tools with numerous applications in science and\\nengineering. These tools can be used to characterize the fundamental limits of\\ndata compression and transmission in the presence of noise. Here, we present a\\npractical guide to key concepts in information theory, focusing on intuitions\\nand providing visual explanations wherever possible. Our presentation assumes\\nonly a familiarity with basic probability theory.'}\n",
            "{'title': 'QIS-XML: An Extensible Markup Language for Quantum Information Science', 'venue/journal/conference': 'arxiv', 'year': '2011', 'authors': 'Pascal Heus, Richard Gomez', 'abstract': 'This Master thesis examines issues of interoperability and integration\\nbetween the Classic Information Science (CIS) and Quantum Information Science\\n(QIS). It provides a short introduction to the Extensible Markup Language (XML)\\nand proceeds to describe the development steps that have lead to a prototype\\nXML specification for quantum computing (QIS-XML). QIS-XML is a proposed\\nframework, based on the widely used standard (XML) to describe, visualize,\\nexchange and process quantum gates and quantum circuits. It also provides a\\npotential approach to a generic programming language for quantum computers\\nthrough the concept of XML driven compilers. Examples are provided for the\\ndescription of commonly used quantum gates and circuits, accompanied with tools\\nto visualize them in standard web browsers. An algorithmic example is also\\npresented, performing a simple addition operation with quantum circuits and\\nrunning the program on a quantum computer simulator. Overall, this initial\\neffort demonstrates how XML technologies could be at the core of the\\narchitecture for describing and programming quantum computers. By leveraging a\\nwidely accepted standard, QIS-XML also builds a bridge between classic and\\nquantum IT, which could foster the acceptance of QIS by the ICT community and\\nfacilitate the understanding of quantum technology by IT experts. This would\\nsupport the consolidation of Classic Information Science and Quantum\\nInformation Science into a Complete Information Science, a challenge that could\\nbe referred to as the \"Information Science Grand Unification Challenge\".'}\n",
            "{'title': 'Transport in gapped bilayer graphene: the role of potential fluctuations\\n  (Supplementary Information)', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'K. Zou, J. Zhu', 'abstract': 'Online Supplementary Information for arXiv:1008.0783'}\n",
            "{'title': 'Weight distributions, zeta functions and Riemann hypothesis for linear\\n  and algebraic geometry codes', 'venue/journal/conference': 'arxiv', 'year': '2018', 'authors': 'Artur Elezi, Tony Shaska', 'abstract': 'This is a survey on weight enumerators, zeta functions and Riemann hypothesis\\nfor linear and algebraic-geometry codes.'}\n",
            "{'title': 'Are LLMs Ready for Real-World Materials Discovery?', 'venue/journal/conference': 'arxiv', 'year': '2024', 'authors': 'Santiago Miret, N M Anoop Krishnan', 'abstract': 'Large Language Models (LLMs) create exciting possibilities for powerful\\nlanguage processing tools to accelerate research in materials science. While\\nLLMs have great potential to accelerate materials understanding and discovery,\\nthey currently fall short in being practical materials science tools. In this\\nposition paper, we show relevant failure cases of LLMs in materials science\\nthat reveal current limitations of LLMs related to comprehending and reasoning\\nover complex, interconnected materials science knowledge. Given those\\nshortcomings, we outline a framework for developing Materials Science LLMs\\n(MatSci-LLMs) that are grounded in materials science knowledge and hypothesis\\ngeneration followed by hypothesis testing. The path to attaining performant\\nMatSci-LLMs rests in large part on building high-quality, multi-modal datasets\\nsourced from scientific literature where various information extraction\\nchallenges persist. As such, we describe key materials science information\\nextraction challenges which need to be overcome in order to build large-scale,\\nmulti-modal datasets that capture valuable materials science knowledge.\\nFinally, we outline a roadmap for applying future MatSci-LLMs for real-world\\nmaterials discovery via: 1. Automated Knowledge Base Generation; 2. Automated\\nIn-Silico Material Design; and 3. MatSci-LLM Integrated Self-Driving Materials\\nLaboratories.'}\n",
            "{'title': 'Ontology Based Query Expansion Using Word Sense Disambiguation', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'M. Barathi, S. Valli', 'abstract': \"The existing information retrieval techniques do not consider the context of\\nthe keywords present in the user's queries. Therefore, the search engines\\nsometimes do not provide sufficient information to the users. New methods based\\non the semantics of user keywords must be developed to search in the vast web\\nspace without incurring loss of information. The semantic based information\\nretrieval techniques need to understand the meaning of the concepts in the user\\nqueries. This will improve the precision-recall of the search results.\\nTherefore, this approach focuses on the concept based semantic information\\nretrieval. This work is based on Word sense disambiguation, thesaurus WordNet\\nand ontology of any domain for retrieving information in order to capture the\\ncontext of particular concept(s) and discover semantic relationships between\\nthem.\"}\n",
            "{'title': 'The Information as Absolute -- 2022 ed', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'Sergey V. Shevchenko, Vladimir V. Tokarevsky', 'abstract': 'This article presents and grounds (i.e. presents proof of the existence, the\\ntruth, the self-consistence and the completeness of) the informational concept\\n(\"the Information as Absolute\" concept) in philosophy and sciences, which was\\ndeveloping from 2007 year till now. The concept defines the Information as\\nultimately common, real and fundamental \"absolute\" phenomenon, which exists as\\nabsolutely infinite set (\"Information\" Set) of elements (members) and\\ninformational (e.g., logical) links between the elements; where any element\\nitself is some informational structure also. Correspondingly, for example,\\nMatter as the substance, radiation, etc., is some system of informational\\npatterns, constituting a specific, and practically infinitesimal comparing to\\nthe Set, element \"Matter\" of the \"Information\" Set. The concept allows for the\\nresolution, or at least for a consideration on a higher level of rational\\ncomprehension, of basic ontological and epistemological problems in philosophy\\nand natural sciences; it clarifies basic fundamental notions such as space,\\ntime, energy, etc., and so is the fundamental base for real development of\\nscience.'}\n",
            "{'title': 'Information, Processes and Games', 'venue/journal/conference': 'arxiv', 'year': '2016', 'authors': 'Samson Abramsky', 'abstract': 'We survey the prospects for an Information Dynamics which can serve as the\\nbasis for a fundamental theory of information, incorporating qualitative and\\nstructural as well as quantitative aspects. We motivate our discussion with\\nsome basic conceptual puzzles: how can information increase in computation, and\\nwhat is it that we are actually computing in general? Then we survey a number\\nof the theories which have been developed within Computer Science, as partial\\nexemplifications of the kind of fundamental theory which we seek: including\\nDomain Theory, Dynamic Logic, and Process Algebra. We look at recent work\\nshowing new ways of combining quantitative and qualitative theories of\\ninformation, as embodied respectively by Domain Theory and Shannon Information\\nTheory. Then we look at Game Semantics and Geometry of Interaction, as examples\\nof dynamic models of logic and computation in which information flow and\\ninteraction are made central and explicit. We conclude by looking briefly at\\nsome key issues for future progress.'}\n",
            "{'title': 'A Digital Library for Research Data and Related Information in the\\n  Social Sciences', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Daniel Hienert, Dagmar Kern, Katarina Boland, Benjamin Zapilko, Peter Mutschke', 'abstract': 'In the social sciences, researchers search for information on the Web, but\\nthis is most often distributed on different websites, search portals, digital\\nlibraries, data archives, and databases. In this work, we present an integrated\\nsearch system for social science information that allows finding information\\naround research data in a single digital library. Users can search for research\\ndata sets, publications, survey variables, questions from questionnaires,\\nsurvey instruments, and tools. Information items are linked to each other so\\nthat users can see, for example, which publications contain data citations to\\nresearch data. The integration and linking of different kinds of information\\nincrease their visibility so that it is easier for researchers to find\\ninformation for re-use. In a log-based usage study, we found that users search\\nacross different information types, that search sessions contain a high rate of\\npositive signals and that link information is often explored.'}\n",
            "{'title': 'Quantum Information Processing: An Essential Primer', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Emina Soljanin', 'abstract': 'Quantum information science is an exciting, wide, rapidly progressing,\\ncross-disciplinary field, and that very nature makes it both attractive and\\nhard to enter. In this primer, we first provide answers to the three essential\\nquestions that any newcomer needs to know: How is quantum information\\nrepresented? How is quantum information processed? How is classical information\\nextracted from quantum states? We then introduce the most basic quantum\\ninformation theoretic notions concerning entropy, sources, and channels, as\\nwell as secure communications and error correction. We conclude with examples\\nthat illustrate the power of quantum correlations. No prior knowledge of\\nquantum mechanics is assumed.'}\n",
            "{'title': 'Merging the Astrophysics and Planetary Science Information Systems', 'venue/journal/conference': 'arxiv', 'year': '2018', 'authors': 'Michael J. Kurtz, Alberto Accomazzi, Edwin A. Henneken', 'abstract': 'Conceptually exoplanet research has one foot in the discipline of\\nAstrophysics and the other foot in Planetary Science. Research strategies for\\nexoplanets will require efficient access to data and information from both\\nrealms. Astrophysics has a sophisticated, well integrated, distributed\\ninformation system with archives and data centers which are interlinked with\\nthe technical literature via the Astrophysics Data System (ADS). The\\ninformation system for Planetary Science does not have a central component\\nlinking the literature with the observational and theoretical data. Here we\\npropose that the Committee on an Exoplanet Science Strategy recommend that this\\nlinkage be built, with the ADS playing the role in Planetary Science which it\\nalready plays in Astrophysics. This will require additional resources for the\\nADS, and the Planetary Data System (PDS), as well as other international\\ncollaborators'}\n",
            "{'title': 'The Physical World as a Virtual Reality', 'venue/journal/conference': 'arxiv', 'year': '2008', 'authors': 'Brian Whitworth', 'abstract': 'This paper explores the idea that the universe is a virtual reality created\\nby information processing, and relates this strange idea to the findings of\\nmodern physics about the physical world. The virtual reality concept is\\nfamiliar to us from online worlds, but our world as a virtual reality is\\nusually a subject for science fiction rather than science. Yet logically the\\nworld could be an information simulation running on a multi-dimensional\\nspace-time screen. Indeed, if the essence of the universe is information,\\nmatter, charge, energy and movement could be aspects of information, and the\\nmany conservation laws could be a single law of information conservation. If\\nthe universe were a virtual reality, its creation at the big bang would no\\nlonger be paradoxical, as every virtual system must be booted up. It is\\nsuggested that whether the world is an objective reality or a virtual reality\\nis a matter for science to resolve. Modern information science can suggest how\\ncore physical properties like space, time, light, matter and movement could\\nderive from information processing. Such an approach could reconcile relativity\\nand quantum theories, with the former being how information processing creates\\nspace-time, and the latter how it creates energy and matter.'}\n",
            "{'title': 'Implicit Contextual Integrity in Online Social Networks', 'venue/journal/conference': 'arxiv', 'year': '2015', 'authors': 'Natalia Criado, Jose M. Such', 'abstract': 'Many real incidents demonstrate that users of Online Social Networks need\\nmechanisms that help them manage their interactions by increasing the awareness\\nof the different contexts that coexist in Online Social Networks and preventing\\nthem from exchanging inappropriate information in those contexts or\\ndisseminating sensitive information from some contexts to others. Contextual\\nintegrity is a privacy theory that conceptualises the appropriateness of\\ninformation sharing based on the contexts in which this information is to be\\nshared. Computational models of Contextual Integrity assume the existence of\\nwell-defined contexts, in which individuals enact pre-defined roles and\\ninformation sharing is governed by an explicit set of norms. However, contexts\\nin Online Social Networks are known to be implicit, unknown a priori and ever\\nchanging; users relationships are constantly evolving; and the information\\nsharing norms are implicit. This makes current Contextual Integrity models not\\nsuitable for Online Social Networks.\\n  In this paper, we propose the first computational model of Implicit\\nContextual Integrity, presenting an information model and an Information\\nAssistant Agent that uses the information model to learn implicit contexts,\\nrelationships and the information sharing norms to help users avoid\\ninappropriate information exchanges and undesired information disseminations.\\nThrough an experimental evaluation, we validate the properties of Information\\nAssistant Agents, which are shown to: infer the information sharing norms even\\nif a small proportion of the users follow the norms and in presence of\\nmalicious users; help reduce the exchange of inappropriate information and the\\ndissemination of sensitive information with only a partial view of the system\\nand the information received and sent by their users; and minimise the burden\\nto the users in terms of raising unnecessary alerts.'}\n",
            "{'title': 'Data visualization in political and social sciences', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'Andrei Zinovyev', 'abstract': 'The basic objective of data visualization is to provide an efficient\\ngraphical display for summarizing and reasoning about quantitative information.\\nDuring the last decades, political science has accumulated a large corpus of\\nvarious kinds of data such as comprehensive factbooks and atlases,\\ncharacterizing all or most of existing states by multiple and objectively\\nassessed numerical indicators within certain time lapse. As a consequence,\\nthere exists a continuous trend for political science to gradually become a\\nmore quantitative scientific field and to use quantitative information in the\\nanalysis and reasoning. It is believed that any objective analysis in political\\nscience must be multidimensional and combine various sources of quantitative\\ninformation; however, human capabilities for perception of large massifs of\\nnumerical information are limited. Hence, methods and approaches for\\nvisualization of quantitative and qualitative data (and, especially\\nmultivariate data) is an extremely important topic. Data visualization\\napproaches can be classified into several groups, starting from creating\\ninformative charts and diagrams (statistical graphics and infographics) and\\nending with advanced statistical methods for visualizing multidimensional\\ntables containing both quantitative and qualitative information. In this\\narticle we provide a short review of existing methods of data visualization\\nmethods with applications in political and social science.'}\n",
            "{'title': 'Seats at the table: the network of the editorial boards in information\\n  and library science', 'venue/journal/conference': 'arxiv', 'year': '2011', 'authors': 'Alberto Baccini And Lucio Barabesi', 'abstract': 'The structural properties of the network generated by the editorial\\nactivities of the members of the boards of \"Information Science & Library\\nScience\" journals are explored through network analysis techniques. The crossed\\npresence of scholars on editorial boards, the phenomenon called interlocking\\neditorship, is considered a proxy of the similarity of editorial policies. The\\nevidences support the idea that this group of journals is better described as a\\nset of only relatively connected subfields. In particular two main subfield are\\nidentified, consisting of research oriented journals devoted respectively to\\nLIS and MIS. The links between these two subsets are weak. Around these two\\nsubsets there are a lot of (relatively) isolated professional journals or\\njournals characterized more by their subject-matter content than by their focus\\non information flows. It is possible to suggest that this configuration of the\\nnetwork may be the consequence of the youthfulness of Information Science &\\nLibrary Science, which has not permitted yet to reach a general consensus\\nthrough scholars on research aims, methods and instruments.'}\n",
            "{'title': 'A unified theory of information transfer and causal relation', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Yang Tian, Hedong Hou, Yaoyuan Wang, Ziyang Zhang, Pei Sun', 'abstract': \"Information transfer between coupled stochastic dynamics, measured by\\ntransfer entropy and information flow, is suggested as a physical process\\nunderlying the causal relation of systems. While information transfer analysis\\nhas booming applications in both science and engineering fields, critical\\nmysteries about its foundations remain unsolved. Fundamental yet difficult\\nquestions concern how information transfer and causal relation originate, what\\nthey depend on, how they differ from each other, and if they are created by a\\nunified and general quantity. These questions essentially determine the\\nvalidity of causal relation measurement via information transfer. Here we\\npursue to lay a complete theoretical basis of information transfer and causal\\nrelation. Beyond the well-known relations between these concepts that\\nconditionally hold, we demonstrate that information transfer and causal\\nrelation universally originate from specific information synergy and redundancy\\nphenomena characterized by high-order mutual information. More importantly, our\\ntheory analytically explains the mechanisms for information transfer and causal\\nrelation to originate, vanish, and differ from each other. Moreover, our theory\\nnaturally defines the effect sizes of information transfer and causal relation\\nbased on high-dimensional coupling events. These results may provide a unified\\nview of information, synergy, and causal relation to bridge Pearl's causal\\ninference theory in computer science and information transfer analysis in\\nphysics.\"}\n",
            "{'title': 'Positive Affirmation of Non-Algorithmic Information Processing', 'venue/journal/conference': 'arxiv', 'year': '2017', 'authors': 'Carlos Eduardo Maldonado', 'abstract': 'One of the most compelling problems in science consists in understanding how\\nliving systems process information. After all, the way they process information\\ndefines their capacities to learning and adaptation. There is an increasing\\nconsensus in that living systems are not machines in any sense. Biological\\nhyper-computation is the concept coined that expresses that living beings\\nprocess information non-algorithmically. Maldonado and Gomez (2015) have\\nbrought up biological hyper-computation as a new problem within complexity\\nscience. This paper aims at proving a positive understanding of non-algorithmic\\nprocesses. A number of arguments are brought that support the claim. This\\nfosters, it is argued, a brand new understanding of information processing\\namong living beings. Some conclusions are drawn at the end.'}\n",
            "{'title': 'Science-Operational Metrics and Issues for the \"Are We Alone?\" Movement', 'venue/journal/conference': 'arxiv', 'year': '2009', 'authors': 'Robert A. Brown', 'abstract': 'A movement is underway to test the uniqueness of Earth. Sponsored primarily\\nby NASA, it is enlisting talented researchers from many disciplines. It is\\nconceiving new telescopes to discover and characterize other worlds like Earth\\naround nearby stars and to obtain their spectra. The goal is to search for\\nsigns of biological activity and perhaps find other cradles of life.\\n  Most effort thus far has focused on the optics to make such observations\\nfeasible. Relatively little attention has been paid to science operations--the\\nlink between instrument and science. Because of the special challenges\\npresented by extrasolar planets, science-operational issues may be limiting\\nfactors for the \"Are We Alone?\" (AWA) movement. Science-operational metrics can\\nhelp compare the merits of direct and astrometric planet searches, and estimate\\nthe concatenated completeness of searching followed by spectroscopy. This\\ncompleteness is the prime science metric of the AWA program. Therefore, the\\ngoals of this white paper are to present representative calculations involving\\nscience-operational metrics, and to promote a science-operational perspective.\\nWe urge the Survey Committee to allow this perspective and such metrics to\\ninform its plan for the future of AWA.'}\n",
            "{'title': 'Game Information System', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'Spits Warnars H. L. H', 'abstract': \"In this Information system age many organizations consider information system\\nas their weapon to compete or gain competitive advantage or give the best\\nservices for non profit organizations. Game Information System as combining\\nInformation System and game is breakthrough to achieve organizations'\\nperformance. The Game Information System will run the Information System with\\ngame and how game can be implemented to run the Information System. Game is not\\nonly for fun and entertainment, but will be a challenge to combine fun and\\nentertainment with Information System. The Challenge to run the information\\nsystem with entertainment, deliver the entertainment with information system\\nall at once. Game information system can be implemented in many sectors as like\\nthe information system itself but in difference's view. A view of game which\\npeople can joy and happy and do their transaction as a fun things.\"}\n",
            "{'title': 'Characterizing information loss in a chaotic double pendulum with the\\n  Information Bottleneck', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Kieran A. Murphy, Dani S. Bassett', 'abstract': 'A hallmark of chaotic dynamics is the loss of information with time. Although\\ninformation loss is often expressed through a connection to Lyapunov exponents\\n-- valid in the limit of high information about the system state -- this\\npicture misses the rich spectrum of information decay across different levels\\nof granularity. Here we show how machine learning presents new opportunities\\nfor the study of information loss in chaotic dynamics, with a double pendulum\\nserving as a model system. We use the Information Bottleneck as a training\\nobjective for a neural network to extract information from the state of the\\nsystem that is optimally predictive of the future state after a prescribed time\\nhorizon. We then decompose the optimally predictive information by distributing\\na bottleneck to each state variable, recovering the relative importance of the\\nvariables in determining future evolution. The framework we develop is broadly\\napplicable to chaotic systems and pragmatic to apply, leveraging data and\\nmachine learning to monitor the limits of predictability and map out the loss\\nof information.'}\n",
            "{'title': 'Crypto Makes AI Evolve', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Behrouz Zolfaghari, Elnaz Rabieinejad, Abbas Yazdinejad, Reza M. Parizi, Ali Dehghantanha', 'abstract': 'Adopting cryptography has given rise to a significant evolution in Artificial\\nIntelligence (AI). This paper studies the path and stages of this evolution. We\\nstart with reviewing existing relevant surveys, noting their shortcomings,\\nespecially the lack of a close look at the evolution process and solid future\\nroadmap. These shortcomings justify the work of this paper. Next, we identify,\\ndefine and discuss five consequent stages in the evolution path, including\\nCrypto-Sensitive AI, Crypto-Adapted AI, Crypto-Friendly AI, Crypto-Enabled AI,\\nCrypto-Protected AI. Then, we establish a future roadmap for further research\\nin this area, focusing on the role of quantum-inspired and bio-inspired AI.'}\n",
            "{'title': 'Towards Human-AI Collaborative Urban Science Research Enabled by\\n  Pre-trained Large Language Models', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Jiayi Fu, Haoying Han, Xing Su, Chao Fan', 'abstract': 'Pre-trained large language models (PLMs) have the potential to support urban\\nscience research through content creation, information extraction, assisted\\nprogramming, text classification, and other technical advances. In this\\nresearch, we explored the opportunities, challenges, and prospects of PLMs in\\nurban science research. Specifically, we discussed potential applications of\\nPLMs to urban institution, urban space, urban information, and citizen\\nbehaviors research through seven examples using ChatGPT. We also examined the\\nchallenges of PLMs in urban science research from both technical and social\\nperspectives. The prospects of the application of PLMs in urban science\\nresearch were then proposed. We found that PLMs can effectively aid in\\nunderstanding complex concepts in urban science, facilitate urban spatial form\\nidentification, assist in disaster monitoring, and sense public sentiment. At\\nthe same time, however, the applications of PLMs in urban science research face\\nevident threats, such as technical limitations, security, privacy, and social\\nbias. The development of fundamental models based on domain knowledge and\\nhuman-AI collaboration may help improve PLMs to support urban science research\\nin future.'}\n",
            "{'title': 'Missing author address information in Web of Science-An explorative\\n  study', 'venue/journal/conference': 'arxiv', 'year': '2018', 'authors': 'Weishu Liu, Guangyuan Hu, Li Tang', 'abstract': 'Bibliometric analysis is increasingly used to evaluate and compare research\\nperformance across geographical regions. However, the problem of missing\\ninformation from author addresses has not attracted sufficient attention from\\nscholars and practitioners. This study probes the missing data problem in the\\nthree core journal citation databases of Web of Science (WoS). Our findings\\nreveal that from 1900 to 2015 over one-fifth of the publications indexed in WoS\\nhave completely missing information from the address field. The magnitude of\\nthe problem varies greatly among time periods, citation databases, document\\ntypes, and publishing languages. The problem is especially serious for research\\nin the sciences and social sciences published before the early 1970s and\\nremains significant for recent publications in the arts and humanities. Further\\nexaminations suggest that many records with completely missing address\\ninformation do not represent scholarly research. Full-text scanning of a random\\nsample reveals that about 40% of the articles have some address information\\nthat is not indexed in WoS. This study also finds that the problem of partially\\nmissing address information for U.S. research has diminished dramatically since\\n1998. The paper ends by providing some discussion and tentative remedies.'}\n",
            "{'title': 'Is Consciousness Computable? Quantifying Integrated Information Using\\n  Algorithmic Information Theory', 'venue/journal/conference': 'arxiv', 'year': '2014', 'authors': 'Phil Maguire, Philippe Moser, Rebecca Maguire, Virgil Griffith', 'abstract': \"In this article we review Tononi's (2008) theory of consciousness as\\nintegrated information. We argue that previous formalizations of integrated\\ninformation (e.g. Griffith, 2014) depend on information loss. Since lossy\\nintegration would necessitate continuous damage to existing memories, we\\npropose it is more natural to frame consciousness as a lossless integrative\\nprocess and provide a formalization of this idea using algorithmic information\\ntheory. We prove that complete lossless integration requires noncomputable\\nfunctions. This result implies that if unitary consciousness exists, it cannot\\nbe modelled computationally.\"}\n",
            "{'title': 'Fundamental connections between utility theories of wealth and\\n  information theory', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Andres F. Ducuara, Paul Skrzypczyk', 'abstract': \"We establish fundamental connections between utility theories of wealth from\\nthe economic sciences and information-theoretic quantities. In particular, we\\nintroduce operational tasks based on betting where both gambler and bookmaker\\nhave access to side information, or betting tasks with double side information\\nfor short. In order to characterise these operational tasks we introduce new\\nconditional R\\\\'enyi divergences, and explore some of their properties.\\nFurthermore, we introduce an utility theory of wealth ratios, and operationally\\ninterpret there the two-parameter $(q,r)$ generalised mutual information\\nmeasure recently introduced by V. M. Ili\\\\'c and I. V. Djordjevi\\\\'c; it\\nquantifies the advantage provided by side information in betting tasks for\\nutility theories of wealth ratios. Moreover, we show that the\\nIli\\\\'c-Djordjevi\\\\'c conditional entropy satisfies a type of generalised chain\\nrule, which generalises that of Arimoto-R\\\\'enyi. Finally, we address the\\nimplications of these results on the quantum resource theories of informative\\nmeasurements and non-constant channels. Altogether, these results further help\\nstrengthening the bridge between the theory of expected utility from the\\neconomic sciences and Shannon's theory of information.\"}\n",
            "{'title': 'Privacy-Preserving Dynamic Personalized Pricing with Demand Learning', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Xi Chen, David Simchi-Levi, Yining Wang', 'abstract': \"The prevalence of e-commerce has made detailed customers' personal\\ninformation readily accessible to retailers, and this information has been\\nwidely used in pricing decisions. When involving personalized information, how\\nto protect the privacy of such information becomes a critical issue in\\npractice. In this paper, we consider a dynamic pricing problem over $T$ time\\nperiods with an \\\\emph{unknown} demand function of posted price and personalized\\ninformation. At each time $t$, the retailer observes an arriving customer's\\npersonal information and offers a price. The customer then makes the purchase\\ndecision, which will be utilized by the retailer to learn the underlying demand\\nfunction. There is potentially a serious privacy concern during this process: a\\nthird party agent might infer the personalized information and purchase\\ndecisions from price changes from the pricing system. Using the fundamental\\nframework of differential privacy from computer science, we develop a\\nprivacy-preserving dynamic pricing policy, which tries to maximize the retailer\\nrevenue while avoiding information leakage of individual customer's information\\nand purchasing decisions. To this end, we first introduce a notion of\\n\\\\emph{anticipating} $(\\\\varepsilon, \\\\delta)$-differential privacy that is\\ntailored to dynamic pricing problem. Our policy achieves both the privacy\\nguarantee and the performance guarantee in terms of regret. Roughly speaking,\\nfor $d$-dimensional personalized information, our algorithm achieves the\\nexpected regret at the order of $\\\\tilde{O}(\\\\varepsilon^{-1} \\\\sqrt{d^3 T})$,\\nwhen the customers' information is adversarially chosen. For stochastic\\npersonalized information, the regret bound can be further improved to\\n$\\\\tilde{O}(\\\\sqrt{d^2T} + \\\\varepsilon^{-2} d^2)$\"}\n",
            "{'title': 'Growth rates of modern science: A bibliometric analysis based on the\\n  number of publications and cited references', 'venue/journal/conference': 'arxiv', 'year': '2014', 'authors': 'Lutz Bornmann, Ruediger Mutz', 'abstract': 'Many studies in information science have looked at the growth of science. In\\nthis study, we re-examine the question of the growth of science. To do this we\\n(i) use current data up to publication year 2012 and (ii) analyse it across all\\ndisciplines and also separately for the natural sciences and for the medical\\nand health sciences. Furthermore, the data are analysed with an advanced\\nstatistical technique - segmented regression analysis - which can identify\\nspecific segments with similar growth rates in the history of science. The\\nstudy is based on two different sets of bibliometric data: (1) The number of\\npublications held as source items in the Web of Science (WoS, Thomson Reuters)\\nper publication year and (2) the number of cited references in the publications\\nof the source items per cited reference year. We have looked at the rate at\\nwhich science has grown since the mid-1600s. In our analysis of cited\\nreferences we identified three growth phases in the development of science,\\nwhich each led to growth rates tripling in comparison with the previous phase:\\nfrom less than 1% up to the middle of the 18th century, to 2 to 3% up to the\\nperiod between the two world wars and 8 to 9% to 2012.'}\n",
            "{'title': 'Enabling Synergy: Improving the Information Infrastructure for Planetary\\n  Science', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Michael J. Kurtz, Alberto Accomazzi, Edwin A. Henneken', 'abstract': 'In this whitepaper we advocate that the Planetary Science (PS) community\\nbuild a discipline-specific digital library, in collaboration with the existing\\nastronomy digital library, ADS. We suggest that the PS data archives increase\\ntheir level of curation to allow for direct linking between the archival data\\nand the derived journal articles. And we suggest that a new component of the PS\\ninformation infrastructure be created to collate and curate information on\\nfeatures and objects in our solar system, beginning with the USGS/IAU Gazetteer\\nof Planetary Nomenclature.'}\n",
            "{'title': 'Some Applications of Isotope - Based technologies: Human Health and\\n  Quantum Information', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'Vladimir G. Plekhanov', 'abstract': \"Technology is the sum of the information, knowledge and agency. This takes\\nenergy and information as fundamental concepts. In this paper I'll try to\\ndescribe very briefly in popular form of some applications of radioactive and\\nstable isotopes in medicine and quantum information, respectively.\"}\n",
            "{'title': 'The Fragility of Quantum Information?', 'venue/journal/conference': 'arxiv', 'year': '2013', 'authors': 'Barbara M. Terhal', 'abstract': 'We address the question whether there is a fundamental reason why quantum\\ninformation is more fragile than classical information. We show that some\\nanswers can be found by considering the existence of quantum memories and their\\ndimensional dependence.'}\n",
            "{'title': 'Consistency of the Maximal Information Coefficient Estimator', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'John Lazarsfeld, Aaron Johnson', 'abstract': 'The Maximal Information Coefficient (MIC) of Reshef et al. (Science, 2011) is\\na statistic for measuring dependence between variable pairs in large datasets.\\nIn this note, we prove that MIC is a consistent estimator of the corresponding\\npopulation statistic MIC$_*$. This corrects an error in an argument of Reshef\\net al. (JMLR, 2016), which we describe.'}\n",
            "{'title': 'Mutual Entropy in Quantum Information and Information Genetics', 'venue/journal/conference': 'arxiv', 'year': '2004', 'authors': 'Masanori Ohya', 'abstract': \"After Shannon, entropy becomes a fundamental quantity to describe not only\\nuncertainity or chaos of a system but also information carried by the system.\\nShannon's important discovery is to give a mathematical expression of the\\nmutual entropy (information), information transmitted from an input system to\\nan output system, by which communication processes could be analyzed on the\\nstage of mathematical science. In this paper, first we review the quantum\\nmutual entropy and discuss its uses in quantum information theory, and secondly\\nwe show how the classical mutual entropy can be used to analyze genomes, in\\nparticular, those of HIV.\"}\n",
            "{'title': 'Information Systems with Witnesses: The Function Space Construction', 'venue/journal/conference': 'arxiv', 'year': '2017', 'authors': 'Dieter Spreen', 'abstract': 'Information systems with witnesses have been introduced in [D. Spreen.\\nGeneralised information systems capture L-domains. arXiv:1610.02260v2] as a\\nlogic-style representation of L-domains: The category of such information\\nsystems with approximable mappings as morphisms is equivalent to the category\\nof L-domains with Scott continuous functions, which is known to be Cartesian\\nclosed. In the present paper a direct proof of the Cartesian closure of the\\ncategory of information systems with witnesses and approximable mapppings is\\ngiven. As is shown, the collection of approximable mappings between two\\ninformation systems with witnesses comes with a natural information system\\nstructure.'}\n",
            "{'title': 'Improving the data quality in the research information systems', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Otmane Azeroual, Mohammad Abuosba', 'abstract': 'In order to introduce an integrated research information system, this will\\nprovide scientific institutions with the necessary information on research\\nactivities and research results in assured quality. Since data collection,\\nduplication, missing values, incorrect formatting, inconsistencies, etc. can\\narise in the collection of research data in different research information\\nsystems, which can have a wide range of negative effects on data quality, the\\nsubject of data quality should be treated with better results. This paper\\nexamines the data quality problems in research information systems and presents\\nthe new techniques that enable organizations to improve their quality of\\nresearch information.'}\n",
            "{'title': 'A genuinely natural information measure', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Andreas Winter', 'abstract': 'The theoretical measuring of information was famously initiated by Shannon in\\nhis mathematical theory of communication, in which he proposed a now widely\\nused quantity, the entropy, measured in bits. Yet, in the same paper, Shannon\\nalso chose to measure the information in continuous systems in nats, which\\ndiffer from bits by the use of the natural rather than the binary logarithm.\\n  We point out that there is nothing natural about the choice of logarithm\\nbasis, rather it is arbitrary. We remedy this problematic state of affairs by\\nproposing a genuinely natural measure of information, which we dub gnats. We\\nshow that gnats have many advantages in information theory, and propose to\\nadopt the underlying methodology throughout science, arts and everyday life.'}\n",
            "{'title': 'Private Private Information', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Kevin He, Fedor Sandomirskiy, Omer Tamuz', 'abstract': \"A private private information structure delivers information about an unknown\\nstate while preserving privacy: An agent's signal contains information about\\nthe state but remains independent of others' sensitive or private information.\\nWe study how informative such structures can be, and characterize those that\\nare optimal in the sense that they cannot be made more informative without\\nviolating privacy. We connect our results to fairness in recommendation systems\\nand explore a number of further applications.\"}\n",
            "{'title': 'Comments on \"Routh Stability Criterion\"', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'T. D. Roopamala, S. K. Katti', 'abstract': 'In this note, we have shown special case on Routh stability criterion, which\\nis not discussed, in previous literature. This idea can be useful in computer\\nscience applications.'}\n",
            "{'title': \"The Massey's method for sport rating: a network science perspective\", 'venue/journal/conference': 'arxiv', 'year': '2017', 'authors': 'Massimo Franceschet, Enrico Bozzo', 'abstract': \"We revisit the Massey's method for rating and ranking in sports and\\ncontextualize it as a general centrality measure in network science.\"}\n",
            "{'title': 'Teaching and Learning Science with Learning Assistants', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'Andrew Ferstl', 'abstract': 'The science content classes for elementary education majors at Winona State\\nUniversity used Learning Assistants for the first time during the 2009 - 2010\\nacademic year. Pre-post information was gathered about the Learning Assistants\\nand the pre-service teachers to gauge the effect of this experience on both\\npopulations.'}\n",
            "{'title': 'Simultaneous games with purchase of randomly supplied perfect\\n  information: Oracle Games', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Matthew J. Young, Andrew Belmonte', 'abstract': 'We study the role of costly information in non-cooperative two-player games\\nwhen an extrinsic third party information broker is introduced asymmetrically,\\nallowing one player to obtain information about the other player\\'s action. This\\nbroker or \"oracle\" is defined by a probability of response, supplying correct\\ninformation randomly; the informed player can pay more for a higher probability\\nof response. We determine the necessary and sufficient conditions for strategy\\nprofiles to be equilibria, in terms of how both players change their strategies\\nin response to the existence of the oracle, as determined by its cost of\\ninformation function. For mixed strategy equilibria, there is a continuous\\nchange as information becomes cheaper, with clear transitions occuring at\\ncritical {\\\\it nodes} at which pure strategies become dominated (or\\nundominated). These nodes separate distinct responses to the information for\\nsale, alternating between regions where the paying player increases the amount\\nof information purchased, and regions where the other player moves away from\\nriskier strategies, in favor of safer bets that minimize losses. We derive\\nconditions for these responses by defining a value of information.'}\n",
            "{'title': 'On the Sample Information About Parameter and Prediction', 'venue/journal/conference': 'arxiv', 'year': '2011', 'authors': 'Nader Ebrahimi, Ehsan S. Soofi, Refik Soyer', 'abstract': \"The Bayesian measure of sample information about the parameter, known as\\nLindley's measure, is widely used in various problems such as developing prior\\ndistributions, models for the likelihood functions and optimal designs. The\\npredictive information is defined similarly and used for model selection and\\noptimal designs, though to a lesser extent. The parameter and predictive\\ninformation measures are proper utility functions and have been also used in\\ncombination. Yet the relationship between the two measures and the effects of\\nconditional dependence between the observable quantities on the Bayesian\\ninformation measures remain unexplored. We address both issues. The\\nrelationship between the two information measures is explored through the\\ninformation provided by the sample about the parameter and prediction jointly.\\nThe role of dependence is explored along with the interplay between the\\ninformation measures, prior and sampling design. For the conditionally\\nindependent sequence of observable quantities, decompositions of the joint\\ninformation characterize Lindley's measure as the sample information about the\\nparameter and prediction jointly and the predictive information as part of it.\\nFor the conditionally dependent case, the joint information about parameter and\\nprediction exceeds Lindley's measure by an amount due to the dependence. More\\nspecific results are shown for the normal linear models and a broad subfamily\\nof the exponential family. Conditionally independent samples provide relatively\\nlittle information for prediction, and the gap between the parameter and\\npredictive information measures grows rapidly with the sample size.\"}\n",
            "{'title': 'ArcMMLU: A Library and Information Science Benchmark for Large Language\\n  Models', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Shitou Zhang, Zuchao Li, Xingshen Liu, Liming Yang, Ping Wang', 'abstract': 'In light of the rapidly evolving capabilities of large language models\\n(LLMs), it becomes imperative to develop rigorous domain-specific evaluation\\nbenchmarks to accurately assess their capabilities. In response to this need,\\nthis paper introduces ArcMMLU, a specialized benchmark tailored for the Library\\n& Information Science (LIS) domain in Chinese. This benchmark aims to measure\\nthe knowledge and reasoning capability of LLMs within four key sub-domains:\\nArchival Science, Data Science, Library Science, and Information Science.\\nFollowing the format of MMLU/CMMLU, we collected over 6,000 high-quality\\nquestions for the compilation of ArcMMLU. This extensive compilation can\\nreflect the diverse nature of the LIS domain and offer a robust foundation for\\nLLM evaluation. Our comprehensive evaluation reveals that while most mainstream\\nLLMs achieve an average accuracy rate above 50% on ArcMMLU, there remains a\\nnotable performance gap, suggesting substantial headroom for refinement in LLM\\ncapabilities within the LIS domain. Further analysis explores the effectiveness\\nof few-shot examples on model performance and highlights challenging questions\\nwhere models consistently underperform, providing valuable insights for\\ntargeted improvements. ArcMMLU fills a critical gap in LLM evaluations within\\nthe Chinese LIS domain and paves the way for future development of LLMs\\ntailored to this specialized area.'}\n",
            "{'title': 'Highly-cited papers in Library and Information Science (LIS): Authors,\\n  institutions, and network structures', 'venue/journal/conference': 'arxiv', 'year': '2015', 'authors': 'Johann Bauer, Loet Leydesdorff, Lutz Bornmann', 'abstract': 'As a follow-up to the highly-cited authors list published by Thomson Reuters\\nin June 2014, we analyze the top-1% most frequently cited papers published\\nbetween 2002 and 2012 included in the Web of Science (WoS) subject category\\n\"Information Science & Library Science.\" 798 authors contributed to 305 top-1%\\npublications; these authors were employed at 275 institutions. The authors at\\nHarvard University contributed the largest number of papers, when the addresses\\nare whole-number counted. However, Leiden University leads the ranking, if\\nfractional counting is used.\\n  Twenty-three of the 798 authors were also listed as most highly-cited authors\\nby Thomson Reuters in June 2014 (http://highlycited.com/). Twelve of these 23\\nauthors were involved in publishing four or more of the 305 papers under study.\\nAnalysis of co-authorship relations among the 798 highly-cited scientists shows\\nthat co-authorships are based on common interests in a specific topic. Three\\ntopics were important between 2002 and 2012: (1) collection and exploitation of\\ninformation in clinical practices, (2) the use of internet in public\\ncommunication and commerce, and (3) scientometrics.'}\n",
            "{'title': 'Science on YouTube: What users find when they search for climate science\\n  and climate manipulation', 'venue/journal/conference': 'arxiv', 'year': '2016', 'authors': 'Joachim Allgaier', 'abstract': 'Online video-sharing sites such as YouTube are very popular and also used by\\na lot of people to obtain knowledge and information, also on science, health\\nand technology. Technically they could be valuable tools for the public\\ncommunication of science and technology, but the users of YouTube are also\\nconfronted with conspiracy theories and erroneous and misleading information\\nthat deviates from scientific consensus views. This contribution details the\\nresults of a study that investigates what kind of information users find when\\nthey are searching for climate science and climate manipulation topics on\\nYouTube and whether this information corresponds with or challenges scientific\\nconsensus views. An innovative methodological approach using the anonymization\\nnetwork Tor is introduced for drawing randomized samples of YouTube videos.\\nThis approach was used to select and examine a sample of 140 YouTube videos on\\nclimate topics.'}\n",
            "{'title': 'User Interests in German Social Science Literature Search - A Large\\n  Scale Log Analysis', 'venue/journal/conference': 'arxiv', 'year': '2018', 'authors': 'Daniel Hienert', 'abstract': 'The social sciences are a broad research field with a lot of sub- and related\\ndisciplines. Accordingly, user interests in a digital library for the social\\nsciences are manifold. In this study we analyzed nine years log data of a\\nsocial science digital library to get an overview of the fields, categories,\\ntopics and detailed information needs users are interested in. Based on the log\\ndata we have built interactive visualizations which give an overview and\\nconcurrently let us look at the detailed interests of users. The underlying log\\ndata and the created visualizations are then used to analyze user interests at\\ndifferent hierarchical levels and on a temporal view. The results show that\\nthere are topical interests of the users in specific fields and topics of the\\nsocial sciences but at the same time there exists a diversity of different\\ninformation needs. Based on these findings we analyze in detail the gap between\\nthe indexing language of the system used to annotate documents and the language\\nusers apply to articulate their information needs.'}\n",
            "{'title': 'Library and Information Science Research in Indian Universities: Growth,\\n  Core Journals, Keywords and Collaboration Patterns', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Swapan Kumar Patra', 'abstract': 'This article maps Library and Information Science (LIS) research in Indian\\nuniversities. As the two prominent citation databases, Web of Science and\\nScopus have very limited coverage of Indian LIS journals, the publications\\ngenerated by the library and science departments of about 114 selected Indian\\nuniversities and the two national institutions of importance in LIS research\\nwere extracted from Library, Information Science & Technology Abstracts\\n(LISTA). The relevant publication records were analyzed using scientometrics\\nand Social Network Analysis (SNA) tools. The study traces the growth of\\npublications, prominent keywords, leading journals where the articles are\\npublished and the institutional collaboration patterns of Indian university\\npublications. The results show that there is a growth in scholarly publications\\nfrom Indian universities in LIS. However, the numbers of publications are\\nlimited to only a few universities and national institutes of importance. The\\nmaximum LIS research outputs are published in Indian journals. Bibliometrics\\nrelated investigations are the most important research areas. Located in major\\ncities of India, the productive institutes show healthy collaboration. The\\nstudy concludes with some observations which may be useful for formulating\\npolicies in LIS research in India.'}\n",
            "{'title': 'The Distributed Information Bottleneck reveals the explanatory structure\\n  of complex systems', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Kieran A. Murphy, Dani S. Bassett', 'abstract': \"The fruits of science are relationships made comprehensible, often by way of\\napproximation. While deep learning is an extremely powerful way to find\\nrelationships in data, its use in science has been hindered by the difficulty\\nof understanding the learned relationships. The Information Bottleneck (IB) is\\nan information theoretic framework for understanding a relationship between an\\ninput and an output in terms of a trade-off between the fidelity and complexity\\nof approximations to the relationship. Here we show that a crucial modification\\n-- distributing bottlenecks across multiple components of the input -- opens\\nfundamentally new avenues for interpretable deep learning in science. The\\nDistributed Information Bottleneck throttles the downstream complexity of\\ninteractions between the components of the input, deconstructing a relationship\\ninto meaningful approximations found through deep learning without requiring\\ncustom-made datasets or neural network architectures. Applied to a complex\\nsystem, the approximations illuminate aspects of the system's nature by\\nrestricting -- and monitoring -- the information about different components\\nincorporated into the approximation. We demonstrate the Distributed IB's\\nexplanatory utility in systems drawn from applied mathematics and condensed\\nmatter physics. In the former, we deconstruct a Boolean circuit into\\napproximations that isolate the most informative subsets of input components\\nwithout requiring exhaustive search. In the latter, we localize information\\nabout future plastic rearrangement in the static structure of a sheared glass,\\nand find the information to be more or less diffuse depending on the system's\\npreparation. By way of a principled scheme of approximations, the Distributed\\nIB brings much-needed interpretability to deep learning and enables\\nunprecedented analysis of information flow through a system.\"}\n",
            "{'title': 'Effects of Information Heterogeneity in Bayesian Routing Games', 'venue/journal/conference': 'arxiv', 'year': '2016', 'authors': 'Jeffrey Liu, Saurabh Amin, Galina Schwartz', 'abstract': 'This article studies the value of information in route choice decisions when\\na fraction of players have access to high accuracy information about traffic\\nincidents relative to others. To model such environments, we introduce a\\nBayesian congestion game, in which players have private information about\\nincidents, and each player chooses her route on a network of parallel links.\\nThe links are prone to incidents that occur with an ex-ante known probability.\\nThe demand is comprised of two player populations: one with access to high\\naccuracy incident information and another with low accuracy information, i.e.\\nthe populations differ only by their access to information. The common\\nknowledge includes: (i) the demand and route cost functions, (ii) the fraction\\nof highly-informed players, (iii) the incident probability, and (iv) the\\nmarginal type distributions induced by the information structure of the game.\\nWe present a full characterization of the Bayesian Wardrop Equilibrium of this\\ngame under the assumption that low information players receive no additional\\ninformation beyond common knowledge. We also compute the cost to individual\\nplayers and the social cost as a function of the fraction of highly-informed\\nplayers when they receive perfectly accurate information. Our first result\\nsuggests that below a certain threshold of highly-informed players, both\\npopulations experience a reduction in individual cost, with the highly-informed\\nplayers receiving a greater reduction. However, above this threshold, both\\npopulations realize the same equilibrium cost. Secondly, there exists another\\n(lower or equal) threshold above which a further increase in the fraction of\\nhighly-informed players does not reduce the expected social costs. Thus, once a\\nsufficiently large number of players are highly informed, wider distribution of\\nmore accurate information is ineffective at best, and otherwise socially\\nharmful.'}\n",
            "{'title': 'Enhancing the Conventional Information Security Management Maturity\\n  Model (ISM3) in Resolving Human Factors in Organization Information Sharing', 'venue/journal/conference': 'arxiv', 'year': '2013', 'authors': 'Oyelami Julius Olusegun, Norafida Binti Ithnin', 'abstract': 'Information sharing in organization has been considered as an important\\napproach in increasing organizational efficiency, performance and decision\\nmaking. With the present and advances in information and communication\\ntechnology, sharing information and exchanging of data across organizations has\\nbecome more feasible in organization. However, information sharing has been a\\ncomplex task over the years and identifying factors that influence information\\nsharing across organization has becomes crucial and critical. Researchers have\\ntaken several methods and approaches to resolve problems in information sharing\\nat all levels without a lasting solution, as sharing is best understood as a\\npractice that reflects behavior, social, economic, legal and technological\\ninfluences. Due to the limitation of the conventional ISM3 standards to address\\nculture, social, legislation and human behavior, the findings in this paper\\nsuggest that, a centralized information structure without human practice,\\ndistribution of information and coordination is not effective. This paper\\nreviews the previous information sharing research, outlines the factors\\naffecting information sharing and the different practices needed to improve the\\nmanagement of information security by recommending several combinations of\\ninformation security and coordination mechanism for reducing uncertainty during\\nsharing of information .This thesis proposes information security management\\nprotocol (ISMP) as an enhancement towards ISM3 to resolve the above problems.\\nThis protocol provides a means for practitioners to identify key factors\\ninvolved in successful information sharing.....'}\n",
            "{'title': 'Securely Trading Unverifiable Information without Trust', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Yuqing Kong, Yiping Ma, Yifan Wu', 'abstract': \"In future, information may become one of the most important assets in\\neconomy. However, unlike common goods (e.g. clothing), information is\\ntroublesome in trading since the information commodities are \\\\emph{vulnerable},\\nas they lose their values immediately after revelation, and possibly\\nunverifiable, as they can be subjective. By authorizing a trusted center (e.g.\\nAmazon) to help manage the information trade, traders are ``forced'' to give\\nthe trusted center the ability to become an information monopolist.\\n  To this end, we need a trust-free (i.e. without a trusted center and with\\nonly strategic traders) unverifiable information trade protocol such that it 1)\\nmotivates the sellers to provide high quality information, and the buyer to pay\\nfor the information with a fair price (truthful); 2) except the owner, the\\ninformation is known only to its buyer if the trade is executed (secure).\\n  In an unverifiable information trade scenario (e.g. a medical company wants\\nto buy experts' opinions on multiple difficult medical images with unknown\\npathological truth from several hospitals), we design a trust-free, truthful,\\nand secure protocol, Smart Info-Dealer (SMind), for information trading, by\\nborrowing three cutting-edge tools that include peer prediction, secure\\nmulti-party computation, and smart contract. With SMind, without a trusted\\ncenter, a seller with high-quality information is able to sell her information\\nsecurely at a fair price and those with low-quality information cannot earn\\nextra money with poor information or steal information from other sellers. We\\nbelieve SMind will help describe a free and secure information trade scenario\\nin the future.\"}\n",
            "{'title': 'Uncertainty Principle and the Standard Quantum Limits', 'venue/journal/conference': 'arxiv', 'year': '2005', 'authors': 'Horace P. Yuen', 'abstract': 'The role of the Uncertainty Principle is examined through the examples of\\nsqueezing, information capacity, and position monitoring. It is suggested that\\nmore attention should be directed to conceptual considerations in quantum\\ninformation science and technology.'}\n",
            "{'title': 'Knowledge Management', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'Mohsen Gerami', 'abstract': 'This paper discusses the important process of knowledge and its management,\\nand differences between tacit and explicit knowledge and understanding the\\nculture as a key issue for the successful implementation of knowledge\\nmanagement, in addition to, this paper is concerned with the four-stage model\\nfor the evolution of information technology (IT) support for knowledge\\nmanagement in law firms.'}\n",
            "{'title': 'Comment: Quantifying the Fraction of Missing Information for Hypothesis\\n  Testing in Statistical and Genetic Studies', 'venue/journal/conference': 'arxiv', 'year': '2011', 'authors': 'I-Shou Chang, Chung-Hsing Chen, Li-Chu Chien, Chao A. Hsiung', 'abstract': 'Comment on \"Quantifying the Fraction of Missing Information for Hypothesis\\nTesting in Statistical and Genetic Studies\" [arXiv:1102.2774]'}\n",
            "{'title': 'Comment: Quantifying the Fraction of Missing Information for Hypothesis\\n  Testing in Statistical and Genetic Studies', 'venue/journal/conference': 'arxiv', 'year': '2011', 'authors': 'Tian Zheng, Shaw-Hwa Lo', 'abstract': 'Comment on \"Quantifying the Fraction of Missing Information for Hypothesis\\nTesting in Statistical and Genetic Studies\" [arXiv:1102.2774]'}\n",
            "{'title': 'Rejoinder: Quantifying the Fraction of Missing Information for\\n  Hypothesis Testing in Statistical and Genetic Studies', 'venue/journal/conference': 'arxiv', 'year': '2011', 'authors': 'Dan L. Nicolae, Xiao-Li Meng, Augustine Kong', 'abstract': 'Rejoinder to \"Quantifying the Fraction of Missing Information for Hypothesis\\nTesting in Statistical and Genetic Studies\" [arXiv:1102.2774]'}\n",
            "{'title': 'Intelligence in Strategic Games', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Pavel Naumov, Yuan Yuan', 'abstract': 'The article considers strategies of coalitions that are based on intelligence\\ninformation about moves of some of the other agents. The main technical result\\nis a sound and complete logical system that describes the interplay between\\ncoalition power modality with intelligence and distributed knowledge modality\\nin games with imperfect information.'}\n",
            "{'title': 'A comparative study on communication structures of Chinese journals in\\n  the social sciences', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'Ping Zhou, Xinning Su, Loet Leydesdorff', 'abstract': 'We argue that the communication structures in the Chinese social sciences\\nhave not yet been sufficiently reformed. Citation patterns among Chinese\\ndomestic journals in three subject areas -- political science and marxism,\\nlibrary and information science, and economics -- are compared with their\\ncounterparts internationally. Like their colleagues in the natural and life\\nsciences, Chinese scholars in the social sciences provide fewer references to\\njournal publications than their international counterparts; like their\\ninternational colleagues, social scientists provide fewer references than\\nnatural sciences. The resulting citation networks, therefore, are sparse.\\nNevertheless, the citation structures clearly suggest that the Chinese social\\nsciences are far less specialized in terms of disciplinary delineations than\\ntheir international counterparts. Marxism studies are more established than\\npolitical science in China. In terms of the impact of the Chinese political\\nsystem on academic fields, disciplines closely related to the political system\\nare less specialized than those weakly related. In the discussion section, we\\nexplore reasons that may cause the current stagnation and provide policy\\nrecommendations.'}\n",
            "{'title': 'Science Fiction as a Worldwide Phenomenon: A Study of International\\n  Creation, Consumption and Dissemination', 'venue/journal/conference': 'arxiv', 'year': '2013', 'authors': 'Elysia Wells', 'abstract': \"This paper examines the international nature of science fiction. The focus of\\nthis research is to determine whether science fiction is primarily English\\nspeaking and Western or global; being created and consumed by people in\\nnon-Western, non-English speaking countries? Science fiction's international\\npresence was found in three ways, by network analysis, by examining a online\\nretailer and with a survey. Condor, a program developed by GalaxyAdvisors was\\nused to determine if science fiction is being talked about by non-English\\nspeakers. An analysis of the international Amazon.com websites was done to\\ndiscover if it was being consumed worldwide. A survey was also conducted to see\\nif people had experience with science fiction. All three research methods\\nrevealed similar results. Science fiction was found to be international, with\\nscience fiction creators originating in different countries and writing in a\\nhost of different languages. English and non-English science fiction was being\\ncreated and consumed all over the world, not just in the English speaking West.\"}\n",
            "{'title': 'A tale of two databases: The use of Web of Science and Scopus in\\n  academic papers', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Junwen Zhu, Weishu Liu', 'abstract': 'Web of Science and Scopus are two world-leading and competing citation\\ndatabases. By using the Science Citation Index Expanded and Social Sciences\\nCitation Index, this paper conducts a comparative, dynamic, and empirical study\\nfocusing on the use of Web of Science (WoS) and Scopus in academic papers\\npublished during 2004 and 2018. This brief communication reveals that although\\nboth Web of Science and Scopus are increasingly used in academic papers, Scopus\\nas a new-comer is really challenging the dominating role of WoS. Researchers\\nfrom more and more countries/regions and knowledge domains are involved in the\\nuse of these two databases. Even though the main producers of related papers\\nare developed economies, some developing economies such as China, Brazil and\\nIran also act important roles but with different patterns in the use of these\\ntwo databases. Both two databases are widely used in meta-analysis related\\nstudies especially for researchers in China. Health/medical science related\\ndomains and the traditional Information Science & Library Science field stand\\nout in the use of citation databases.'}\n",
            "{'title': 'Prediction Methods and Applications in the Science of Science: A Survey', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Jie Hou, Hanxiao Pan, Teng Guo, Ivan Lee, Xiangjie Kong, Feng Xia', 'abstract': 'Science of science has become a popular topic that attracts great attentions\\nfrom the research community. The development of data analytics technologies and\\nthe readily available scholarly data enable the exploration of data-driven\\nprediction, which plays a pivotal role in finding the trend of scientific\\nimpact. In this paper, we analyse methods and applications in data-driven\\nprediction in the science of science, and discuss their significance. First, we\\nintroduce the background and review the current state of the science of\\nscience. Second, we review data-driven prediction based on paper citation\\ncount, and investigate research issues in this area. Then, we discuss methods\\nto predict scholar impact, and we analyse different approaches to promote the\\nscholarly collaboration in the collaboration network. This paper also discusses\\nopen issues and existing challenges, and suggests potential research\\ndirections.'}\n",
            "{'title': 'Philosophy of science viewed through the lense of \"References\\n  Publication Years spectrosopy\" (RPYS)', 'venue/journal/conference': 'arxiv', 'year': '2014', 'authors': 'K. Brad Wray, Lutz Bornmann', 'abstract': \"We examine the sub-field of philosophy of science using a new method\\ndeveloped in information science, Referenced Publication Years Spectroscopy\\n(RPYS). RPYS allows us to identify peak years in citations in a field, which\\npromises to help scholars identify the key contributions to a field, and\\nrevolutionary discoveries in a field. We discovered that philosophy of science,\\na sub-field in the humanities, differs significantly from other fields examined\\nwith this method. Books play a more important role in philosophy of science\\nthan in the sciences. Further, Einstein's famous 1905 papers created a citation\\npeak in the philosophy of science literature. But rather than being a\\ncontribution to the philosophy of science, their importance lies in the fact\\nthat they are revolutionary contributions to physics with important\\nimplications for philosophy of science.\"}\n",
            "{'title': 'How Do Data Science Workers Communicate Intermediate Results?', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Rock Yuren Pang, Ruotong Wang, Joely Nelson, Leilani Battle', 'abstract': 'Data science workers increasingly collaborate on large-scale projects before\\ncommunicating insights to a broader audience in the form of visualization.\\nWhile prior work has modeled how data science teams, oftentimes with distinct\\nroles and work processes, communicate knowledge to outside stakeholders, we\\nhave little knowledge of how data science workers communicate intermediately\\nbefore delivering the final products. In this work, we contribute a nuanced\\ndescription of the intermediate communication process within data science\\nteams. By analyzing interview data with 8 self-identified data science workers,\\nwe characterized the data science intermediate communication process with four\\nfactors, including the types of audience, communication goals, shared\\nartifacts, and mode of communication. We also identified overarching challenges\\nin the current communication process. We also discussed design implications\\nthat might inform better tools that facilitate intermediate communication\\nwithin data science teams.'}\n",
            "{'title': 'Network Analysis of the iNaturalist Citizen Science Community', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Yu Lu Liu, Thomas Jiralerspong', 'abstract': \"In recent years, citizen science has become a larger and larger part of the\\nscientific community. Its ability to crowd source data and expertise from\\nthousands of citizen scientists makes it invaluable. Despite the field's\\ngrowing popularity, the interactions and structure of citizen science projects\\nare still poorly understood and under analyzed. We use the iNaturalist citizen\\nscience platform as a case study to analyze the structure of citizen science\\nprojects. We frame the data from iNaturalist as a bipartite network and use\\nvisualizations as well as established network science techniques to gain\\ninsights into the structure and interactions between users in citizen science\\nprojects. Finally, we propose a novel unique benchmark for network science\\nresearch by using the iNaturalist data to create a network which has an unusual\\nstructure relative to other common benchmark networks. We demonstrate using a\\nlink prediction task that this network can be used to gain novel insights into\\na variety of network science methods.\"}\n",
            "{'title': 'Complexity science approach to economic crime', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'János Kertész, Johannes Wachs', 'abstract': 'In this comment we discuss how complexity science and network science are\\nparticularly useful for identifying and describing the hidden traces of\\neconomic misbehaviour such as fraud and corruption.'}\n",
            "{'title': 'Regional Centres for Space Science and Technology Education (Affiliated\\n  to the United Nations)', 'venue/journal/conference': 'arxiv', 'year': '1999', 'authors': 'H. J. Haubold', 'abstract': 'Education is a prerequisite to master the challenges of space science and\\ntechnology. Efforts to understand and control space science and technology are\\nnecessarily intertwined with social expressions in the cultures where science\\nand technology is carried out. The United Nations is leading an effort to\\nestablish regional Centres for Space Science and Technology Education in major\\nregions on Earth. The status of the establishment of such institutions in Asia\\nand the Pacific, Africa, Latin America and the Caribbean, Western Asia, and\\nEastern Europe is briefly described in this article.'}\n",
            "{'title': 'A new generation of science overlay maps with an application to the\\n  history of biosystematics', 'venue/journal/conference': 'arxiv', 'year': '2015', 'authors': 'Sandor Soos', 'abstract': 'The paper proposes a text-mining based analytical framework aiming at the\\ncognitive organization of complex scientific discourses. The approach is based\\non models recently developed in science mapping, being a generalization of the\\nso-called Science Overlay Mapping methodology, referred to as Topic Overlay\\nMapping (TOM). It is shown that via applications of TOM in visualization,\\ndocument clustering, time series analysis etc. the in-depth exploration and\\neven the measurement of cognitive complexity and its dynamics is feasible for\\nscientific domains. As a use case, an empirical study is presented into the\\ndiscovery of a long-standing complex, interdisciplinary discourse, the debate\\non the species concept in biosystematics.'}\n",
            "{'title': 'Proceedings 11th Doctoral Workshop on Mathematical and Engineering\\n  Methods in Computer Science', 'venue/journal/conference': 'arxiv', 'year': '2016', 'authors': 'Jan Bouda, Lukáš Holík, Jan Kofroň, Jan Strejček, Adam Rambousek', 'abstract': 'MEMICS provides a forum for doctoral students interested in applications of\\nmathematical and engineering methods in computer science. Besides a rich\\ntechnical programme (including invited talks, regular papers, and\\npresentations), MEMICS also offers friendly social activities and exciting\\nopportunities for meeting like-minded people. MEMICS submissions traditionally\\ncover all areas of computer science (such as parallel and distributed\\ncomputing, computer networks, modern hardware and its design, non-traditional\\ncomputing architectures, information systems and databases, multimedia and\\ngraphics, verification and testing, computer security, as well as all related\\nareas of theoretical computer science).'}\n",
            "{'title': 'Building Data Science Capabilities into University Data Warehouse to\\n  Predict Graduation', 'venue/journal/conference': 'arxiv', 'year': '2018', 'authors': 'Joonas Pesonen, Anna Fomkin, Lauri Jokipii', 'abstract': \"The discipline of data science emerged to combine statistical methods with\\ncomputing. At Aalto University, Finland, we have taken first steps to bring\\neducational data science as a part of daily operations of Management\\nInformation Services. This required changes in IT environment: we enhanced data\\nwarehouse infrastructure with a data science lab, where we can read predictive\\nmodel training data from data warehouse database and use the created predictive\\nmodels in database queries. We then conducted a data science pilot with an\\nobjective to predict students' graduation probability and time-to-degree with\\nstudent registry data. Further ethical and legal considerations are needed\\nbefore using predictions in daily operations of the university.\"}\n",
            "{'title': 'Connecting science fundamentals and career proficiency -- Development of\\n  multi-disciplinary science curriculum', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Farida Selim', 'abstract': 'The current conventional approach in teaching STEM seems inadequate; it is\\nfailing both the students and the work-force demands in several aspects.\\nPerhaps it is succeeding in equipping students with information and facts but\\nsurely falls short in developing them intellectually and in sparking their\\npassion for science. Mixing research with learning by adding components in\\ncourses to enforce creativity, reasoning, and to connect the dots between\\ndifferent topics and different subjects can be an effective counter to this\\ntendency. Multi-disciplinary science curriculum connecting the dots between\\ndifferent subjects and the dots between fundamentals and applications and\\ndemonstrating clear path to current and future industrial jobs can offer a good\\napproach for STEM education. An example for mutli-disciplinary science course\\nis presented.'}\n",
            "{'title': 'Materials Informatics: An Algorithmic Design Rule', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Bhupesh Bishnoi', 'abstract': 'Materials informatics, data-enabled investigation, is a \"fourth paradigm\" in\\nmaterials science research after the conventional empirical approach,\\ntheoretical science, and computational research. Materials informatics has two\\nessential ingredients: fingerprinting materials proprieties and the theory of\\nstatistical inference and learning. We have researched the organic\\nsemiconductor\\'s enigmas through the materials informatics approach. By applying\\ndiverse neural network topologies, logical axiom, and inferencing information\\nscience, we have developed data-driven procedures for novel organic\\nsemiconductor discovery for the semiconductor industry and knowledge extraction\\nfor the materials science community. We have reviewed and corresponded with\\nvarious algorithms for the neural network design topology for the materials\\ninformatics dataset.'}\n",
            "{'title': 'Information Bottleneck in Control Tasks with Recurrent Spiking Neural\\n  Networks', 'venue/journal/conference': 'arxiv', 'year': '2017', 'authors': 'Madhavun Candadai Vasu, Eduardo Izquierdo', 'abstract': 'The nervous system encodes continuous information from the environment in the\\nform of discrete spikes, and then decodes these to produce smooth motor\\nactions. Understanding how spikes integrate, represent, and process information\\nto produce behavior is one of the greatest challenges in neuroscience.\\nInformation theory has the potential to help us address this challenge.\\nInformational analyses of deep and feed-forward artificial neural networks\\nsolving static input-output tasks, have led to the proposal of the\\n\\\\emph{Information Bottleneck} principle, which states that deeper layers encode\\nmore relevant yet minimal information about the inputs. Such an analyses on\\nnetworks that are recurrent, spiking, and perform control tasks is relatively\\nunexplored. Here, we present results from a Mutual Information analysis of a\\nrecurrent spiking neural network that was evolved to perform the classic\\npole-balancing task. Our results show that these networks deviate from the\\n\\\\emph{Information Bottleneck} principle prescribed for feed-forward networks.'}\n",
            "{'title': 'Knowledge discovery via multidimensional science maps: the case of the\\n  Species Problem', 'venue/journal/conference': 'arxiv', 'year': '2014', 'authors': 'Sandor Soos', 'abstract': 'Science mapping (SM), the study of the organization and development of\\nscience and technology, is a rapidly developing field within information\\nscience. The volume of available data allows this methodology to empirically\\naddress such issues as the historical development of topics, discourses, fields\\nor the entire science system. Based on the pool of related methods, we are\\nproposing an integration of various maps to obtain a novel kind of science map\\nwe call multidimensional. The basic idea behind is to combine the most\\ninformative relations available from various maps based on different\\nbibliometric indicators, in order to produce a rich structrue for the study of\\nknowledge dynamics, with special emphasis on causal-historical connections. As\\na proof of concept, we deploy the proposed framework in an extensive case study\\non a historical topic from the life sciences, namely, the debate on the species\\nconcept in biosystematics.'}\n",
            "{'title': 'Neural Embeddings of Scholarly Periodicals Reveal Complex Disciplinary\\n  Organizations', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Hao Peng, Qing Ke, Ceren Budak, Daniel M. Romero, Yong-Yeol Ahn', 'abstract': 'Understanding the structure of knowledge domains is one of the foundational\\nchallenges in science of science. Here, we propose a neural embedding technique\\nthat leverages the information contained in the citation network to obtain\\ncontinuous vector representations of scientific periodicals. We demonstrate\\nthat our periodical embeddings encode nuanced relationships between periodicals\\nas well as the complex disciplinary and interdisciplinary structure of science,\\nallowing us to make cross-disciplinary analogies between periodicals.\\nFurthermore, we show that the embeddings capture meaningful \"axes\" that\\nencompass knowledge domains, such as an axis from \"soft\" to \"hard\" sciences or\\nfrom \"social\" to \"biological\" sciences, which allow us to quantitatively ground\\nperiodicals on a given dimension. By offering novel quantification in science\\nof science, our framework may in turn facilitate the study of how knowledge is\\ncreated and organized.'}\n",
            "{'title': 'Citation advantage of COVID-19 related publications', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Weishu Liu, Xuping Huangfu, Haifeng Wang', 'abstract': \"With the global spread of the COVID-19 pandemic, scientists from various\\ndisciplines responded quickly to this historical public health emergency. The\\nsudden boom of COVID-19 related papers in a short period of time may bring\\nunexpected influence to some commonly used bibliometric indicators. By a\\nlarge-scale investigation using Science Citation Index Expanded and Social\\nSciences Citation Index, this brief communication confirms the citation\\nadvantage of COVID-19 related papers empirically through the lens of Essential\\nScience Indicators' highly cited paper. More than 8% of COVID-19 related papers\\npublished during 2020 and 2021 were selected as Essential Science Indicators\\nhighly cited papers, which was much higher than the set global benchmark value\\nof 1%. The citation advantage of COVID-19 related papers for different Web of\\nScience categories/countries/journal impact factor quartiles were also\\ndemonstrated. The distortions of COVID-19 related papers' citation advantage to\\nsome bibliometric indicators such as journal impact factor were discussed at\\nthe end of this brief communication.\"}\n",
            "{'title': 'Gender Imbalance and Spatiotemporal Patterns of Contributions to Citizen\\n  Science Projects: the case of Zooniverse', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Khairunnisa Ibrahim, Samuel Khodursky, Taha Yasseri', 'abstract': 'Citizen Science is research undertaken by professional scientists and members\\nof the public collaboratively. Despite numerous benefits of citizen science for\\nboth the advancement of science and the community of the citizen scientists,\\nthere is still no comprehensive knowledge of patterns of contributions, and the\\ndemography of contributors to citizen science projects. In this paper we\\nprovide a first overview of spatiotemporal and gender distribution of citizen\\nscience workforce by analyzing 54 million classifications contributed by more\\nthan 340 thousand citizen science volunteers from 198 countries to one of the\\nlargest citizen science platforms, Zooniverse. First we report on the uneven\\ngeographical distribution of the citizen scientist and model the variations\\namong countries based on the socio-economic conditions as well as the level of\\nresearch investment in each country. Analyzing the temporal features of\\ncontributions, we report on high \"burstiness\" of participation instances as\\nwell as the leisurely nature of participation suggested by the time of the day\\nthat the citizen scientists were the most active. Finally, we discuss the\\ngender imbalance among citizen scientists (about 30% female) and compare it\\nwith other collaborative projects as well as the gender distribution in more\\nformal scientific activities. Citizen science projects need further attention\\nfrom outside of the academic community, and our findings can help attract the\\nattention of public and private stakeholders, as well as to inform the design\\nof the platforms and science policy making processes.'}\n",
            "{'title': 'Calculation of the minimum computational complexity based on information\\n  entropy', 'venue/journal/conference': 'arxiv', 'year': '2012', 'authors': 'Xue Wu', 'abstract': 'In order to find out the limiting speed of solving a specific problem using\\ncomputer, this essay provides a method based on information entropy. The\\nrelationship between the minimum computational complexity and information\\nentropy change is illustrated. A few examples are served as evidence of such\\nconnection. Meanwhile some basic rules of modeling problems are established.\\nFinally, the nature of solving problems with computer programs is disclosed to\\nsupport this theory and a redefinition of information entropy in this filed is\\nproposed. This will develop a new field of science.'}\n",
            "{'title': 'Privacy Things: Systematic Approach to Privacy and Personal Identifiable\\n  Information', 'venue/journal/conference': 'arxiv', 'year': '2018', 'authors': 'Sabah S. Al-Fedaghi', 'abstract': \"Defining privacy and related notions such as Personal Identifiable\\nInformation (PII) is a central notion in computer science and other fields. The\\ntheoretical, technological, and application aspects of PII require a framework\\nthat provides an overview and systematic structure for the discipline's topics.\\nThis paper develops a foundation for representing information privacy. It\\nintroduces a coherent conceptualization of the privacy senses built upon\\ndiagrammatic representation. A new framework is presented based on a flow-based\\nmodel that includes generic operations performed on PII.\"}\n",
            "{'title': 'The Physics of Quantum Information', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'John Preskill', 'abstract': 'Rapid ongoing progress in quantum information science makes this an apt time\\nfor a Solvay Conference focused on The Physics of Quantum Information. Here I\\nreview four intertwined themes encompassed by this topic: Quantum computer\\nscience, quantum hardware, quantum matter, and quantum gravity. Though the time\\nscale for broad practical impact of quantum computation is still uncertain, in\\nthe near future we can expect noteworthy progress toward scalable\\nfault-tolerant quantum computing, and discoveries enabled by programmable\\nquantum simulators. In the longer term, controlling highly complex quantum\\nmatter will open the door to profound scientific advances and powerful new\\ntechnologies.'}\n",
            "{'title': 'Public Reaction to Scientific Research via Twitter Sentiment Prediction', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Murtuza Shahzad, Hamed Alhoori', 'abstract': 'Social media users share their ideas, thoughts, and emotions with other\\nusers. However, it is not clear how online users would respond to new research\\noutcomes. This study aims to predict the nature of the emotions expressed by\\nTwitter users toward scientific publications. Additionally, we investigate what\\nfeatures of the research articles help in such prediction. Identifying the\\nsentiments of research articles on social media will help scientists gauge a\\nnew societal impact of their research articles.'}\n",
            "{'title': 'Novel Framework for Hidden Data in the Image Page within Executable File\\n  Using Computation between Advanced Encryption Standard and Distortion\\n  Techniques', 'venue/journal/conference': 'arxiv', 'year': '2009', 'authors': 'A. W. Naji, Shihab A. Hameed, B. B. Zaidan, Wajdi F. Al-Khateeb, Othman O. Khalifa, A. A. Zaidan, Teddy S. Gunawan', 'abstract': 'The hurried development of multimedia and internet allows for wide\\ndistribution of digital media data. It becomes much easier to edit, modify and\\nduplicate digital information. In additional, digital document is also easy to\\ncopy and distribute, therefore it may face many threats. It became necessary to\\nfind an appropriate protection due to the significance, accuracy and\\nsensitivity of the information. Furthermore, there is no formal method to be\\nfollowed to discover a hidden data. In this paper, a new information hiding\\nframework is presented.The proposed framework aim is implementation of\\nframework computation between advance encryption standard (AES) and distortion\\ntechnique (DT) which embeds information in image page within executable file\\n(EXE file) to find a secure solution to cover file without change the size of\\ncover file. The framework includes two main functions; first is the hiding of\\nthe information in the image page of EXE file, through the execution of four\\nprocess (specify the cover file, specify the information file, encryption of\\nthe information, and hiding the information) and the second function is the\\nextraction of the hiding information through three process (specify the stego\\nfile, extract the information, and decryption of the information).'}\n",
            "{'title': 'What did Erwin Mean? The Physics of Information from the Materials\\n  Genomics of Aperiodic Crystals and Water to Molecular Information Catalysts\\n  and Life', 'venue/journal/conference': 'arxiv', 'year': '2015', 'authors': 'Dowman P. Varn, James P. Crutchfield', 'abstract': \"Erwin Schrodinger famously and presciently ascribed the vehicle transmitting\\nthe hereditary information underlying life to an `aperiodic crystal'. We\\ncompare and contrast this, only later discovered to be stored in the linear\\nbiomolecule DNA, with the information bearing, layered quasi-one-dimensional\\nmaterials investigated by the emerging field of chaotic crystallography.\\nDespite differences in functionality, the same information measures capture\\nstructure and novelty in both, suggesting an intimate coherence between the\\ninformation character of biotic and abiotic matter---a broadly applicable\\nphysics of information. We review layered solids and consider three examples of\\nhow information- and computation-theoretic techniques are being applied to\\nunderstand their structure. In particular, (i) we review recent efforts to\\napply new kinds of information measures to quantify disordered crystals; (ii)\\nwe discuss the structure of ice I in information-theoretic terms; and (iii) we\\nrecount recent experimental results on tris(bicyclo[2.1.1]hexeno)benzene TBHB),\\nshowing how an information-theoretic analysis yields additional insight into\\nits structure. We then illustrate a new Second Law of Thermodynamics that\\ndescribes information processing in active low-dimensional materials, reviewing\\nMaxwell's Demon and a new class of molecular devices that act as information\\ncatalysts. Lastly, we conclude by speculating on how these ideas from\\ninformational materials science may impact biology.\"}\n",
            "{'title': 'How do scientific disciplines evolve in applied sciences? The properties\\n  of scientific fission and ambidextrous scientific drivers', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Mario Coccia', 'abstract': 'One of the fundamental questions in science is how scientific disciplines\\nevolve and sustain progress in society. No studies to date allows us to explain\\nthe endogenous processes that support the evolution of scientific disciplines\\nand emergence of new scientific fields in applied sciences of physics. This\\nstudy confronts this problem here by investigating the evolution of\\nexperimental physics to explain and generalize some characteristics of the\\ndynamics of applied sciences. Empirical analysis suggests properties about the\\nevolution of experimental physics and in general of applied sciences, such as:\\na) scientific fission, the evolution of scientific disciplines generates a\\nprocess of division into two or more research fields that evolve as autonomous\\nentities over time; b) ambidextrous drivers of science, the evolution of\\nscience via scientific fission is due to scientific discoveries or new\\ntechnologies; c) new driving research fields, the drivers of scientific\\ndisciplines are new research fields rather than old ones; d) science driven by\\ndevelopment of general purpose technologies, the evolution of experimental\\nphysics and applied sciences is due to the convergence of experimental and\\ntheoretical branches of physics associated with the development of computer,\\ninformation systems and applied computational science. Results also reveal that\\naverage duration of the upwave of scientific production in scientific fields\\nsupporting experimental physics is about 80 years. Overall, then, this study\\nbegins the process of clarifying and generalizing, as far as possible, some\\ncharacteristics of the evolutionary dynamics of scientific disciplines that can\\nlay a foundation for the development of comprehensive properties explaining the\\nevolution of science as a whole for supporting fruitful research policy\\nimplications directed to advancement of science and technological progress in\\nsociety.'}\n",
            "{'title': 'Theoretical limit of the compression for the information', 'venue/journal/conference': 'arxiv', 'year': '2002', 'authors': 'A. Lavrenov', 'abstract': 'The pit recording of file, the coefficient of compression are introduced. The\\ntheoretical limit of the information compression as minimal coefficient of\\ncompression for the given length of alphabet are found.'}\n",
            "{'title': 'Cognitive MIMO Radio: A Competitive Optimality Design Based on Subspace\\n  Projections', 'venue/journal/conference': 'arxiv', 'year': '2008', 'authors': 'Gesualdo Scutari, Daniel P. Palomar, Sergio Barbarossa', 'abstract': 'Cognitive MIMO Radio: A Competitive Optimality Design Based on Subspace\\nProjections'}\n",
            "{'title': 'Optimal Strategies in Perfect-Information Stochastic Games with Tail\\n  Winning Conditions', 'venue/journal/conference': 'arxiv', 'year': '2008', 'authors': 'Hugo Gimbert, Florian Horn', 'abstract': 'We prove that optimal strategies exist in every perfect-information\\nstochastic game with finitely many states and actions and a tail winning\\ncondition.'}\n",
            "{'title': 'Quantum Mechanics and Quantum Information Science: The Nature of $Ψ$', 'venue/journal/conference': 'arxiv', 'year': '2014', 'authors': 'Partha Ghose', 'abstract': 'An overview is given of the nature of the quantum mechanical wave function.'}\n",
            "{'title': 'Constructions and necessities of some permutation polynomials', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Xiaogang Liu', 'abstract': 'Permutation polynomials over finite fields have important applications in\\nmany areas of science and engineering such as coding theory, cryptography,\\ncombinatorial design, etc. In this paper, we construct several new classes of\\npermutation polynomials, and the necessities of some permutation polynomials\\nare studied.'}\n",
            "{'title': 'New variant of ElGamal signature scheme', 'venue/journal/conference': 'arxiv', 'year': '2013', 'authors': 'Omar Khadir', 'abstract': 'In this paper, a new variant of ElGamal signature scheme is presented and its\\nsecurity analyzed. We also give, for its theoretical interest, a general form\\nof the signature equation.'}\n",
            "{'title': 'Virtual Astronomy, Information Technology, and the New Scientific\\n  Methodology', 'venue/journal/conference': 'arxiv', 'year': '2005', 'authors': 'S. G. Djorgovski', 'abstract': 'All sciences, including astronomy, are now entering the era of information\\nabundance. The exponentially increasing volume and complexity of modern data\\nsets promises to transform the scientific practice, but also poses a number of\\ncommon technological challenges. The Virtual Observatory concept is the\\nastronomical community\\'s response to these challenges: it aims to harness the\\nprogress in information technology in the service of astronomy, and at the same\\ntime provide a valuable testbed for information technology and applied computer\\nscience. Challenges broadly fall into two categories: data handling (or \"data\\nfarming\"), including issues such as archives, intelligent storage, databases,\\ninteroperability, fast networks, etc., and data mining, data understanding, and\\nknowledge discovery, which include issues such as automated clustering and\\nclassification, multivariate correlation searches, pattern recognition,\\nvisualization in highly hyperdimensional parameter spaces, etc., as well as\\nvarious applications of machine learning in these contexts. Such techniques are\\nforming a methodological foundation for science with massive and complex data\\nsets in general, and are likely to have a much broather impact on the modern\\nsociety, commerce, information economy, security, etc. There is a powerful\\nemerging synergy between the computationally enabled science and the\\nscience-driven computing, which will drive the progress in science,\\nscholarship, and many other venues in the 21st century.'}\n",
            "{'title': 'Exploiting Conceptual Knowledge for Querying Information Systems', 'venue/journal/conference': 'arxiv', 'year': '2011', 'authors': 'Joachim Selke, Wolf-Tilo Balke', 'abstract': \"Whereas today's information systems are well-equipped for efficient query\\nhandling, their strict mathematical foundations hamper their use for everyday\\ntasks. In daily life, people expect information to be offered in a personalized\\nand focused way. But currently, personalization in digital systems still only\\ntakes explicit knowledge into account and does not yet process conceptual\\ninformation often naturally implied by users. We discuss how to bridge the gap\\nbetween users and today's systems, building on results from cognitive\\npsychology.\"}\n",
            "{'title': 'Enhancing Information Systems Security in Educational Organizations in\\n  KSA through proposing security model', 'venue/journal/conference': 'arxiv', 'year': '2011', 'authors': 'Hussain A. H. Awad, Fadi M. Battah', 'abstract': 'It is well known that technology utilization is not restricted for one sector\\nthan the other anymore, Educational organizations share many parts of their\\ninformation systems with commercial organizations. In this paper we will try to\\nidentify the main characteristics of information systems in educational\\norganizations, then we will propose a model of two parts to enhance the\\ninformation systems security, the first part of the model will handle the\\npolicy and laws of the information system, the second part will provide a\\ntechnical approach on how to audit and subsequently maintain the security of\\ninformation system.'}\n",
            "{'title': 'Destination Information Management System for Tourist', 'venue/journal/conference': 'arxiv', 'year': '2014', 'authors': 'Shafii Muhammad Abdulhamid, Gana Usman', 'abstract': 'The use of information and communication technology in our day to day\\nactivities is now unavoidable. In tourism developments, destination information\\nand management systems are used to guide visitors and provide information to\\nboth visitors and management of the tour sites. In this paper, information and\\nnavigation system was designed for tourists, taking some Niger state of Nigeria\\ntourism destinations into account. The information management system was\\ndesigned using Java Applet (NetBeans IDE 6.1), Hypertext MarkUp Language\\n(HTML), Personal Home Page (PHP), Java script and MySQL as the back-end\\nintegration database. Two different MySQL servers were used, the MySQL query\\nbrowser and the WAMP5 server to compare the effectiveness of the system\\ndeveloped.'}\n",
            "{'title': 'Strategic Information Exchange', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'Dinah Rosenberg, Eilon Solan, Nicolas Vieille', 'abstract': \"We study a class of two-player repeated games with incomplete information and\\ninformational externalities. In these games, two states are chosen at the\\noutset, and players get private information on the pair, before engaging in\\nrepeated play. The payoff of each player only depends on his `own' state and on\\nhis own action. We study to what extent, and how, information can be exchanged\\nin equilibrium. We prove that provided the private information of each player\\nis valuable for the other player, the set of sequential equilibrium payoffs\\nconverges to the set of feasible and individually rational payoffs as players\\nbecome patient.\"}\n",
            "{'title': 'Mutual Information as Privacy-Loss Measure in Strategic Communication', 'venue/journal/conference': 'arxiv', 'year': '2015', 'authors': 'Farhad Farokhi, Girish Nair', 'abstract': 'A game is introduced to study the effect of privacy in strategic\\ncommunication between well-informed senders and a receiver. The receiver wants\\nto accurately estimate a random variable. The sender, however, wants to\\ncommunicate a message that balances a trade-off between providing an accurate\\nmeasurement and minimizing the amount of leaked private information, which is\\nassumed to be correlated with the to-be-estimated variable. The mutual\\ninformation between the transmitted message and the private information is used\\nas a measure of the amount of leaked information. An equilibrium is constructed\\nand its properties are investigated.'}\n",
            "{'title': 'A Survey on Quantum Channel Capacities', 'venue/journal/conference': 'arxiv', 'year': '2018', 'authors': 'Laszlo Gyongyosi, Sandor Imre, Hung Viet Nguyen', 'abstract': 'Quantum information processing exploits the quantum nature of information. It\\noffers fundamentally new solutions in the field of computer science and extends\\nthe possibilities to a level that cannot be imagined in classical communication\\nsystems. For quantum communication channels, many new capacity definitions were\\ndeveloped in comparison to classical counterparts. A quantum channel can be\\nused to realize classical information transmission or to deliver quantum\\ninformation, such as quantum entanglement. Here we review the properties of the\\nquantum communication channel, the various capacity measures and the\\nfundamental differences between the classical and quantum channels.'}\n",
            "{'title': 'The Rule Responder eScience Infrastructure', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'Adrian Paschke, Zhili Zhao', 'abstract': 'To a large degree information and services for chemical e-Science have become\\naccessible - anytime, anywhere - but not necessarily useful. The Rule Responder\\neScience middleware is about providing information consumers with rule-based\\nagents to transform existing information into relevant information of practical\\nconsequences, hence providing control to the end-users to express in a\\ndeclarative rule-based way how to turn existing information into personally\\nrelevant information and how to react or make automated decisions on top of it.'}\n",
            "{'title': 'Midiendo la calidad de la informacion gestionada: algunas reflexiones\\n  conceptuales-metodologicas', 'venue/journal/conference': 'arxiv', 'year': '2014', 'authors': 'C. L. González-Valiente', 'abstract': 'The study, based on a documental classic analysis, presents conceptual and\\nmethodological guidelines concerning the design of methodologies that help to\\nmeasure the quality of information that is managed in organizations. It is\\ndescribed the process of information management and the importance of\\nimplementing quality principles in it. There are exposed the four dimensions of\\ninformation quality as part of an indicators integration which characterize the\\ninformational contents. There are defined each of the phases in the\\nmethodological design to evaluate the information. There also are indicated the\\nimplications of this activity for information professionals.'}\n",
            "{'title': 'Introductory review to quantum information retrieval', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Alexander Lebedev, Andrei Khrennikov', 'abstract': 'Recently people started to understand that applications of the mathematical\\nformalism of quantum theory are not reduced to physics. Nowadays, this\\nformalism is widely used outside of quantum physics, in particular, in\\ncognition, psychology, decision making, information processing, especially\\ninformation retrieval. The latter is very promising. The aim of this brief\\nintroductory review is to stimulate research in this exciting area of\\ninformation science. This paper is not aimed to present a complete review on\\nthe state of art in quantum information retrieval.'}\n",
            "{'title': 'Beyond information: A bit of meaning', 'venue/journal/conference': 'arxiv', 'year': '2013', 'authors': 'Olaf Dreyer', 'abstract': 'Is our world just information? We argue that our current notion of\\ninformation has one serious shortcoming: It is quite literally meaningless. We\\nsuggest a meaningful extension of the notion of information that is dynamic,\\ninternal, approximate, contains an element of randomness, and is layered. This\\nnew notion of information derives from the interactions of material objects.\\nOur answer to the essay question then is Bit from It or, more appropriately,\\nBit++ from It. We discuss how our new notion of information sheds light on the\\nmeasurement problem in quantum mechanics and how it can be applied in\\nphilosophy and computer science.'}\n",
            "{'title': 'ExIt-OOS: Towards Learning from Planning in Imperfect Information Games', 'venue/journal/conference': 'arxiv', 'year': '2018', 'authors': 'Andy Kitchen, Michela Benedetti', 'abstract': 'The current state of the art in playing many important perfect information\\ngames, including Chess and Go, combines planning and deep reinforcement\\nlearning with self-play. We extend this approach to imperfect information games\\nand present ExIt-OOS, a novel approach to playing imperfect information games\\nwithin the Expert Iteration framework and inspired by AlphaZero. We use Online\\nOutcome Sampling, an online search algorithm for imperfect information games in\\nplace of MCTS. While training online, our neural strategy is used to improve\\nthe accuracy of playouts in OOS, allowing a learning and planning feedback loop\\nfor imperfect information games.'}\n",
            "{'title': 'Integration and interoperability accessing electronic information\\n  resources in science and technology: the proposal of Brazilian Digital\\n  Library', 'venue/journal/conference': 'arxiv', 'year': '2002', 'authors': 'Carlos H. Marcondes, Luis Fernando Sayao', 'abstract': 'This paper describes technological and methodological options to achieve\\ninteroperability in accessing electronic information resources, available in\\nInternet, in the scope of Brazilian Digital Library in Science and Technology\\nProject - BDL, developed by Brazilian Institute for Scientific and Technical\\nInformation - IBICT. It stresses the impact of the Web in the publishing and\\ncommunication processes in science and technology and also in the information\\nsystems and libraries. The work points out the two major objectives of the BDL\\nProject: facilitates electronic publishing of different full text materials\\nsuch as theses, journal articles, conference papers,grey literature - by\\nBrazilian scientific community, so amplifying their nationally and\\ninternationally visibility; and achieving, through a unified gateway, thus\\navoiding a user to navigate and query across different information resources\\nindividually. The work explains technological options and standards that will\\nassure interoperability in this context.'}\n",
            "{'title': 'Studying Maximum Information Leakage Using Karush-Kuhn-Tucker Conditions', 'venue/journal/conference': 'arxiv', 'year': '2009', 'authors': 'Han Chen, Pasquale Malacaria', 'abstract': 'When studying the information leakage in programs or protocols, a natural\\nquestion arises: \"what is the worst case scenario?\". This problem of\\nidentifying the maximal leakage can be seen as a channel capacity problem in\\nthe information theoretical sense. In this paper, by combining two powerful\\ntheories: Information Theory and Karush-Kuhn-Tucker conditions, we demonstrate\\na very general solution to the channel capacity problem. Examples are given to\\nshow how our solution can be applied to practical contexts of programs and\\nanonymity protocols, and how this solution generalizes previous approaches to\\nthis problem.'}\n",
            "{'title': 'On the Consistency among Prior, Posteriors, and Information Sets\\n  (Extended Abstract)', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Satoshi Fukuda', 'abstract': \"This paper studies implications of the consistency conditions among prior,\\nposteriors, and information sets on introspective properties of qualitative\\nbelief induced from information sets. The main result reformulates the\\nconsistency conditions as: (i) the information sets, without any assumption,\\nalmost surely form a partition; and (ii) the posterior at a state is equal to\\nthe Bayes conditional probability given the corresponding information set.\\nImplications are as follows. First, each posterior is uniquely determined.\\nSecond, qualitative belief reduces to fully introspective knowledge in a\\n``standard'' environment. Thus, a care must be taken when one studies\\nnon-veridical belief or non-introspective knowledge. Third, an information\\npartition compatible with the consistency conditions is uniquely determined by\\nthe posteriors. Fourth, qualitative and probability-one beliefs satisfy truth\\naxiom almost surely. The paper also sheds light on how the additivity of the\\nposteriors yields negative introspective properties of beliefs.\"}\n",
            "{'title': 'Funding information in Web of Science: An updated overview', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Weishu Liu, Li Tang, Guangyuan Hu', 'abstract': \"Despite the limitations of funding acknowledgment (FA) data in Web of Science\\n(WoS), studies using FA information have increased rapidly over the last\\nseveral years. Considering this WoS'recent practice of updating funding data,\\nthis paper further investigates the characteristics and distribution of FA data\\nin four WoS journal citation indexes. The research reveals that FA information\\ncoverage variances persist cross all four citation indexes by time coverage,\\nlanguage and document type. Our evidence suggests an improvement in FA\\ninformation collection in humanity and social science research. Departing from\\nprevious studies, we argue that FA text (FT) alone no longer seems an\\nappropriate field to retrieve and analyze funding information, since a\\nsubstantial number of documents only report funding agency or grant number\\ninformation in respective fields. Articles written in Chinese have a higher FA\\npresence rate than other non-English WoS publications. This updated study\\nconcludes with a discussion of new findings and practical guidance for the\\nfuture retrieval and analysis of funded research.\"}\n",
            "{'title': 'Modeling Information Change in Science Communication with Semantically\\n  Matched Paraphrases', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Dustin Wright, Jiaxin Pei, David Jurgens, Isabelle Augenstein', 'abstract': 'Whether the media faithfully communicate scientific information has long been\\na core issue to the science community. Automatically identifying paraphrased\\nscientific findings could enable large-scale tracking and analysis of\\ninformation changes in the science communication process, but this requires\\nsystems to understand the similarity between scientific information across\\nmultiple domains. To this end, we present the SCIENTIFIC PARAPHRASE AND\\nINFORMATION CHANGE DATASET (SPICED), the first paraphrase dataset of scientific\\nfindings annotated for degree of information change. SPICED contains 6,000\\nscientific finding pairs extracted from news stories, social media discussions,\\nand full texts of original papers. We demonstrate that SPICED poses a\\nchallenging task and that models trained on SPICED improve downstream\\nperformance on evidence retrieval for fact checking of real-world scientific\\nclaims. Finally, we show that models trained on SPICED can reveal large-scale\\ntrends in the degrees to which people and organizations faithfully communicate\\nnew scientific findings. Data, code, and pre-trained models are available at\\nhttp://www.copenlu.com/publication/2022_emnlp_wright/.'}\n",
            "{'title': 'Frequentist Prediction Sets for Species Abundance using Indirect\\n  Information', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Elizabeth Bersson, Peter D. Hoff', 'abstract': 'Citizen science databases that consist of volunteer-led sampling efforts of\\nspecies communities are relied on as essential sources of data in ecology.\\nSummarizing such data across counties with frequentist-valid prediction sets\\nfor each county provides an interpretable comparison across counties of varying\\nsize or composition. As citizen science data often feature unequal sampling\\nefforts across a spatial domain, prediction sets constructed with indirect\\nmethods that share information across counties may be used to improve\\nprecision. In this article, we present a nonparametric framework to obtain\\nprecise prediction sets for a multinomial random sample based on indirect\\ninformation that maintain frequentist coverage guarantees for each county. We\\ndetail a simple algorithm to obtain prediction sets for each county using\\nindirect information where the computation time does not depend on the sample\\nsize and scales nicely with the number of species considered. The indirect\\ninformation may be estimated by a proposed empirical Bayes procedure based on\\ninformation from auxiliary data. Our approach makes inference for under-sampled\\ncounties more precise, while maintaining area-specific frequentist validity for\\neach county. Our method is used to provide a useful description of avian\\nspecies abundance in North Carolina, USA based on citizen science data from the\\neBird database.'}\n",
            "{'title': 'Indonesia embraces the Data Science', 'venue/journal/conference': 'arxiv', 'year': '2015', 'authors': 'Hokky Situngkir', 'abstract': 'The information era is the time when information is not only largely\\ngenerated, but also vastly processed in order to extract and generated more\\ninformation. The complex nature of modern living is represented by the various\\nkind of data. Data can be in the forms of signals, images, texts, or manifolds\\nresembling the horizon of observation. The task of the emerging data sciences\\nare to extract information from the data, for people gain new insights of the\\ncomplex world. The insights may came from the new way of the data\\nrepresentation, be it a visualizations, mapping, or other. The insights may\\nalso come from the implementation of mathematical analysis and or computational\\nprocessing giving new insights of what the states of the nature represented by\\nthe data. Both ways implement the methodologies reducing the dimensionality of\\nthe data. The relations between the two functions, representation and analysis\\nare the heart of how information in data is transformed mathematically and\\ncomputationally into new information. The paper discusses some practices, along\\nwith various data coming from the social life in Indonesia to gain new insights\\nabout Indonesia in the emerging data sciences. The data sciences in Indonesia\\nhas made Indonesian Data Cartograms, Indonesian Celebrity Sentiment Mapping,\\nEthno-Clustering Maps, social media community detection, and a lot more to\\ncome, become possible. All of these are depicted as the exemplifications on how\\nData Science has become integral part of the technology bringing data closer to\\npeople.'}\n",
            "{'title': 'MatScIE: An automated tool for the generation of databases of methods\\n  and parameters used in the computational materials science literature', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Souradip Guha, Ankan Mullick, Jatin Agrawal, Swetarekha Ram, Samir Ghui, Seung-Cheol Lee, Satadeep Bhattacharjee, Pawan Goyal', 'abstract': 'The number of published articles in the field of materials science is growing\\nrapidly every year. This comparatively unstructured data source, which contains\\na large amount of information, has a restriction on its re-usability, as the\\ninformation needed to carry out further calculations using the data in it must\\nbe extracted manually. It is very important to obtain valid and contextually\\ncorrect information from the online (offline) data, as it can be useful not\\nonly to generate inputs for further calculations, but also to incorporate them\\ninto a querying framework. Retaining this context as a priority, we have\\ndeveloped an automated tool, MatScIE (Material Scince Information Extractor)\\nthat can extract relevant information from material science literature and make\\na structured database that is much easier to use for material simulations.\\nSpecifically, we extract the material details, methods, code, parameters, and\\nstructure from the various research articles. Finally, we created a web\\napplication where users can upload published articles and view/download the\\ninformation obtained from this tool and can create their own databases for\\ntheir personal uses.'}\n",
            "{'title': 'Using current research information systems to investigate data\\n  acquisition and data sharing practices of computer scientists', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Antti Mikael Rousi', 'abstract': \"Without sufficient information about research data practices occurring in a\\nparticular research organisation, there is a risk of mismatching research data\\nservice efforts with the needs of its researchers. This study describes how\\ndata acquiring and data sharing occurring within a particular research\\norganisation can be investigated by using current research information system\\npublication data. A sample of 193 journal articles published by researchers in\\nthe computer science department of the case study's university during 2019 were\\nextracted for scrutiny from the current research information system. For these\\n193 articles, a classification of the main study types was developed to\\naccommodate the multidisciplinary nature of the case department's research\\nagenda. Furthermore, a coding framework was developed to capture the key\\nelements of data acquiring and data sharing. The articles representing life\\nsciences and computational research relatively frequently reused open data,\\nwhereas data acquisition of experimental research, human interaction studies\\nand human intervention studies often relied on collecting original data. Data\\nsharing also differed between the computationally intensive study types of life\\nsciences and computational research and the study types relying on collection\\nof original data. Research data were not available for reuse in only a minority\\nof life science (n= 2; 7%) and computational research (n = 15; 14%) studies.\\nThe study types of experimental research, human interaction studies and human\\nintervention studies less frequently made their data available for reuse. This\\nstudy demonstrates that analyses of publications listed in current research\\ninformation systems provide detailed descriptions how the affiliated\\nresearchers acquire and share research data.\"}\n",
            "{'title': 'On Network Science and Mutual Information for Explaining Deep Neural\\n  Networks', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Brian Davis, Umang Bhatt, Kartikeya Bhardwaj, Radu Marculescu, José M. F. Moura', 'abstract': 'In this paper, we present a new approach to interpret deep learning models.\\nBy coupling mutual information with network science, we explore how information\\nflows through feedforward networks. We show that efficiently approximating\\nmutual information allows us to create an information measure that quantifies\\nhow much information flows between any two neurons of a deep learning model. To\\nthat end, we propose NIF, Neural Information Flow, a technique for codifying\\ninformation flow that exposes deep learning model internals and provides\\nfeature attributions.'}\n",
            "{'title': 'Self-organizing Networks of Information Gathering Cognitive Agents', 'venue/journal/conference': 'arxiv', 'year': '2015', 'authors': 'Ahmed M. Alaa, Kartik Ahuja, Mihaela Van der Schaar', 'abstract': 'In many scenarios, networks emerge endogenously as cognitive agents establish\\nlinks in order to exchange information. Network formation has been widely\\nstudied in economics, but only on the basis of simplistic models that assume\\nthat the value of each additional piece of information is constant. In this\\npaper we present a first model and associated analysis for network formation\\nunder the much more realistic assumption that the value of each additional\\npiece of information depends on the type of that piece of information and on\\nthe information already possessed: information may be complementary or\\nredundant. We model the formation of a network as a non-cooperative game in\\nwhich the actions are the formation of links and the benefit of forming a link\\nis the value of the information exchanged minus the cost of forming the link.\\nWe characterize the topologies of the networks emerging at a Nash equilibrium\\n(NE) of this game and compare the efficiency of equilibrium networks with the\\nefficiency of centrally designed networks. To quantify the impact of\\ninformation redundancy and linking cost on social information loss, we provide\\nestimates for the Price of Anarchy (PoA); to quantify the impact on individual\\ninformation loss we introduce and provide estimates for a measure we call\\nMaximum Information Loss (MIL). Finally, we consider the setting in which\\nagents are not endowed with information, but must produce it. We show that the\\nvalidity of the well-known \"law of the few\" depends on how information\\naggregates; in particular, the \"law of the few\" fails when information displays\\ncomplementarities.'}\n",
            "{'title': 'False Consensus, Information Theory, and Prediction Markets', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Yuqing Kong, Grant Schoenebeck', 'abstract': \"We study a setting where Bayesian agents with a common prior have private\\ninformation related to an event's outcome and sequentially make public\\nannouncements relating to their information. Our main result shows that when\\nagents' private information is independent conditioning on the event's outcome\\nwhenever agents have similar beliefs about the outcome, their information is\\naggregated. That is, there is no false consensus.\\n  Our main result has a short proof based on a natural information theoretic\\nframework. A key ingredient of the framework is the equivalence between the\\nsign of the ``interaction information'' and a super/sub-additive property of\\nthe value of people's information. This provides an intuitive interpretation\\nand an interesting application of the interaction information, which measures\\nthe amount of information shared by three random variables.\\n  We illustrate the power of this information theoretic framework by reproving\\ntwo additional results within it: 1) that agents quickly agree when announcing\\n(summaries of) beliefs in round robin fashion [Aaronson 2005]; and 2) results\\nfrom [Chen et al 2010] on when prediction market agents should release\\ninformation to maximize their payment. We also interpret the information\\ntheoretic framework and the above results in prediction markets by proving that\\nthe expected reward of revealing information is the conditional mutual\\ninformation of the information revealed.\"}\n",
            "{'title': 'Theorizing Information Sources for Hope: Belief, Desire, Imagination,\\n  and Metacognition', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Tim Gorichanaz', 'abstract': 'Introduction. Hope is a positive attitude oriented toward a possible (yet\\nuncertain), desired outcome. Though hope is a virtue, hopelessness is\\nwidespread and seems related not only to current events but also to information\\nabout current events. This paper examines how hope can be sparked through\\ninformation. Method. This study uses the philosophical methods of conceptual\\nanalysis and design to advance a theoretical argument. Analysis. First, a\\nconceptualization of hope is offered, drawing on work primarily in virtue\\nethics. Then, four types of information sources for hope are theorized,\\nbuilding on and synthesizing work from philosophy and psychology. Results. Four\\ncategories of information source conducive to hopefulness are identified:\\ninformation for forming beliefs about the past or future; information for\\nengaging the moral imagination regarding possibilities for the future;\\ninformation for sparking desire for particular moral outcomes; and information\\nfor metacognition, or about how we become informed with respect to hope.\\nConclusions. Hope is, in many cases, responsive to information. This suggests a\\nmoral opportunity for information professionals and scholars to work toward\\nconnecting people with information for hope, particularly in difficult times.\\nAvenues for further research, particularly in information behavior and\\npractices, are suggested.'}\n",
            "{'title': 'Mapping Interdisciplinarity at the Interfaces between the Science\\n  Citation Index and the Social Science Citation Index', 'venue/journal/conference': 'arxiv', 'year': '2009', 'authors': 'Loet Leydesdorff', 'abstract': 'The two Journal Citation Reports of the Science Citation Index 2004 and the\\nSocial Science Citation Index 2004 were combined in order to analyze and map\\njournals and specialties at the edges and in the overlap between the two\\ndatabases. For journals which belong to the overlap (e.g., Scientometrics), the\\nmerger mainly enriches our insight into the structure which can be obtained\\nfrom the two databases separately; but in the case of scientific journals which\\nare more marginal in either database, the combination can provide a new\\nperspective on the position and function of these journals (e.g., Environment\\nand Planning B-Planning and Design). The combined database additionally enables\\nus to map citation environments in terms of the various specialties\\ncomprehensively. Using the vector-space model, visualizations are provided for\\nspecialties that are parts of the overlap (information science, science &\\ntechnology studies). On the basis of the resulting visualizations,\\n\"betweenness\"--a measure from social network analysis--is suggested as an\\nindicator for measuring the interdisciplinarity of journals.'}\n",
            "{'title': 'Data-driven materials science: status, challenges and perspectives', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Lauri Himanen, Amber Geurts, Adam S. Foster, Patrick Rinke', 'abstract': 'Data-driven science is heralded as a new paradigm in materials science. In\\nthis field, data is the new resource, and knowledge is extracted from materials\\ndata sets that are too big or complex for traditional human reasoning -\\ntypically with the intent to discover new or improved materials or materials\\nphenomena. Multiple factors, including the open science movement, national\\nfunding, and progress in information technology, have fueled its development.\\nSuch related tools as materials databases, machine learning, and\\nhigh-throughput methods are now established as parts of the materials research\\ntoolset. However, there are a variety of challenges that impede progress in\\ndata-driven materials science: data veracity, integration of experimental and\\ncomputational data, data longevity, standardization, and the gap between\\nindustrial interests and academic efforts. In this perspective article, we\\ndiscuss the historical development and current state of data-driven materials\\nscience, building from the early evolution of open science to the rapid\\nexpansion of materials data infrastructures. We also review key successes and\\nchallenges so far, providing a perspective on the future development of the\\nfield.'}\n",
            "{'title': 'A Review into Data Science and Its Approaches in Mechanical Engineering', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Ashkan Yousefi Zadeh, Meysam Shahbazy', 'abstract': \"Nowadays it is inevitable to use intelligent systems to improve the\\nperformance and optimization of different components of devices or factories.\\nFurthermore, it's so essential to have appropriate predictions to make better\\ndecisions in businesses, medical studies, and engineering studies, etc. One of\\nthe newest and most widely used of these methods is a field called Data Science\\nthat all of the scientists, engineers, and factories need to learn and use in\\ntheir careers. This article briefly introduced data science and reviewed its\\nmethods, especially it's usages in mechanical engineering and challenges and\\nways of developing data science in mechanical engineering. In the introduction,\\ndifferent definitions of data science and its background in technology\\nreviewed. In the following, data science methodology which is the process that\\na data scientist needs to do in its works been discussed. Further, some\\nresearches in the mechanical engineering area that used data science methods in\\ntheir studies, are reviewed. Eventually, it has been discussed according to the\\nsubjects that have been reviewed in the article, why it is necessary to use\\ndata science in mechanical engineering researches and projects.\"}\n",
            "{'title': 'Common Information based Markov Perfect Equilibria for Linear-Gaussian\\n  Games with Asymmetric Information', 'venue/journal/conference': 'arxiv', 'year': '2014', 'authors': 'Abhishek Gupta, Ashutosh Nayyar, Cedric Langbort, Tamer Basar', 'abstract': \"We consider a class of two-player dynamic stochastic nonzero-sum games where\\nthe state transition and observation equations are linear, and the primitive\\nrandom variables are Gaussian. Each controller acquires possibly different\\ndynamic information about the state process and the other controller's past\\nactions and observations. This leads to a dynamic game of asymmetric\\ninformation among the controllers. Building on our earlier work on finite games\\nwith asymmetric information, we devise an algorithm to compute a Nash\\nequilibrium by using the common information among the controllers. We call such\\nequilibria common information based Markov perfect equilibria of the game,\\nwhich can be viewed as a refinement of Nash equilibrium in games with\\nasymmetric information. If the players' cost functions are quadratic, then we\\nshow that under certain conditions a unique common information based Markov\\nperfect equilibrium exists. Furthermore, this equilibrium can be computed by\\nsolving a sequence of linear equations. We also show through an example that\\nthere could be other Nash equilibria in a game of asymmetric information, not\\ncorresponding to common information based Markov perfect equilibria.\"}\n",
            "{'title': 'Information Flow in Logical Environments', 'venue/journal/conference': 'arxiv', 'year': '2016', 'authors': 'Robert E. Kent', 'abstract': 'This paper describes information flow within logical environments. The theory\\nof information flow, the logic of distributed systems, was first defined by\\nBarwise and Seligman (Information Flow: The Logic of Distributed Systems.\\n1997). Logical environments are a semantic-oriented version of institutions.\\nThe theory of institutions, which was initiated by Goguen and Burstall\\n(Institutions: Abstract Model Theory for Specification and Programming. 1992),\\nis abstract model theory. Information flow is the flow of information in\\nchannels over distributed systems. The semantic integration of distributed\\nsystems, be they ontologies, databases or other information resources, can be\\ndefined in terms of the channel theory of information flow. As originally\\ndefined, the theory of information flow uses only a specific logical\\nenvironment in order to discuss information flow. This paper shows how\\ninformation flow can be defined in an arbitrary logical environment.'}\n",
            "{'title': 'Market Making with Decreasing Utility for Information', 'venue/journal/conference': 'arxiv', 'year': '2014', 'authors': 'Miroslav Dudík, Rafael Frongillo, Jennifer Wortman Vaughan', 'abstract': \"We study information elicitation in cost-function-based combinatorial\\nprediction markets when the market maker's utility for information decreases\\nover time. In the sudden revelation setting, it is known that some piece of\\ninformation will be revealed to traders, and the market maker wishes to prevent\\nguaranteed profits for trading on the sure information. In the gradual decrease\\nsetting, the market maker's utility for (partial) information decreases\\ncontinuously over time. We design adaptive cost functions for both settings\\nwhich: (1) preserve the information previously gathered in the market; (2)\\neliminate (or diminish) rewards to traders for the publicly revealed\\ninformation; (3) leave the reward structure unaffected for other information;\\nand (4) maintain the market maker's worst-case loss. Our constructions utilize\\nmixed Bregman divergence, which matches our notion of utility for information.\"}\n",
            "{'title': 'Information encryption in the expert management of strategic uncertainty', 'venue/journal/conference': 'arxiv', 'year': '2016', 'authors': 'Seth Frey, Paul L. Williams, Dominic K. Albino', 'abstract': 'Strategic agents in incomplete-information environments have a conflicted\\nrelationship with uncertainty: it can keep them unpredictable to their\\nopponents, but it must also be overcome to predict the actions of those\\nopponents. We use a multivariate generalization of information theory to\\ncharacterize the information processing behavior of strategic reasoning\\nexperts. We compare expert and novice poker players --- \"sharks\" and \"fish\" ---\\nover 1.75 million hands of online two-player No-Limit Texas Hold\\'em (NLHE).\\nComparing the effects of privately known and publicly signaled information on\\nwagering behavior, we find that the behavior of sharks coheres with information\\nthat emerges only from the interaction of public and private sources ---\\n\"synergistic\" information that does not exist in either source alone. This\\nimplies that the effect of public information on shark behavior is better\\nencrypted: it cannot be reconstructed without access to the hidden state of\\nprivate cards. Integrative information processing affects not only one\\'s own\\nstrategic behavior, but the ability of others to predict it. By characterizing\\nthe informational structure of complex strategic interactions, we offer a\\ndetailed account of how experts extract, process, and conceal valuable\\ninformation in high-uncertainty, high-stakes competitive environments.'}\n",
            "{'title': 'Information Spillover in Multiple Zero-sum Games', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Lucas Pahl', 'abstract': 'This paper considers an infinitely repeated three-player Bayesian game with\\nlack of information on two sides, in which an informed player plays two\\nzero-sum games simultaneously at each stage against two uninformed players.\\nThis is a generalization of the Aumann et al. [1] two-player zero-sum one-sided\\nincomplete information model. Under a correlated prior, the informed player\\nfaces the problem of how to optimally disclose information among two uninformed\\nplayers in order to maximize his long-term average payoffs. Our objective is to\\nunderstand the adverse effects of \\\\information spillover\" from one game to the\\nother in the equilibrium payoff set of the informed player. We provide\\nconditions under which the informed player can fully overcome such adverse\\neffects and characterize equilibrium payoffs. In a second result, we show how\\nthe effects of information spillover on the equilibrium payoff set of the\\ninformed player might be severe.'}\n",
            "{'title': 'Author Growth Outstrips Publication Growth in Computer Science and\\n  Publication Quality Correlates with Collaboration', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Stephen M. Blackburn, Kathryn S. McKinley, Lexing Xie', 'abstract': \"Although the computer science community successfully harnessed exponential\\nincreases in computer performance to drive societal and economic change, the\\nexponential growth in publications is proving harder to accommodate. To gain a\\ndeeper understanding of publication growth and inform how the computer science\\ncommunity should handle this growth, we analyzed publication practices from\\nseveral perspectives: ACM sponsored publications in the ACM Digital Library as\\na whole: subdisciplines captured by ACM's Special Interest Groups (SIGs); ten\\ntop conferences; institutions; four top U.S. departments; authors; faculty; and\\nPhDs between 1990 and 2012. ACM publishes a large fraction of all computer\\nscience research. We first summarize how we believe our main findings inform\\n(1) expectations on publication growth, (2) how to distinguish research quality\\nfrom output quantity; and (3) the evaluation of individual researchers. We then\\nfurther motivate the study of computer science publication practices and\\ndescribe our methodology and results in detail.\"}\n",
            "{'title': \"How Informal Science Education Influences Elementary Students'\\n  Perceptions of Science and Themselves\", 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Molly K Finn, Renato Mazzei, Blake Drechsler, Zoie Telkamp, Mihika Rao, Prakamya Agrawal, Anne McAlister', 'abstract': \"Underrepresentation in STEM fields starts early, with elementary students\\nalready showing differences based on gender and race in their interest in\\nscience, belief in their ability to do science, and belief that their personal\\nidentity aligns with being a scientist. Here we discuss an out-of-school time\\nastronomy program that focuses on enriching science education in under-served\\nschool systems to promote students' excitement about science and help them see\\nscientists as people just like them. Before, after, and throughout the program,\\nwe survey students on their perceptions of science, themselves, and their\\nactivities. We find that over the course of our program, students become more\\nconfident in their science abilities. Student ideas about science remain\\nunchanged, but largely align with Nature of Science ideals. We also find that\\non days that students report they were creative and asked questions, they were\\nmore likely to say they felt like a scientist and were interested in the day's\\ntopic. Our results suggest that incorporating creativity and opportunities to\\nask questions can be just as important as doing experiments for generating\\ninterest in and a sense of participating in science.\"}\n",
            "{'title': 'AI for social science and social science of AI: A Survey', 'venue/journal/conference': 'arxiv', 'year': '2024', 'authors': 'Ruoxi Xu, Yingfei Sun, Mengjie Ren, Shiguang Guo, Ruotong Pan, Hongyu Lin, Le Sun, Xianpei Han', 'abstract': 'Recent advancements in artificial intelligence, particularly with the\\nemergence of large language models (LLMs), have sparked a rethinking of\\nartificial general intelligence possibilities. The increasing human-like\\ncapabilities of AI are also attracting attention in social science research,\\nleading to various studies exploring the combination of these two fields. In\\nthis survey, we systematically categorize previous explorations in the\\ncombination of AI and social science into two directions that share common\\ntechnical approaches but differ in their research objectives. The first\\ndirection is focused on AI for social science, where AI is utilized as a\\npowerful tool to enhance various stages of social science research. While the\\nsecond direction is the social science of AI, which examines AI agents as\\nsocial entities with their human-like cognitive and linguistic capabilities. By\\nconducting a thorough review, particularly on the substantial progress\\nfacilitated by recent advancements in large language models, this paper\\nintroduces a fresh perspective to reassess the relationship between AI and\\nsocial science, provides a cohesive framework that allows researchers to\\nunderstand the distinctions and connections between AI for social science and\\nsocial science of AI, and also summarized state-of-art experiment simulation\\nplatforms to facilitate research in these two directions. We believe that as AI\\ntechnology continues to advance and intelligent agents find increasing\\napplications in our daily lives, the significance of the combination of AI and\\nsocial science will become even more prominent.'}\n",
            "{'title': 'Virtual Observatory: From Concept to Implementation', 'venue/journal/conference': 'arxiv', 'year': '2005', 'authors': 'S. G. Djorgovski, R. Williams', 'abstract': 'We review the origins of the Virtual Observatory (VO) concept, and the\\ncurrent status of the efforts in this field. VO is the response of the\\nastronomical community to the challenges posed by the modern massive and\\ncomplex data sets. It is a framework in which information technology is\\nharnessed to organize, maintain, and explore the rich information content of\\nthe exponentially growing data sets, and to enable a qualitatively new science\\nto be done with them. VO will become a complete, open, distributed, web-based\\nframework for astronomy of the early 21st century. A number of significant\\nefforts worldwide are now striving to convert this vision into reality. The\\ntechnological and methodological challenges posed by the information-rich\\nastronomy are also common to many other fields. We see a fundamental change in\\nthe way all science is done, driven by the information technology revolution.'}\n",
            "{'title': 'A polynomial axles-detection algorithm for a four-contacts treadle', 'venue/journal/conference': 'arxiv', 'year': '2001', 'authors': 'Giancarlo Crocetti', 'abstract': 'This submission was removed because it contained proprietary information that\\nwas distributed without permission.'}\n",
            "{'title': 'The Aharonov-Bohm-Effect, Non-commutative Geometry, Dislocation Theory,\\n  and Magnetism', 'venue/journal/conference': 'arxiv', 'year': '2007', 'authors': 'U. Krey', 'abstract': 'The four items mentioned in the title are put into context in an informal\\nway.'}\n",
            "{'title': 'Music, Complexity, Information', 'venue/journal/conference': 'arxiv', 'year': '2008', 'authors': 'Damian H. Zanette', 'abstract': 'These are the preparatory notes for a Science & Music essay, \"Playing by\\nnumbers\", appeared in Nature 453 (2008) 988-989.'}\n",
            "{'title': 'WLAN PIDS', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'Deng Bin', 'abstract': 'This paper discuss two structures of WLAN system fit to Passenger Information\\nDisplay System which is partly of subway.'}\n",
            "{'title': 'The irregular (integer) tetrahedron as a warehouse of biological\\n  information', 'venue/journal/conference': 'arxiv', 'year': '2012', 'authors': 'Tidjani Negadi', 'abstract': 'This paper is devoted to a new classification of the twenty amino acids based\\non the heronian (integer) tetrahedron.'}\n",
            "{'title': 'Green Computer Science Millennial Students Examination', 'venue/journal/conference': 'arxiv', 'year': '2018', 'authors': 'Leon Andretti Abdillah', 'abstract': 'Smartphones as one of information technology products have been affected\\nhigher education in various aspects. This article explains the useness of\\nsmartphones in facilitating online examination in information systems and\\ncomputer science students. The research objective to be achieved by the\\nresearchers through the research, are as follows: 1) Utilizing smartphone as a\\nmedia test online exam for green computing environment, 2) How to use social\\ninformation technologies in online test, and 3) Explore the facilities or\\nfeatures that could be used for the online exam implementation. The observation\\nwas conducted with 100 students as respondents. Researcher used google forms to\\ndisseminate questions for online examination. The findings of the research\\nshowed that most the college students used Android OS for their online\\nexamination. Social technology like google forms has rich features in\\nsupporting online examination for computer science students. The use of\\nsmartphones, google forms, and facebook can create an atmosphere of modern,\\ngreen computer science exams, efficient, and environmentally friendly.'}\n",
            "{'title': 'Quantum Computation and Quantum Information', 'venue/journal/conference': 'arxiv', 'year': '2012', 'authors': 'Yazhen Wang', 'abstract': 'Quantum computation and quantum information are of great current interest in\\ncomputer science, mathematics, physical sciences and engineering. They will\\nlikely lead to a new wave of technological innovations in communication,\\ncomputation and cryptography. As the theory of quantum physics is fundamentally\\nstochastic, randomness and uncertainty are deeply rooted in quantum\\ncomputation, quantum simulation and quantum information. Consequently quantum\\nalgorithms are random in nature, and quantum simulation utilizes Monte Carlo\\ntechniques extensively. Thus statistics can play an important role in quantum\\ncomputation and quantum simulation, which in turn offer great potential to\\nrevolutionize computational statistics. While only pseudo-random numbers can be\\ngenerated by classical computers, quantum computers are able to produce genuine\\nrandom numbers; quantum computers can exponentially or quadratically speed up\\nmedian evaluation, Monte Carlo integration and Markov chain simulation. This\\npaper gives a brief review on quantum computation, quantum simulation and\\nquantum information. We introduce the basic concepts of quantum computation and\\nquantum simulation and present quantum algorithms that are known to be much\\nfaster than the available classic algorithms. We provide a statistical\\nframework for the analysis of quantum algorithms and quantum simulation.'}\n",
            "{'title': 'Detecting and modelling real percolation and phase transitions of\\n  information on social media', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Jiarong Xie, Fanhui Meng, Jiachen Sun, Xiao Ma, Gang Yan, Yanqing Hu', 'abstract': \"It is widely believed that information spread on social media is a\\npercolation process, with parallels to phase transitions in theoretical\\nphysics. However, evidence for this hypothesis is limited, as phase transitions\\nhave not been directly observed in any social media. Here, through analysis of\\n100 million Weibo and 40 million Twitter users, we identify percolation-like\\nspread, and find that it happens more readily than current theoretical models\\nwould predict. The lower percolation threshold can be explained by the\\nexistence of positive feedback in the coevolution between network structure and\\nuser activity level, such that more active users gain more followers. Moreover,\\nthis coevolution induces an extreme imbalance in users' influence. Our findings\\nindicate that the ability of information to spread across social networks is\\nhigher than expected, with implications for many information spread problems.\"}\n",
            "{'title': 'Inorganic photonic materials for lasers and biomedicine in the infrared', 'venue/journal/conference': 'arxiv', 'year': '2013', 'authors': 'H. -T. Sun', 'abstract': 'Inorganic photonic materials demonstrating luminescence in the infrared\\nspectral range (> 1 um) play a vital important role in modern information,\\ntelecommunication, and biomedicine disciplines.'}\n",
            "{'title': 'The lifecycle of provenance metadata and its associated challenges and\\n  opportunities', 'venue/journal/conference': 'arxiv', 'year': '2016', 'authors': 'Paolo Missier', 'abstract': 'This chapter outlines some of the challenges and opportunities associated\\nwith adopting provenance principles and standards in a variety of disciplines,\\nincluding data publication and reuse, and information sciences.'}\n",
            "{'title': 'Real-time intelligent big data processing: technology, platform, and\\n  applications', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Tongya Zheng, Gang Chen, Xinyu Wang, Chun Chen, Xingen Wang, Sihui Luo', 'abstract': 'Human beings keep exploring the physical space using information means. Only\\nrecently, with the rapid development of information technologies and the\\nincreasing accumulation of data, human beings can learn more about the unknown\\nworld with data-driven methods. Given data timeliness, there is a growing\\nawareness of the importance of real-time data. There are two categories of\\ntechnologies accounting for data processing: batching big data and streaming\\nprocessing, which have not been integrated well. Thus, we propose an innovative\\nincremental processing technology named after Stream Cube to process both big\\ndata and stream data. Also, we implement a real-time intelligent data\\nprocessing system, which is based on real-time acquisition, real-time\\nprocessing, real-time analysis, and real-time decision-making. The real-time\\nintelligent data processing technology system is equipped with a batching big\\ndata platform, data analysis tools, and machine learning models. Based on our\\napplications and analysis, the real-time intelligent data processing system is\\na crucial solution to the problems of the national society and economy.'}\n",
            "{'title': 'MMSE Channel Estimation for Two-Port Demodulation Reference Signals in\\n  New Radio', 'venue/journal/conference': 'arxiv', 'year': '2020', 'authors': 'Dejin Kong, Xiang-Gen Xia, Pei Liu, Qibiao Zhu', 'abstract': 'Two-port demodulation reference signals (DMRS) have been employed in new\\nradio (NR) recently. In this paper, we firstly propose a minimum mean square\\nerror (MMSE) scheme with full priori knowledge (F-MMSE) to achieve the channel\\nestimation of two-port DMRS in NR. When the two ports are assigned to different\\nusers, the full priori knowledge of two ports is not easy to be obtained for\\none user. Then, we present a MMSE scheme with partial priori knowledge\\n(P-MMSE). Finally, numerical results show that the proposed schemes achieve\\nsatisfactory channel estimation performance. Moreover, for both mean square\\nerror and bit error ratio metrics, the proposed schemes can achieve better\\nperformance compared with the classical discrete Fourier transform based\\nchannel estimation. Particularly, P-MMSE scheme delivers almost the same\\nperformance compared with F-MMSE scheme by a small amount of prior knowledge.'}\n",
            "{'title': 'Using Full-text Content of Academic Articles to Build a Methodology\\n  Taxonomy of Information Science in China', 'venue/journal/conference': 'arxiv', 'year': '2021', 'authors': 'Heng Zhang, Chengzhi Zhang', 'abstract': 'Research on the construction of traditional information science methodology\\ntaxonomy is mostly conducted manually. From the limited corpus, researchers\\nhave attempted to summarize some of the research methodology entities into\\nseveral abstract levels (generally three levels); however, they have been\\nunable to provide a more granular hierarchy. Moreover, updating the methodology\\ntaxonomy is traditionally a slow process. In this study, we collected full-text\\nacademic papers related to information science. First, we constructed a basic\\nmethodology taxonomy with three levels by manual annotation. Then, the word\\nvectors of the research methodology entities were trained using the full-text\\ndata. Accordingly, the research methodology entities were clustered and the\\nbasic methodology taxonomy was expanded using the clustering results to obtain\\na methodology taxonomy with more levels. This study provides new concepts for\\nconstructing a methodology taxonomy of information science. The proposed\\nmethodology taxonomy is semi-automated; it is more detailed than conventional\\nschemes and the speed of taxonomy renewal has been enhanced.'}\n",
            "{'title': 'Nanomaterials for Quantum Information Science and Engineering', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Adam Alfieri, Surendra B. Anantharaman, Huiqin Zhang, Deep Jariwala', 'abstract': 'Quantum information science and engineering (QISE) which entails use of\\nquantum mechanical states for information processing, communications, and\\nsensing and the area of nanoscience and nanotechnology have dominated condensed\\nmatter physics and materials science research in the 21st century. Solid state\\ndevices for QISE have, to this point, predominantly been designed with bulk\\nmaterials as their constituents. In this review, we consider how nanomaterials\\n(i.e. materials with intrinsic quantum confinement) may offer inherent\\nadvantages over conventional materials for QISE. We identify the materials\\nchallenges for specific types of qubits, and we identify how emerging\\nnanomaterials may overcome these challenges. Challenges for and progress\\ntowards nanomaterials based quantum devices are identified. We aim to help\\nclose the gap between the nanotechnology and quantum information communities\\nand inspire research that will lead to next generation quantum devices for\\nscalable and practical quantum applications.'}\n",
            "{'title': 'Towards Automated Survey Variable Search and Summarization in Social\\n  Science Publications', 'venue/journal/conference': 'arxiv', 'year': '2022', 'authors': 'Yavuz Selim Kartal, Sotaro Takeshita, Tornike Tsereteli, Kai Eckert, Henning Kroll, Philipp Mayr, Simone Paolo Ponzetto, Benjamin Zapilko, Andrea Zielinski', 'abstract': 'Nowadays there is a growing trend in many scientific disciplines to support\\nresearchers by providing enhanced information access through linking of\\npublications and underlying datasets, so as to support research with\\ninfrastructure to enhance reproducibility and reusability of research results.\\nIn this research note, we present an overview of an ongoing research project,\\nnamed VADIS (VAriable Detection, Interlinking and Summarization), that aims at\\ndeveloping technology and infrastructure for enhanced information access in the\\nSocial Sciences via search and summarization of publications on the basis of\\nautomatic identification and indexing of survey variables in text. We provide\\nan overview of the overarching vision underlying our project, its main\\ncomponents, and related challenges, as well as a thorough discussion of how\\nthese are meant to address the limitations of current information access\\nsystems for publications in the Social Sciences. We show how this goal can be\\nconcretely implemented in an end-user system by presenting a search prototype,\\nwhich is based on user requirements collected from qualitative interviews with\\nempirical Social Science researchers.'}\n",
            "{'title': 'Characteristics of LIS Research Articles Affecting Their Citation Impact', 'venue/journal/conference': 'arxiv', 'year': '2023', 'authors': 'Kalervo Jarvelin, Yu-Wei Chang, Pertti Vakkari', 'abstract': \"The paper analyses the citation impact of Library and Information Science,\\nLIS for short, research articles published in 31 leading international LIS\\njournals in 2015. The main research question is: to what degree do authors'\\ndisciplinary composition in association with other content characteristics of\\nLIS articles affect their citation impact? The impact is analysed in terms of\\nthe number of citations received and their authority, using outlier\\nnormalization and subfield normalization. The article characteristics analysed\\nusing quantitative content analysis include topic, methodology, type of\\ncontribution, and the disciplinary composition of their author teams. The\\ncitations received by the articles are traced from 2015 to May 2021. Citing\\ndocument authority is measured by the citations they had received up to May\\n2021. The overall finding was that authors' disciplinary composition is\\nsignificantly associated with citation scores. The differences in citation\\nscores between disciplinary compositions appeared typically within information\\nretrieval and scientific communication. In both topics LIS and computer science\\njointly received significantly higher citation scores than many disciplines\\nlike LIS alone or humanities in information retrieval, or natural sciences,\\nmedicine, or social sciences alone in scientific communication. The paper is\\noriginal in allowing joint analysis of content, authorship composition, and\\nimpact.\"}\n",
            "{'title': 'Stripping syntax from complexity: An information-theoretical perspective\\n  on complex systems', 'venue/journal/conference': 'arxiv', 'year': '2016', 'authors': 'Rick Quax, Omri Har-Shemesh, Stefan Thurner, Peter M. A. Sloot', 'abstract': 'Claude Shannons information theory (1949) has had a revolutionary impact on\\ncommunication science. A crucial property of his framework is that it decouples\\nthe meaning of a message from the mechanistic details from the actual\\ncommunication process itself, which opened the way to solve long-standing\\ncommunication problems. Here we argue that a similar impact could be expected\\nby applying information theory in the context of complexity science to answer\\nlong-standing, cross-domain questions about the nature of complex systems. This\\nhappens by decoupling the domain-specific model details (e.g., neuronal\\nnetworks, ecosystems, flocks of birds) from the cross-domain phenomena that\\ncharacterize complex systems (e.g., criticality, robustness, tipping points).\\nThis goes beyond using information theory as a non-linear correlation measure,\\nnamely it allows describing a complex system entirely in terms of the storage,\\ntransfer, and modification of informational bits. After all, a phenomenon that\\ndoes not depend on model details should best be studied in a framework that\\nstrips away all such details. We highlight the first successes of\\ninformation-theoretic descriptions in the recent complexity literature, and\\nemphasize that this type of research is still in its infancy. Finally we sketch\\nhow such an information-theoretic description may even lead to a new type of\\nuniversality among complex systems, with a potentially tremendous impact. The\\ngoal of this perspective article is to motivate a paradigm shift in the young\\nfield of complexity science using a lesson learnt in communication science.'}\n",
            "{'title': 'How science maps reveal knowledge transfer: new measurement for a\\n  historical case', 'venue/journal/conference': 'arxiv', 'year': '2014', 'authors': 'Sándor Soós', 'abstract': 'Modelling actors of science via science (overlay) maps has recently become a\\npopular practice in Interdisciplinarity Research (IDR). The benefits of this\\ntoolkit have also been recognized for other areas of scientometrics, such as\\nthe study of science dynamics. In this paper we propose novel methods of\\nmeasuring knowledge diffusion/integration based on previous applications of the\\noverlay methodology. New indices called Mean Overlay Distance and Overlay\\nDiversity Ratio, respectively, are being drawn from previous uses of the\\nStirling index as the main proxy for knowledge diversification. We demonstrate\\nthe added value of this proposal via a case study addressing the development of\\na rather complex discourse in biology, usually referred to as the Species\\nProblem. The selected topic is known for a history connecting various research\\nfields and traditions, being, therefore, both an ideal and challenging case for\\nthe study of knowledge diffusion.'}\n",
            "{'title': 'Which are the influential publications in the Web of Science subject\\n  categories over a long period of time? CRExplorer software used for big-data\\n  analyses in bibliometrics', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Andreas Thor, Lutz Bornmann, Robin Haunschild, Loet Leydesdorff', 'abstract': 'What are the landmark papers in scientific disciplines? On whose shoulders\\ndoes research in these fields stand? Which papers are indispensable for\\nscientific progress? These are typical questions which are not only of interest\\nfor researchers (who frequently know the answers - or guess to know them), but\\nalso for the interested general public. Citation counts can be used to identify\\nvery useful papers, since they reflect the wisdom of the crowd; in this case,\\nthe scientists using the published results for their own research. In this\\nstudy, we identified with recently developed methods for the program CRExplorer\\nlandmark publications in nearly all Web of Science subject categories (WoSSCs).\\nThese are publications which belong more frequently than other publications\\nacross the citing years to the top-per mill in their subject category. The\\nresults for three subject categories \"Information Science and Library Science\",\\n\"Computer Science, Information Systems\", and \"Computer Science, Software\\nEngineering\" are exemplarily discussed in more detail. The results for the\\nother WoSSCs can be found online at http://crexplorer.net.'}\n",
            "{'title': 'Analysis of Computational Science Papers from ICCS 2001-2016 using Topic\\n  Modeling and Graph Theory', 'venue/journal/conference': 'arxiv', 'year': '2017', 'authors': 'Tesfamariam M. Abuhay, Sergey V. Kovalchuk, Klavdiya O. Bochenina, George Kampis, Valeria V. Krzhizhanovskaya, Michael H. Lees', 'abstract': 'This paper presents results of topic modeling and network models of topics\\nusing the International Conference on Computational Science corpus, which\\ncontains domain-specific (computational science) papers over sixteen years (a\\ntotal of 5695 papers). We discuss topical structures of International\\nConference on Computational Science, how these topics evolve over time in\\nresponse to the topicality of various problems, technologies and methods, and\\nhow all these topics relate to one another. This analysis illustrates\\nmultidisciplinary research and collaborations among scientific communities, by\\nconstructing static and dynamic networks from the topic modeling results and\\nthe keywords of authors. The results of this study give insights about the past\\nand future trends of core discussion topics in computational science. We used\\nthe Non-negative Matrix Factorization topic modeling algorithm to discover\\ntopics and labeled and grouped results hierarchically.'}\n",
            "{'title': 'Representing Contextualized Information in the NSDL', 'venue/journal/conference': 'arxiv', 'year': '2006', 'authors': 'Carl Lagoze, Dean Krafft, Tim Cornwell, Dean Eckstrom, Susan Jesuroga, Chris Wilper', 'abstract': 'The NSDL (National Science Digital Library) is funded by the National Science\\nFoundation to advance science and match education. The inital product was a\\nmetadata-based digital library providing search and access to distributed\\nresources. Our recent work recognizes the importance of context - relations,\\nmetadata, annotations - for the pedagogical value of a digital library. This\\nnew architecture uses Fedora, a tool for representing complex content, data,\\nmetadata, web-based services, and semantic relationships, as the basis of an\\ninformation network overlay (INO). The INO provides an extensible knowl-edge\\nbase for an expanding suite of digital library services.'}\n",
            "{'title': 'Quantum Information Science and Nanotechnology', 'venue/journal/conference': 'arxiv', 'year': '2009', 'authors': 'Alexander Yu. Vlasov', 'abstract': 'In this note is touched upon an application of quantum information science\\n(QIS) in nanotechnology area. The laws of quantum mechanics may be very\\nimportant for nano-scale objects. A problem with simulating of quantum systems\\nis well known and quantum computer was initially suggested by R. Feynman just\\nas the way to overcome such difficulties. Mathematical methods developed in QIS\\nalso may be applied for description of nano-devices. Few illustrative examples\\nare mentioned and they may be related with so-called fourth generation of\\nnanotechnology products.'}\n",
            "{'title': 'Analysis of Birth weight using Singular Value Decomposition', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'D. Nagarajan, P. Sunitha, V. Nagarajan, V. Seethalekshmi', 'abstract': 'The researchers have drawn much attention about the birth weight of newborn\\nbabies in the last three decades. The birth weight is one of the vital roles in\\nthe babys health. So many researchers such as (2),(1) and (4) analyzed the\\nbirth weight of babies. The aim of this paper is to analyze the birth weight\\nand some other birth weight related variable, using singular value\\ndecomposition and multiple linear regression.'}\n",
            "{'title': 'A Survivability Strategy in Route Optimization Mobile Network by Memetic\\n  Algorithm', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'K. K. Guatam, Anurag Rai', 'abstract': 'The capability to provide network service even under a significant network\\nsystem element disruption is the backbone for the survival of route optimize of\\nmobile network Technology in today s world. Keeping this view in mind, the\\npresent paper highlights a new method based on memetic algorithm.'}\n",
            "{'title': 'A Methodology for Empirical Quality Assessment of Object-Oriented Design', 'venue/journal/conference': 'arxiv', 'year': '2010', 'authors': 'Devpriya Soni, Namita Shrivastava, M. Kumar', 'abstract': 'The direct measurement of quality is difficult because there is no way we can\\nmeasure quality factors. For measuring these factors, we have to express them\\nin terms of metrics or models. Researchers have developed quality models that\\nattempt to measure quality in terms of attributes, characteristics and metrics.\\nIn this work we have proposed the methodology of controlled experimentation\\ncoupled with power of Logical Scoring of Preferences to evaluate global quality\\nof four object-oriented designs.'}\n",
            "{'title': \"Which cities' paper output and citation impact are above expectation in\\n  information science? Some improvements of our previous mapping approaches\", 'venue/journal/conference': 'arxiv', 'year': '2011', 'authors': 'Lutz Bornmann, Loet Leydesdorff', 'abstract': 'Bornmann and Leydesdorff (in press) proposed methods based on Web-of-Science\\ndata to identify field-specific excellence in cities where highly-cited papers\\nwere published more frequently than can be expected. Top performers in output\\nare cities in which authors are located who publish a number of highly-cited\\npapers that is statistically significantly higher than can be expected for\\nthese cities. Using papers published between 1989 and 2009 in information\\nscience improvements to the methods of Bornmann and Leydesdorff (in press) are\\npresented and an alternative mapping approach based on the indicator I3 is\\nintroduced here. The I3 indicator was introduced by Leydesdorff and Bornmann\\n(in press).'}\n",
            "{'title': 'A Study of Various Steganographic Techniques Used for Information Hiding', 'venue/journal/conference': 'arxiv', 'year': '2014', 'authors': 'C. P. Sumathi, T. Santanam, G. Umamaheswari', 'abstract': 'Steganography derives from the Greek word steganos, meaning covered or\\nsecret, and graphy (writing or drawing). Steganography is a technology where\\nmodern data compression, information theory, spread spectrum, and cryptography\\ntechnologies are brought together to satisfy the need for privacy on the\\nInternet. This paper is an attempt to analyse the various techniques used in\\nsteganography and to identify areas in which this technique can be applied, so\\nthat the human race can be benefited at large.'}\n",
            "{'title': 'Signatures in Shape Analysis: an Efficient Approach to Motion\\n  Identification', 'venue/journal/conference': 'arxiv', 'year': '2019', 'authors': 'Elena Celledoni, Pål Erik Lystad, Nikolas Tapia', 'abstract': 'Signatures provide a succinct description of certain features of paths in a\\nreparametrization invariant way. We propose a method for classifying shapes\\nbased on signatures, and compare it to current approaches based on the SRV\\ntransform and dynamic programming.'}\n",
            "{'title': 'Betweenness Centrality in Some Classes of Graphs', 'venue/journal/conference': 'arxiv', 'year': '2014', 'authors': 'Sunil Kumar R, Kannan Balakrishnan, M. Jathavedan', 'abstract': 'There are several centrality measures that have been introduced and studied\\nfor real world networks. They account for the different vertex characteristics\\nthat permit them to be ranked in order of importance in the network.\\nBetweenness centrality is a measure of the influence of a vertex over the flow\\nof information between every pair of vertices under the assumption that\\ninformation primarily flows over the shortest path between them. In this paper\\nwe present betweenness centrality of some important classes of graphs.'}\n",
            "{'title': '(De-)Composing Causality in Labeled Transition Systems', 'venue/journal/conference': 'arxiv', 'year': '2016', 'authors': 'Georgiana Caltais, Stefan Leue, Mohammad Reza Mousavi', 'abstract': 'In this paper we introduce a notion of counterfactual causality in the\\nHalpern and Pearl sense that is compositional with respect to the interleaving\\nof transition systems. The formal framework for reasoning on what caused the\\nviolation of a safety property is established in the context of labeled\\ntransition systems and Hennessy Milner logic. The compositionality results are\\ndevised for non-communicating systems.'}\n",
            "{'title': 'Information Measures in Detecting and Recognizing Symmetries', 'venue/journal/conference': 'arxiv', 'year': '2002', 'authors': 'Denis V. Popel', 'abstract': 'This paper presents a method to detect and recognize symmetries in Boolean\\nfunctions. The idea is to use information theoretic measures of Boolean\\nfunctions to detect sub-space of possible symmetric variables. Coupled with the\\nnew techniques of efficient estimations of information measures on Binary\\nDecision Diagrams (BDDs) we obtain promised results in symmetries detection for\\nlarge-scale functions.'}\n",
            "Got 200 articles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3269f7-55c7-4206-ad0f-6fe58b38c701"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting praw\n",
            "  Downloading praw-7.7.1-py3-none-any.whl (191 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/191.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m184.3/191.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m191.0/191.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting prawcore<3,>=2.1 (from praw)\n",
            "  Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
            "Collecting update-checker>=0.18 (from praw)\n",
            "  Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.7.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.2.2)\n",
            "Installing collected packages: update-checker, prawcore, praw\n",
            "Successfully installed praw-7.7.1 prawcore-2.4.0 update-checker-0.18.0\n",
            "Requirement already satisfied: praw in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.10/dist-packages (from praw) (1.7.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from prawcore<3,>=2.1->praw) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install praw\n",
        "!pip install --upgrade praw"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import praw\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "GeUg91eME66x"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Reddit API client\n",
        "reddit_posts = praw.Reddit(client_id = \"**********\", #peronal use script\n",
        "                    client_secret = \"*************\", #secret token\n",
        "                    usernme = \"*********\", #profile username\n",
        "                    password = \"*******\", #profile password\n",
        "                    user_agent = \"my-user-agent\", #profile agent\n",
        "                    check_for_async=False)"
      ],
      "metadata": {
        "id": "BQT7PE5cSuFk"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of subreddits to scrape\n",
        "subreddit_list_posts =  ['Diamond']\n",
        "\n",
        "# Initialize lists to store post data\n",
        "author = []\n",
        "id = []\n",
        "link_flair_text = []\n",
        "num_comments = []\n",
        "score = []\n",
        "title = []\n",
        "upvote_ratio = []\n",
        "\n",
        "# Loop through each subreddit\n",
        "for subred in subreddit_list_posts:\n",
        "    # Access the subreddit\n",
        "    subreddit = reddit_posts.subreddit(subred)\n",
        "\n",
        "    # Get the top 1000 hot posts from the subreddit\n",
        "    hot_post = subreddit.hot(limit=1000)\n",
        "\n",
        "    # Extract desired data from each post and store in respective lists\n",
        "for sub in hot_post:\n",
        "    author.append(sub.author)\n",
        "    id.append(sub.id)\n",
        "    link_flair_text.append(sub.link_flair_text)\n",
        "    num_comments.append(sub.num_comments)\n",
        "    score.append(sub.score)\n",
        "    title.append(sub.title)\n",
        "    upvote_ratio.append(sub.upvote_ratio)\n",
        "\n",
        "    # Print progress for each subreddit\n",
        "    print(subred, 'completed; ', end='')\n",
        "    print('total', len(author), 'posts have been scraped')\n",
        "\n",
        "# Create a dataframe using the extracted data\n",
        "df = pd.DataFrame({'ID':id,\n",
        "                   'Author':author,\n",
        "                   'Title':title,\n",
        "                   'Count_of_Comments':num_comments,\n",
        "                   'Upvote_Count':score,\n",
        "                   'Upvote_Ratio':upvote_ratio,\n",
        "                   'Flair':link_flair_text\n",
        "                  })\n",
        "df.to_csv('reddit_dataset_final.csv', index = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKiuzNHLE7D_",
        "outputId": "9cb5a4a0-d9b9-42e6-9283-c6f6006834f3"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diamond completed; total 1 posts have been scraped\n",
            "Diamond completed; total 2 posts have been scraped\n",
            "Diamond completed; total 3 posts have been scraped\n",
            "Diamond completed; total 4 posts have been scraped\n",
            "Diamond completed; total 5 posts have been scraped\n",
            "Diamond completed; total 6 posts have been scraped\n",
            "Diamond completed; total 7 posts have been scraped\n",
            "Diamond completed; total 8 posts have been scraped\n",
            "Diamond completed; total 9 posts have been scraped\n",
            "Diamond completed; total 10 posts have been scraped\n",
            "Diamond completed; total 11 posts have been scraped\n",
            "Diamond completed; total 12 posts have been scraped\n",
            "Diamond completed; total 13 posts have been scraped\n",
            "Diamond completed; total 14 posts have been scraped\n",
            "Diamond completed; total 15 posts have been scraped\n",
            "Diamond completed; total 16 posts have been scraped\n",
            "Diamond completed; total 17 posts have been scraped\n",
            "Diamond completed; total 18 posts have been scraped\n",
            "Diamond completed; total 19 posts have been scraped\n",
            "Diamond completed; total 20 posts have been scraped\n",
            "Diamond completed; total 21 posts have been scraped\n",
            "Diamond completed; total 22 posts have been scraped\n",
            "Diamond completed; total 23 posts have been scraped\n",
            "Diamond completed; total 24 posts have been scraped\n",
            "Diamond completed; total 25 posts have been scraped\n",
            "Diamond completed; total 26 posts have been scraped\n",
            "Diamond completed; total 27 posts have been scraped\n",
            "Diamond completed; total 28 posts have been scraped\n",
            "Diamond completed; total 29 posts have been scraped\n",
            "Diamond completed; total 30 posts have been scraped\n",
            "Diamond completed; total 31 posts have been scraped\n",
            "Diamond completed; total 32 posts have been scraped\n",
            "Diamond completed; total 33 posts have been scraped\n",
            "Diamond completed; total 34 posts have been scraped\n",
            "Diamond completed; total 35 posts have been scraped\n",
            "Diamond completed; total 36 posts have been scraped\n",
            "Diamond completed; total 37 posts have been scraped\n",
            "Diamond completed; total 38 posts have been scraped\n",
            "Diamond completed; total 39 posts have been scraped\n",
            "Diamond completed; total 40 posts have been scraped\n",
            "Diamond completed; total 41 posts have been scraped\n",
            "Diamond completed; total 42 posts have been scraped\n",
            "Diamond completed; total 43 posts have been scraped\n",
            "Diamond completed; total 44 posts have been scraped\n",
            "Diamond completed; total 45 posts have been scraped\n",
            "Diamond completed; total 46 posts have been scraped\n",
            "Diamond completed; total 47 posts have been scraped\n",
            "Diamond completed; total 48 posts have been scraped\n",
            "Diamond completed; total 49 posts have been scraped\n",
            "Diamond completed; total 50 posts have been scraped\n",
            "Diamond completed; total 51 posts have been scraped\n",
            "Diamond completed; total 52 posts have been scraped\n",
            "Diamond completed; total 53 posts have been scraped\n",
            "Diamond completed; total 54 posts have been scraped\n",
            "Diamond completed; total 55 posts have been scraped\n",
            "Diamond completed; total 56 posts have been scraped\n",
            "Diamond completed; total 57 posts have been scraped\n",
            "Diamond completed; total 58 posts have been scraped\n",
            "Diamond completed; total 59 posts have been scraped\n",
            "Diamond completed; total 60 posts have been scraped\n",
            "Diamond completed; total 61 posts have been scraped\n",
            "Diamond completed; total 62 posts have been scraped\n",
            "Diamond completed; total 63 posts have been scraped\n",
            "Diamond completed; total 64 posts have been scraped\n",
            "Diamond completed; total 65 posts have been scraped\n",
            "Diamond completed; total 66 posts have been scraped\n",
            "Diamond completed; total 67 posts have been scraped\n",
            "Diamond completed; total 68 posts have been scraped\n",
            "Diamond completed; total 69 posts have been scraped\n",
            "Diamond completed; total 70 posts have been scraped\n",
            "Diamond completed; total 71 posts have been scraped\n",
            "Diamond completed; total 72 posts have been scraped\n",
            "Diamond completed; total 73 posts have been scraped\n",
            "Diamond completed; total 74 posts have been scraped\n",
            "Diamond completed; total 75 posts have been scraped\n",
            "Diamond completed; total 76 posts have been scraped\n",
            "Diamond completed; total 77 posts have been scraped\n",
            "Diamond completed; total 78 posts have been scraped\n",
            "Diamond completed; total 79 posts have been scraped\n",
            "Diamond completed; total 80 posts have been scraped\n",
            "Diamond completed; total 81 posts have been scraped\n",
            "Diamond completed; total 82 posts have been scraped\n",
            "Diamond completed; total 83 posts have been scraped\n",
            "Diamond completed; total 84 posts have been scraped\n",
            "Diamond completed; total 85 posts have been scraped\n",
            "Diamond completed; total 86 posts have been scraped\n",
            "Diamond completed; total 87 posts have been scraped\n",
            "Diamond completed; total 88 posts have been scraped\n",
            "Diamond completed; total 89 posts have been scraped\n",
            "Diamond completed; total 90 posts have been scraped\n",
            "Diamond completed; total 91 posts have been scraped\n",
            "Diamond completed; total 92 posts have been scraped\n",
            "Diamond completed; total 93 posts have been scraped\n",
            "Diamond completed; total 94 posts have been scraped\n",
            "Diamond completed; total 95 posts have been scraped\n",
            "Diamond completed; total 96 posts have been scraped\n",
            "Diamond completed; total 97 posts have been scraped\n",
            "Diamond completed; total 98 posts have been scraped\n",
            "Diamond completed; total 99 posts have been scraped\n",
            "Diamond completed; total 100 posts have been scraped\n",
            "Diamond completed; total 101 posts have been scraped\n",
            "Diamond completed; total 102 posts have been scraped\n",
            "Diamond completed; total 103 posts have been scraped\n",
            "Diamond completed; total 104 posts have been scraped\n",
            "Diamond completed; total 105 posts have been scraped\n",
            "Diamond completed; total 106 posts have been scraped\n",
            "Diamond completed; total 107 posts have been scraped\n",
            "Diamond completed; total 108 posts have been scraped\n",
            "Diamond completed; total 109 posts have been scraped\n",
            "Diamond completed; total 110 posts have been scraped\n",
            "Diamond completed; total 111 posts have been scraped\n",
            "Diamond completed; total 112 posts have been scraped\n",
            "Diamond completed; total 113 posts have been scraped\n",
            "Diamond completed; total 114 posts have been scraped\n",
            "Diamond completed; total 115 posts have been scraped\n",
            "Diamond completed; total 116 posts have been scraped\n",
            "Diamond completed; total 117 posts have been scraped\n",
            "Diamond completed; total 118 posts have been scraped\n",
            "Diamond completed; total 119 posts have been scraped\n",
            "Diamond completed; total 120 posts have been scraped\n",
            "Diamond completed; total 121 posts have been scraped\n",
            "Diamond completed; total 122 posts have been scraped\n",
            "Diamond completed; total 123 posts have been scraped\n",
            "Diamond completed; total 124 posts have been scraped\n",
            "Diamond completed; total 125 posts have been scraped\n",
            "Diamond completed; total 126 posts have been scraped\n",
            "Diamond completed; total 127 posts have been scraped\n",
            "Diamond completed; total 128 posts have been scraped\n",
            "Diamond completed; total 129 posts have been scraped\n",
            "Diamond completed; total 130 posts have been scraped\n",
            "Diamond completed; total 131 posts have been scraped\n",
            "Diamond completed; total 132 posts have been scraped\n",
            "Diamond completed; total 133 posts have been scraped\n",
            "Diamond completed; total 134 posts have been scraped\n",
            "Diamond completed; total 135 posts have been scraped\n",
            "Diamond completed; total 136 posts have been scraped\n",
            "Diamond completed; total 137 posts have been scraped\n",
            "Diamond completed; total 138 posts have been scraped\n",
            "Diamond completed; total 139 posts have been scraped\n",
            "Diamond completed; total 140 posts have been scraped\n",
            "Diamond completed; total 141 posts have been scraped\n",
            "Diamond completed; total 142 posts have been scraped\n",
            "Diamond completed; total 143 posts have been scraped\n",
            "Diamond completed; total 144 posts have been scraped\n",
            "Diamond completed; total 145 posts have been scraped\n",
            "Diamond completed; total 146 posts have been scraped\n",
            "Diamond completed; total 147 posts have been scraped\n",
            "Diamond completed; total 148 posts have been scraped\n",
            "Diamond completed; total 149 posts have been scraped\n",
            "Diamond completed; total 150 posts have been scraped\n",
            "Diamond completed; total 151 posts have been scraped\n",
            "Diamond completed; total 152 posts have been scraped\n",
            "Diamond completed; total 153 posts have been scraped\n",
            "Diamond completed; total 154 posts have been scraped\n",
            "Diamond completed; total 155 posts have been scraped\n",
            "Diamond completed; total 156 posts have been scraped\n",
            "Diamond completed; total 157 posts have been scraped\n",
            "Diamond completed; total 158 posts have been scraped\n",
            "Diamond completed; total 159 posts have been scraped\n",
            "Diamond completed; total 160 posts have been scraped\n",
            "Diamond completed; total 161 posts have been scraped\n",
            "Diamond completed; total 162 posts have been scraped\n",
            "Diamond completed; total 163 posts have been scraped\n",
            "Diamond completed; total 164 posts have been scraped\n",
            "Diamond completed; total 165 posts have been scraped\n",
            "Diamond completed; total 166 posts have been scraped\n",
            "Diamond completed; total 167 posts have been scraped\n",
            "Diamond completed; total 168 posts have been scraped\n",
            "Diamond completed; total 169 posts have been scraped\n",
            "Diamond completed; total 170 posts have been scraped\n",
            "Diamond completed; total 171 posts have been scraped\n",
            "Diamond completed; total 172 posts have been scraped\n",
            "Diamond completed; total 173 posts have been scraped\n",
            "Diamond completed; total 174 posts have been scraped\n",
            "Diamond completed; total 175 posts have been scraped\n",
            "Diamond completed; total 176 posts have been scraped\n",
            "Diamond completed; total 177 posts have been scraped\n",
            "Diamond completed; total 178 posts have been scraped\n",
            "Diamond completed; total 179 posts have been scraped\n",
            "Diamond completed; total 180 posts have been scraped\n",
            "Diamond completed; total 181 posts have been scraped\n",
            "Diamond completed; total 182 posts have been scraped\n",
            "Diamond completed; total 183 posts have been scraped\n",
            "Diamond completed; total 184 posts have been scraped\n",
            "Diamond completed; total 185 posts have been scraped\n",
            "Diamond completed; total 186 posts have been scraped\n",
            "Diamond completed; total 187 posts have been scraped\n",
            "Diamond completed; total 188 posts have been scraped\n",
            "Diamond completed; total 189 posts have been scraped\n",
            "Diamond completed; total 190 posts have been scraped\n",
            "Diamond completed; total 191 posts have been scraped\n",
            "Diamond completed; total 192 posts have been scraped\n",
            "Diamond completed; total 193 posts have been scraped\n",
            "Diamond completed; total 194 posts have been scraped\n",
            "Diamond completed; total 195 posts have been scraped\n",
            "Diamond completed; total 196 posts have been scraped\n",
            "Diamond completed; total 197 posts have been scraped\n",
            "Diamond completed; total 198 posts have been scraped\n",
            "Diamond completed; total 199 posts have been scraped\n",
            "Diamond completed; total 200 posts have been scraped\n",
            "Diamond completed; total 201 posts have been scraped\n",
            "Diamond completed; total 202 posts have been scraped\n",
            "Diamond completed; total 203 posts have been scraped\n",
            "Diamond completed; total 204 posts have been scraped\n",
            "Diamond completed; total 205 posts have been scraped\n",
            "Diamond completed; total 206 posts have been scraped\n",
            "Diamond completed; total 207 posts have been scraped\n",
            "Diamond completed; total 208 posts have been scraped\n",
            "Diamond completed; total 209 posts have been scraped\n",
            "Diamond completed; total 210 posts have been scraped\n",
            "Diamond completed; total 211 posts have been scraped\n",
            "Diamond completed; total 212 posts have been scraped\n",
            "Diamond completed; total 213 posts have been scraped\n",
            "Diamond completed; total 214 posts have been scraped\n",
            "Diamond completed; total 215 posts have been scraped\n",
            "Diamond completed; total 216 posts have been scraped\n",
            "Diamond completed; total 217 posts have been scraped\n",
            "Diamond completed; total 218 posts have been scraped\n",
            "Diamond completed; total 219 posts have been scraped\n",
            "Diamond completed; total 220 posts have been scraped\n",
            "Diamond completed; total 221 posts have been scraped\n",
            "Diamond completed; total 222 posts have been scraped\n",
            "Diamond completed; total 223 posts have been scraped\n",
            "Diamond completed; total 224 posts have been scraped\n",
            "Diamond completed; total 225 posts have been scraped\n",
            "Diamond completed; total 226 posts have been scraped\n",
            "Diamond completed; total 227 posts have been scraped\n",
            "Diamond completed; total 228 posts have been scraped\n",
            "Diamond completed; total 229 posts have been scraped\n",
            "Diamond completed; total 230 posts have been scraped\n",
            "Diamond completed; total 231 posts have been scraped\n",
            "Diamond completed; total 232 posts have been scraped\n",
            "Diamond completed; total 233 posts have been scraped\n",
            "Diamond completed; total 234 posts have been scraped\n",
            "Diamond completed; total 235 posts have been scraped\n",
            "Diamond completed; total 236 posts have been scraped\n",
            "Diamond completed; total 237 posts have been scraped\n",
            "Diamond completed; total 238 posts have been scraped\n",
            "Diamond completed; total 239 posts have been scraped\n",
            "Diamond completed; total 240 posts have been scraped\n",
            "Diamond completed; total 241 posts have been scraped\n",
            "Diamond completed; total 242 posts have been scraped\n",
            "Diamond completed; total 243 posts have been scraped\n",
            "Diamond completed; total 244 posts have been scraped\n",
            "Diamond completed; total 245 posts have been scraped\n",
            "Diamond completed; total 246 posts have been scraped\n",
            "Diamond completed; total 247 posts have been scraped\n",
            "Diamond completed; total 248 posts have been scraped\n",
            "Diamond completed; total 249 posts have been scraped\n",
            "Diamond completed; total 250 posts have been scraped\n",
            "Diamond completed; total 251 posts have been scraped\n",
            "Diamond completed; total 252 posts have been scraped\n",
            "Diamond completed; total 253 posts have been scraped\n",
            "Diamond completed; total 254 posts have been scraped\n",
            "Diamond completed; total 255 posts have been scraped\n",
            "Diamond completed; total 256 posts have been scraped\n",
            "Diamond completed; total 257 posts have been scraped\n",
            "Diamond completed; total 258 posts have been scraped\n",
            "Diamond completed; total 259 posts have been scraped\n",
            "Diamond completed; total 260 posts have been scraped\n",
            "Diamond completed; total 261 posts have been scraped\n",
            "Diamond completed; total 262 posts have been scraped\n",
            "Diamond completed; total 263 posts have been scraped\n",
            "Diamond completed; total 264 posts have been scraped\n",
            "Diamond completed; total 265 posts have been scraped\n",
            "Diamond completed; total 266 posts have been scraped\n",
            "Diamond completed; total 267 posts have been scraped\n",
            "Diamond completed; total 268 posts have been scraped\n",
            "Diamond completed; total 269 posts have been scraped\n",
            "Diamond completed; total 270 posts have been scraped\n",
            "Diamond completed; total 271 posts have been scraped\n",
            "Diamond completed; total 272 posts have been scraped\n",
            "Diamond completed; total 273 posts have been scraped\n",
            "Diamond completed; total 274 posts have been scraped\n",
            "Diamond completed; total 275 posts have been scraped\n",
            "Diamond completed; total 276 posts have been scraped\n",
            "Diamond completed; total 277 posts have been scraped\n",
            "Diamond completed; total 278 posts have been scraped\n",
            "Diamond completed; total 279 posts have been scraped\n",
            "Diamond completed; total 280 posts have been scraped\n",
            "Diamond completed; total 281 posts have been scraped\n",
            "Diamond completed; total 282 posts have been scraped\n",
            "Diamond completed; total 283 posts have been scraped\n",
            "Diamond completed; total 284 posts have been scraped\n",
            "Diamond completed; total 285 posts have been scraped\n",
            "Diamond completed; total 286 posts have been scraped\n",
            "Diamond completed; total 287 posts have been scraped\n",
            "Diamond completed; total 288 posts have been scraped\n",
            "Diamond completed; total 289 posts have been scraped\n",
            "Diamond completed; total 290 posts have been scraped\n",
            "Diamond completed; total 291 posts have been scraped\n",
            "Diamond completed; total 292 posts have been scraped\n",
            "Diamond completed; total 293 posts have been scraped\n",
            "Diamond completed; total 294 posts have been scraped\n",
            "Diamond completed; total 295 posts have been scraped\n",
            "Diamond completed; total 296 posts have been scraped\n",
            "Diamond completed; total 297 posts have been scraped\n",
            "Diamond completed; total 298 posts have been scraped\n",
            "Diamond completed; total 299 posts have been scraped\n",
            "Diamond completed; total 300 posts have been scraped\n",
            "Diamond completed; total 301 posts have been scraped\n",
            "Diamond completed; total 302 posts have been scraped\n",
            "Diamond completed; total 303 posts have been scraped\n",
            "Diamond completed; total 304 posts have been scraped\n",
            "Diamond completed; total 305 posts have been scraped\n",
            "Diamond completed; total 306 posts have been scraped\n",
            "Diamond completed; total 307 posts have been scraped\n",
            "Diamond completed; total 308 posts have been scraped\n",
            "Diamond completed; total 309 posts have been scraped\n",
            "Diamond completed; total 310 posts have been scraped\n",
            "Diamond completed; total 311 posts have been scraped\n",
            "Diamond completed; total 312 posts have been scraped\n",
            "Diamond completed; total 313 posts have been scraped\n",
            "Diamond completed; total 314 posts have been scraped\n",
            "Diamond completed; total 315 posts have been scraped\n",
            "Diamond completed; total 316 posts have been scraped\n",
            "Diamond completed; total 317 posts have been scraped\n",
            "Diamond completed; total 318 posts have been scraped\n",
            "Diamond completed; total 319 posts have been scraped\n",
            "Diamond completed; total 320 posts have been scraped\n",
            "Diamond completed; total 321 posts have been scraped\n",
            "Diamond completed; total 322 posts have been scraped\n",
            "Diamond completed; total 323 posts have been scraped\n",
            "Diamond completed; total 324 posts have been scraped\n",
            "Diamond completed; total 325 posts have been scraped\n",
            "Diamond completed; total 326 posts have been scraped\n",
            "Diamond completed; total 327 posts have been scraped\n",
            "Diamond completed; total 328 posts have been scraped\n",
            "Diamond completed; total 329 posts have been scraped\n",
            "Diamond completed; total 330 posts have been scraped\n",
            "Diamond completed; total 331 posts have been scraped\n",
            "Diamond completed; total 332 posts have been scraped\n",
            "Diamond completed; total 333 posts have been scraped\n",
            "Diamond completed; total 334 posts have been scraped\n",
            "Diamond completed; total 335 posts have been scraped\n",
            "Diamond completed; total 336 posts have been scraped\n",
            "Diamond completed; total 337 posts have been scraped\n",
            "Diamond completed; total 338 posts have been scraped\n",
            "Diamond completed; total 339 posts have been scraped\n",
            "Diamond completed; total 340 posts have been scraped\n",
            "Diamond completed; total 341 posts have been scraped\n",
            "Diamond completed; total 342 posts have been scraped\n",
            "Diamond completed; total 343 posts have been scraped\n",
            "Diamond completed; total 344 posts have been scraped\n",
            "Diamond completed; total 345 posts have been scraped\n",
            "Diamond completed; total 346 posts have been scraped\n",
            "Diamond completed; total 347 posts have been scraped\n",
            "Diamond completed; total 348 posts have been scraped\n",
            "Diamond completed; total 349 posts have been scraped\n",
            "Diamond completed; total 350 posts have been scraped\n",
            "Diamond completed; total 351 posts have been scraped\n",
            "Diamond completed; total 352 posts have been scraped\n",
            "Diamond completed; total 353 posts have been scraped\n",
            "Diamond completed; total 354 posts have been scraped\n",
            "Diamond completed; total 355 posts have been scraped\n",
            "Diamond completed; total 356 posts have been scraped\n",
            "Diamond completed; total 357 posts have been scraped\n",
            "Diamond completed; total 358 posts have been scraped\n",
            "Diamond completed; total 359 posts have been scraped\n",
            "Diamond completed; total 360 posts have been scraped\n",
            "Diamond completed; total 361 posts have been scraped\n",
            "Diamond completed; total 362 posts have been scraped\n",
            "Diamond completed; total 363 posts have been scraped\n",
            "Diamond completed; total 364 posts have been scraped\n",
            "Diamond completed; total 365 posts have been scraped\n",
            "Diamond completed; total 366 posts have been scraped\n",
            "Diamond completed; total 367 posts have been scraped\n",
            "Diamond completed; total 368 posts have been scraped\n",
            "Diamond completed; total 369 posts have been scraped\n",
            "Diamond completed; total 370 posts have been scraped\n",
            "Diamond completed; total 371 posts have been scraped\n",
            "Diamond completed; total 372 posts have been scraped\n",
            "Diamond completed; total 373 posts have been scraped\n",
            "Diamond completed; total 374 posts have been scraped\n",
            "Diamond completed; total 375 posts have been scraped\n",
            "Diamond completed; total 376 posts have been scraped\n",
            "Diamond completed; total 377 posts have been scraped\n",
            "Diamond completed; total 378 posts have been scraped\n",
            "Diamond completed; total 379 posts have been scraped\n",
            "Diamond completed; total 380 posts have been scraped\n",
            "Diamond completed; total 381 posts have been scraped\n",
            "Diamond completed; total 382 posts have been scraped\n",
            "Diamond completed; total 383 posts have been scraped\n",
            "Diamond completed; total 384 posts have been scraped\n",
            "Diamond completed; total 385 posts have been scraped\n",
            "Diamond completed; total 386 posts have been scraped\n",
            "Diamond completed; total 387 posts have been scraped\n",
            "Diamond completed; total 388 posts have been scraped\n",
            "Diamond completed; total 389 posts have been scraped\n",
            "Diamond completed; total 390 posts have been scraped\n",
            "Diamond completed; total 391 posts have been scraped\n",
            "Diamond completed; total 392 posts have been scraped\n",
            "Diamond completed; total 393 posts have been scraped\n",
            "Diamond completed; total 394 posts have been scraped\n",
            "Diamond completed; total 395 posts have been scraped\n",
            "Diamond completed; total 396 posts have been scraped\n",
            "Diamond completed; total 397 posts have been scraped\n",
            "Diamond completed; total 398 posts have been scraped\n",
            "Diamond completed; total 399 posts have been scraped\n",
            "Diamond completed; total 400 posts have been scraped\n",
            "Diamond completed; total 401 posts have been scraped\n",
            "Diamond completed; total 402 posts have been scraped\n",
            "Diamond completed; total 403 posts have been scraped\n",
            "Diamond completed; total 404 posts have been scraped\n",
            "Diamond completed; total 405 posts have been scraped\n",
            "Diamond completed; total 406 posts have been scraped\n",
            "Diamond completed; total 407 posts have been scraped\n",
            "Diamond completed; total 408 posts have been scraped\n",
            "Diamond completed; total 409 posts have been scraped\n",
            "Diamond completed; total 410 posts have been scraped\n",
            "Diamond completed; total 411 posts have been scraped\n",
            "Diamond completed; total 412 posts have been scraped\n",
            "Diamond completed; total 413 posts have been scraped\n",
            "Diamond completed; total 414 posts have been scraped\n",
            "Diamond completed; total 415 posts have been scraped\n",
            "Diamond completed; total 416 posts have been scraped\n",
            "Diamond completed; total 417 posts have been scraped\n",
            "Diamond completed; total 418 posts have been scraped\n",
            "Diamond completed; total 419 posts have been scraped\n",
            "Diamond completed; total 420 posts have been scraped\n",
            "Diamond completed; total 421 posts have been scraped\n",
            "Diamond completed; total 422 posts have been scraped\n",
            "Diamond completed; total 423 posts have been scraped\n",
            "Diamond completed; total 424 posts have been scraped\n",
            "Diamond completed; total 425 posts have been scraped\n",
            "Diamond completed; total 426 posts have been scraped\n",
            "Diamond completed; total 427 posts have been scraped\n",
            "Diamond completed; total 428 posts have been scraped\n",
            "Diamond completed; total 429 posts have been scraped\n",
            "Diamond completed; total 430 posts have been scraped\n",
            "Diamond completed; total 431 posts have been scraped\n",
            "Diamond completed; total 432 posts have been scraped\n",
            "Diamond completed; total 433 posts have been scraped\n",
            "Diamond completed; total 434 posts have been scraped\n",
            "Diamond completed; total 435 posts have been scraped\n",
            "Diamond completed; total 436 posts have been scraped\n",
            "Diamond completed; total 437 posts have been scraped\n",
            "Diamond completed; total 438 posts have been scraped\n",
            "Diamond completed; total 439 posts have been scraped\n",
            "Diamond completed; total 440 posts have been scraped\n",
            "Diamond completed; total 441 posts have been scraped\n",
            "Diamond completed; total 442 posts have been scraped\n",
            "Diamond completed; total 443 posts have been scraped\n",
            "Diamond completed; total 444 posts have been scraped\n",
            "Diamond completed; total 445 posts have been scraped\n",
            "Diamond completed; total 446 posts have been scraped\n",
            "Diamond completed; total 447 posts have been scraped\n",
            "Diamond completed; total 448 posts have been scraped\n",
            "Diamond completed; total 449 posts have been scraped\n",
            "Diamond completed; total 450 posts have been scraped\n",
            "Diamond completed; total 451 posts have been scraped\n",
            "Diamond completed; total 452 posts have been scraped\n",
            "Diamond completed; total 453 posts have been scraped\n",
            "Diamond completed; total 454 posts have been scraped\n",
            "Diamond completed; total 455 posts have been scraped\n",
            "Diamond completed; total 456 posts have been scraped\n",
            "Diamond completed; total 457 posts have been scraped\n",
            "Diamond completed; total 458 posts have been scraped\n",
            "Diamond completed; total 459 posts have been scraped\n",
            "Diamond completed; total 460 posts have been scraped\n",
            "Diamond completed; total 461 posts have been scraped\n",
            "Diamond completed; total 462 posts have been scraped\n",
            "Diamond completed; total 463 posts have been scraped\n",
            "Diamond completed; total 464 posts have been scraped\n",
            "Diamond completed; total 465 posts have been scraped\n",
            "Diamond completed; total 466 posts have been scraped\n",
            "Diamond completed; total 467 posts have been scraped\n",
            "Diamond completed; total 468 posts have been scraped\n",
            "Diamond completed; total 469 posts have been scraped\n",
            "Diamond completed; total 470 posts have been scraped\n",
            "Diamond completed; total 471 posts have been scraped\n",
            "Diamond completed; total 472 posts have been scraped\n",
            "Diamond completed; total 473 posts have been scraped\n",
            "Diamond completed; total 474 posts have been scraped\n",
            "Diamond completed; total 475 posts have been scraped\n",
            "Diamond completed; total 476 posts have been scraped\n",
            "Diamond completed; total 477 posts have been scraped\n",
            "Diamond completed; total 478 posts have been scraped\n",
            "Diamond completed; total 479 posts have been scraped\n",
            "Diamond completed; total 480 posts have been scraped\n",
            "Diamond completed; total 481 posts have been scraped\n",
            "Diamond completed; total 482 posts have been scraped\n",
            "Diamond completed; total 483 posts have been scraped\n",
            "Diamond completed; total 484 posts have been scraped\n",
            "Diamond completed; total 485 posts have been scraped\n",
            "Diamond completed; total 486 posts have been scraped\n",
            "Diamond completed; total 487 posts have been scraped\n",
            "Diamond completed; total 488 posts have been scraped\n",
            "Diamond completed; total 489 posts have been scraped\n",
            "Diamond completed; total 490 posts have been scraped\n",
            "Diamond completed; total 491 posts have been scraped\n",
            "Diamond completed; total 492 posts have been scraped\n",
            "Diamond completed; total 493 posts have been scraped\n",
            "Diamond completed; total 494 posts have been scraped\n",
            "Diamond completed; total 495 posts have been scraped\n",
            "Diamond completed; total 496 posts have been scraped\n",
            "Diamond completed; total 497 posts have been scraped\n",
            "Diamond completed; total 498 posts have been scraped\n",
            "Diamond completed; total 499 posts have been scraped\n",
            "Diamond completed; total 500 posts have been scraped\n",
            "Diamond completed; total 501 posts have been scraped\n",
            "Diamond completed; total 502 posts have been scraped\n",
            "Diamond completed; total 503 posts have been scraped\n",
            "Diamond completed; total 504 posts have been scraped\n",
            "Diamond completed; total 505 posts have been scraped\n",
            "Diamond completed; total 506 posts have been scraped\n",
            "Diamond completed; total 507 posts have been scraped\n",
            "Diamond completed; total 508 posts have been scraped\n",
            "Diamond completed; total 509 posts have been scraped\n",
            "Diamond completed; total 510 posts have been scraped\n",
            "Diamond completed; total 511 posts have been scraped\n",
            "Diamond completed; total 512 posts have been scraped\n",
            "Diamond completed; total 513 posts have been scraped\n",
            "Diamond completed; total 514 posts have been scraped\n",
            "Diamond completed; total 515 posts have been scraped\n",
            "Diamond completed; total 516 posts have been scraped\n",
            "Diamond completed; total 517 posts have been scraped\n",
            "Diamond completed; total 518 posts have been scraped\n",
            "Diamond completed; total 519 posts have been scraped\n",
            "Diamond completed; total 520 posts have been scraped\n",
            "Diamond completed; total 521 posts have been scraped\n",
            "Diamond completed; total 522 posts have been scraped\n",
            "Diamond completed; total 523 posts have been scraped\n",
            "Diamond completed; total 524 posts have been scraped\n",
            "Diamond completed; total 525 posts have been scraped\n",
            "Diamond completed; total 526 posts have been scraped\n",
            "Diamond completed; total 527 posts have been scraped\n",
            "Diamond completed; total 528 posts have been scraped\n",
            "Diamond completed; total 529 posts have been scraped\n",
            "Diamond completed; total 530 posts have been scraped\n",
            "Diamond completed; total 531 posts have been scraped\n",
            "Diamond completed; total 532 posts have been scraped\n",
            "Diamond completed; total 533 posts have been scraped\n",
            "Diamond completed; total 534 posts have been scraped\n",
            "Diamond completed; total 535 posts have been scraped\n",
            "Diamond completed; total 536 posts have been scraped\n",
            "Diamond completed; total 537 posts have been scraped\n",
            "Diamond completed; total 538 posts have been scraped\n",
            "Diamond completed; total 539 posts have been scraped\n",
            "Diamond completed; total 540 posts have been scraped\n",
            "Diamond completed; total 541 posts have been scraped\n",
            "Diamond completed; total 542 posts have been scraped\n",
            "Diamond completed; total 543 posts have been scraped\n",
            "Diamond completed; total 544 posts have been scraped\n",
            "Diamond completed; total 545 posts have been scraped\n",
            "Diamond completed; total 546 posts have been scraped\n",
            "Diamond completed; total 547 posts have been scraped\n",
            "Diamond completed; total 548 posts have been scraped\n",
            "Diamond completed; total 549 posts have been scraped\n",
            "Diamond completed; total 550 posts have been scraped\n",
            "Diamond completed; total 551 posts have been scraped\n",
            "Diamond completed; total 552 posts have been scraped\n",
            "Diamond completed; total 553 posts have been scraped\n",
            "Diamond completed; total 554 posts have been scraped\n",
            "Diamond completed; total 555 posts have been scraped\n",
            "Diamond completed; total 556 posts have been scraped\n",
            "Diamond completed; total 557 posts have been scraped\n",
            "Diamond completed; total 558 posts have been scraped\n",
            "Diamond completed; total 559 posts have been scraped\n",
            "Diamond completed; total 560 posts have been scraped\n",
            "Diamond completed; total 561 posts have been scraped\n",
            "Diamond completed; total 562 posts have been scraped\n",
            "Diamond completed; total 563 posts have been scraped\n",
            "Diamond completed; total 564 posts have been scraped\n",
            "Diamond completed; total 565 posts have been scraped\n",
            "Diamond completed; total 566 posts have been scraped\n",
            "Diamond completed; total 567 posts have been scraped\n",
            "Diamond completed; total 568 posts have been scraped\n",
            "Diamond completed; total 569 posts have been scraped\n",
            "Diamond completed; total 570 posts have been scraped\n",
            "Diamond completed; total 571 posts have been scraped\n",
            "Diamond completed; total 572 posts have been scraped\n",
            "Diamond completed; total 573 posts have been scraped\n",
            "Diamond completed; total 574 posts have been scraped\n",
            "Diamond completed; total 575 posts have been scraped\n",
            "Diamond completed; total 576 posts have been scraped\n",
            "Diamond completed; total 577 posts have been scraped\n",
            "Diamond completed; total 578 posts have been scraped\n",
            "Diamond completed; total 579 posts have been scraped\n",
            "Diamond completed; total 580 posts have been scraped\n",
            "Diamond completed; total 581 posts have been scraped\n",
            "Diamond completed; total 582 posts have been scraped\n",
            "Diamond completed; total 583 posts have been scraped\n",
            "Diamond completed; total 584 posts have been scraped\n",
            "Diamond completed; total 585 posts have been scraped\n",
            "Diamond completed; total 586 posts have been scraped\n",
            "Diamond completed; total 587 posts have been scraped\n",
            "Diamond completed; total 588 posts have been scraped\n",
            "Diamond completed; total 589 posts have been scraped\n",
            "Diamond completed; total 590 posts have been scraped\n",
            "Diamond completed; total 591 posts have been scraped\n",
            "Diamond completed; total 592 posts have been scraped\n",
            "Diamond completed; total 593 posts have been scraped\n",
            "Diamond completed; total 594 posts have been scraped\n",
            "Diamond completed; total 595 posts have been scraped\n",
            "Diamond completed; total 596 posts have been scraped\n",
            "Diamond completed; total 597 posts have been scraped\n",
            "Diamond completed; total 598 posts have been scraped\n",
            "Diamond completed; total 599 posts have been scraped\n",
            "Diamond completed; total 600 posts have been scraped\n",
            "Diamond completed; total 601 posts have been scraped\n",
            "Diamond completed; total 602 posts have been scraped\n",
            "Diamond completed; total 603 posts have been scraped\n",
            "Diamond completed; total 604 posts have been scraped\n",
            "Diamond completed; total 605 posts have been scraped\n",
            "Diamond completed; total 606 posts have been scraped\n",
            "Diamond completed; total 607 posts have been scraped\n",
            "Diamond completed; total 608 posts have been scraped\n",
            "Diamond completed; total 609 posts have been scraped\n",
            "Diamond completed; total 610 posts have been scraped\n",
            "Diamond completed; total 611 posts have been scraped\n",
            "Diamond completed; total 612 posts have been scraped\n",
            "Diamond completed; total 613 posts have been scraped\n",
            "Diamond completed; total 614 posts have been scraped\n",
            "Diamond completed; total 615 posts have been scraped\n",
            "Diamond completed; total 616 posts have been scraped\n",
            "Diamond completed; total 617 posts have been scraped\n",
            "Diamond completed; total 618 posts have been scraped\n",
            "Diamond completed; total 619 posts have been scraped\n",
            "Diamond completed; total 620 posts have been scraped\n",
            "Diamond completed; total 621 posts have been scraped\n",
            "Diamond completed; total 622 posts have been scraped\n",
            "Diamond completed; total 623 posts have been scraped\n",
            "Diamond completed; total 624 posts have been scraped\n",
            "Diamond completed; total 625 posts have been scraped\n",
            "Diamond completed; total 626 posts have been scraped\n",
            "Diamond completed; total 627 posts have been scraped\n",
            "Diamond completed; total 628 posts have been scraped\n",
            "Diamond completed; total 629 posts have been scraped\n",
            "Diamond completed; total 630 posts have been scraped\n",
            "Diamond completed; total 631 posts have been scraped\n",
            "Diamond completed; total 632 posts have been scraped\n",
            "Diamond completed; total 633 posts have been scraped\n",
            "Diamond completed; total 634 posts have been scraped\n",
            "Diamond completed; total 635 posts have been scraped\n",
            "Diamond completed; total 636 posts have been scraped\n",
            "Diamond completed; total 637 posts have been scraped\n",
            "Diamond completed; total 638 posts have been scraped\n",
            "Diamond completed; total 639 posts have been scraped\n",
            "Diamond completed; total 640 posts have been scraped\n",
            "Diamond completed; total 641 posts have been scraped\n",
            "Diamond completed; total 642 posts have been scraped\n",
            "Diamond completed; total 643 posts have been scraped\n",
            "Diamond completed; total 644 posts have been scraped\n",
            "Diamond completed; total 645 posts have been scraped\n",
            "Diamond completed; total 646 posts have been scraped\n",
            "Diamond completed; total 647 posts have been scraped\n",
            "Diamond completed; total 648 posts have been scraped\n",
            "Diamond completed; total 649 posts have been scraped\n",
            "Diamond completed; total 650 posts have been scraped\n",
            "Diamond completed; total 651 posts have been scraped\n",
            "Diamond completed; total 652 posts have been scraped\n",
            "Diamond completed; total 653 posts have been scraped\n",
            "Diamond completed; total 654 posts have been scraped\n",
            "Diamond completed; total 655 posts have been scraped\n",
            "Diamond completed; total 656 posts have been scraped\n",
            "Diamond completed; total 657 posts have been scraped\n",
            "Diamond completed; total 658 posts have been scraped\n",
            "Diamond completed; total 659 posts have been scraped\n",
            "Diamond completed; total 660 posts have been scraped\n",
            "Diamond completed; total 661 posts have been scraped\n",
            "Diamond completed; total 662 posts have been scraped\n",
            "Diamond completed; total 663 posts have been scraped\n",
            "Diamond completed; total 664 posts have been scraped\n",
            "Diamond completed; total 665 posts have been scraped\n",
            "Diamond completed; total 666 posts have been scraped\n",
            "Diamond completed; total 667 posts have been scraped\n",
            "Diamond completed; total 668 posts have been scraped\n",
            "Diamond completed; total 669 posts have been scraped\n",
            "Diamond completed; total 670 posts have been scraped\n",
            "Diamond completed; total 671 posts have been scraped\n",
            "Diamond completed; total 672 posts have been scraped\n",
            "Diamond completed; total 673 posts have been scraped\n",
            "Diamond completed; total 674 posts have been scraped\n",
            "Diamond completed; total 675 posts have been scraped\n",
            "Diamond completed; total 676 posts have been scraped\n",
            "Diamond completed; total 677 posts have been scraped\n",
            "Diamond completed; total 678 posts have been scraped\n",
            "Diamond completed; total 679 posts have been scraped\n",
            "Diamond completed; total 680 posts have been scraped\n",
            "Diamond completed; total 681 posts have been scraped\n",
            "Diamond completed; total 682 posts have been scraped\n",
            "Diamond completed; total 683 posts have been scraped\n",
            "Diamond completed; total 684 posts have been scraped\n",
            "Diamond completed; total 685 posts have been scraped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" This week class and this coding tutorial are really helpful in recalling all these topics for me. I have read these before 1 year and now I'm revising these web scraping tasks. I have to practice more on these web scraping techniques so that I will have good command on these tools. The seriious challenge that I encountered is to find a proper scrapper to scrap the data from twitter and reddit  \"\"\""
      ],
      "metadata": {
        "id": "akAVJn9YBTQT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "7a6c49cb-2ac9-42a7-d41d-dacc35fc9dfc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" This week class and this coding tutorial are really helpful in recalling all these topics for me. I have read these before 1 year and now I'm revising these web scraping tasks. I have to practice more on these web scraping techniques so that I will have good command on these tools. The seriious challenge that I encountered is to find a proper scrapper to scrap the data from twitter and reddit  \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FAwjwmgKYXBz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}